# Bash Shell {#rse-bash}

## Questions {#rse-bash-questions}

```{r, child="questions/bash.md"}
```

## Introduction {#rse-bash-intro}

At a high level, computers do four things:

-   run programs
-   store data
-   communicate with each other, and
-   interact with us

They can do the last of these in many different ways,
including through a keyboard and mouse, touch screen interfaces, speech
recognition systems, or even brain-computer interfaces.
While touch and voice interfaces are becoming more commonplace, most interaction is still
done using traditional screens, mice, touchpads and keyboards.

The **graphical user interface** (GUI) is the most widely used way to interact with
personal computers. We give instructions (to run a program, to copy a file, to create
a new folder/directory) with the convenience of a few mouse clicks.
This way of interacting with a computer is intuitive and easy to learn,
but scales very poorly if we are to give a large stream of instructions.

In a **command-line interface** (CLI),
you communicate to the computer through writing commands instead of clicking with the mouse.
This can be intimidating since it requires the knowledge of a few commands to get started,
but just as when learning a natural language you can give quickly give
efficient, precise instructions as your proficiency increases.
Command-line interfaces are particularly good at automating repetitive tasks.
For example,
suppose we needed to copy the third line of each of a thousand text files stored
in thousand different directories and paste it into a single file line by line.
Using the traditional GUI approach of mouse clicks,
this task would take several hours.
With a command-line interface,
a single instruction can be repeated (as is or with some modification) as many times as we want,
so this task could be accomplished in a few minutes at most.

The heart of a command-line interface is a **read-evaluate-print loop** (REPL). It is called
so because when you type a command and press <kbd>Return</kbd>
(also known as <kbd>Enter</kbd>) the command-line interface
reads your command,
evaluates (or "executes") it,
prints the output of your command,
then loops back and waits for you to enter another command.

### Unix shells

GUIs and CLIs are examples of **shells**, interfaces that sits around the
computers innermost software and interpret the our instructions whether they
are mouse clicks or written commands. A Unix shell is a common CLI for Unix
systems (such as Linux and macOS) and you will often hear the term "shell" used
colloquially to specifically refer to Unix shells. In essence, a Unix shell is
a program which can run built-in and external programs.
Those programs can be as complicated as climate modeling software and as simple
as a program that creates a new directory. The simple programs which are used
to perform stand alone tasks are usually referred to as commands.

The most popular Unix shell is Bash, (the Bourne Again SHell -- named to pun
its predecessor, the Bourne shell). Bash is the default shell on most modern
implementations of Unix and in most packages that provide Unix-like tools for
Windows. When Bash is first opened, you are presented with a **prompt**,
indicating that the shell is waiting for input.

```shell
$
```

Unix shells typically use `$ ` as the prompt, but may use a different symbol.
In the examples for this lesson, we'll show the prompt as `$ `.
Most importantly:
when typing commands, either from these lessons or from other sources,
*do not type the prompt*, only the commands that follow it.

So let's try our first command, which will tell us our username:

```shell
$ whoami
```

```text
amira
```

(Amira is one of the learner personas described earlier.)


### Are CLIs more difficult to learn than GUIs?

A CLI employs a different model of interaction than a GUI,
which will take some effort and time to learn since most of us are used to working with GUIs.
A GUI presents you with choices and you select one.
With a CLI the choices are combinations of commands and parameters,
more like words in a language than buttons on a screen.
They are not presented to you so you must learn a few,
like learning some vocabulary in a new language.
But a small number of commands gets you a long way,
and we'll cover those essential few in this lesson.

### Flexibility and automation

The grammar of a Unix shell allows you to combine existing tools into powerful
pipelines and handle large volumes of data automatically. Sequences of
commands can be written into a *script*, improving the reproducibility of
workflows and allowing you to repeat them easily.

In addition, the command-line is often the easiest way to interact with remote machines and supercomputers.
Familiarity with the shell is near essential to run a variety of specialized tools and resources
including high-performance computing systems.
As clusters and cloud computing systems become more popular for scientific data crunching,
being able to interact with the shell is becoming a necessary skill.
We can build on the command-line skills covered here
to tackle a wide range of scientific questions and computational challenges.

## Navigating the filesystem {#rse-bash-navigation}

The part of the operating system responsible for managing files and directories
is called the **filesystem**.
It organizes our data into files,
which hold information,
and directories (also called "folders"),
which hold files or other directories.

Several commands are frequently used to create, inspect, rename, and delete files and directories.
To start exploring them, we'll go to our open Bash window.

First let's find out where we are by running a command called `pwd`
(which stands for "print working directory"). Directories are like *places* -- at any time
while we are using the shell we are in exactly one place, called
our **current working directory**. By default, commands mostly read and write files in the
current working directory, so knowing where you are before running
a command is important.

```shell
$ pwd
```

```text
/Users/amira
```

Here,
the computer's response is `/Users/amira`,
which is the path to Amira's **home directory**:

> **Home Directory Variation**
>
> The home directory path will look different on different operating systems.
> On Linux it may look like `/home/amira`,
> and on Windows it will be similar to `C:\Documents and Settings\amira` or
> `C:\Users\amira`.
> (Note that it may look slightly different for different versions of Windows.)
> In future examples, we've used Mac output as the default - Linux and Windows
> output may differ slightly, but should be generally similar.

To understand what a "home directory" is,
let's have a look at how the filesystem as a whole is organized. For the
sake of this example, we'll be
illustrating the filesystem on Amira's computer. After this
illustration, you'll be learning commands to explore your own filesystem,
which will be constructed in a similar way, but not be exactly identical.

On Amira's computer, the filesystem looks like this:

*TODO: Figure - The Filesystem [../fig/filesystem.svg](https://github.com/swcarpentry/shell-novice/blob/gh-pages/fig/filesystem.svg)*


At the top is the **root directory**
that holds everything else.
We refer to it using a slash character, `/`, on its own;
this is the leading slash in `/Users/amira`.

Inside that directory are several other directories:
`bin` (which is where some built-in programs are stored),
`data` (for miscellaneous data files),
`Users` (where users' personal directories are located),
`tmp` (for temporary files that don't need to be stored long-term),
and so on.

We know that our current working directory `/Users/amira` is stored inside `/Users`
because `/Users` is the first part of its name.
Similarly,
we know that `/Users` is stored inside the root directory `/`
because its name begins with `/`.

> **Slashes**
>
> Notice that there are two meanings for the `/` character.
> When it appears at the front of a file or directory name,
> it refers to the root directory. When it appears *inside* a name,
> it's just a separator. On Windows, backslashes (`\\ `) are used instead of
> forward slashes as directory separators in paths.

Underneath `/Users`,
we find one directory for each user with an account on this machine,
namely Amira, Jun and Sami.

*TODO: Figure - Home Directories [../fig/home-directories.svg](https://github.com/swcarpentry/shell-novice/blob/gh-pages/fig/home-directories.svg)*

The user Jun's files are stored in `/Users/jun`,
user Sami's in `/Users/sami`,
and Amira's in `/Users/amira`.  Because Amira is the logged in user in
our examples, `pwd` returns `/Users/amira` as our home directory.
Typically, when you open a new command prompt you will be in
your home directory to start.

Now let's learn the command that will let us see the contents of our
own filesystem.  We can see what's in our home directory by running `ls`,
which stands for "listing":

```shell
$ ls
```

```text
Applications Documents    Library      Music        Public
Desktop      Downloads    Movies       Pictures
```

(Again, your results may be slightly different depending on your operating
system and how you have customized your filesystem.)

`ls` prints the names of the files and directories in the current directory.
We can make its output more comprehensible by using the `-F` **option**
(also known as a **switch** or a **flag**) ,
which tells `ls` to classify the output
by adding a marker to file and directory names to indicate what they are:
- a trailing `/` indicates that this is a directory
- `*` indicates an executable (i.e. a program we can run)

Depending on your default options,
the shell might also use colors to indicate whether each entry is a file or
directory.

```shell
$ ls -F
```

```text
Applications/ Documents/    Library/      Music/        Public/
Desktop/      Downloads/    Movies/       Pictures/
```

Here,
we can see that our home directory contains mostly **sub-directories**.
Any names in your output that don't have a classification symbol
are plain old **files**.

### General syntax of a shell command

Consider the command below as a general example of a command,
which we will dissect into its component parts:

```shell
$ ls -F /
```

`ls` is the **command**, with an **option** `-F` and an **argument** `/`.
We've already encountered options (also called **switches** or **flags**) which
either start with a single dash (`-`) or two dashes (`--`), and they change the behaviour of a command.
Arguments often tell the command what to operate on (e.g. files and directories).
Technically, an option is a type of argument.
Arguments are referred to as **parameters** internally in the function during its execution.
Command can often be called without options and arguments
and with more than one option and more than one argument,

Each part is separated by spaces: if you omit the space
between `ls` and `-F` the shell will look for a command called `ls-F`, which
doesn't exist. Also, capitalization is important: `ls -r` is different to `ls -R`.

Putting all that together, our command above gives us a listing
of files and directories in the root directory `/`.
An example of the output you might get from the above command is given below:

```shell
$ ls -F /
Applications/         System/
Library/              Users/
Network/              Volumes/
```

### Getting help

`ls` has lots of other **options**. There are two common ways to find out how
to use a command and what options it accepts:

1. We can pass a `--help` option to the command, such as: `$ ls --help`

2. We can read its manual with `man`, such as: `$ man ls`

Depending on your environment you might find that only one of these works
(either `man` or `--help`).
We'll describe both ways below.

#### The `--help` option

Many Bash built-in commands, and programs that people have written that can be
run from within bash, support a `--help` option to display more
information on how to use the command or program.

```shell
$ ls --help
```

```text
Usage: ls [OPTION]... [FILE]...
List information about the FILEs (the current directory by default).
Sort entries alphabetically if none of -cftuvSUX nor --sort is specified.

Mandatory arguments to long options are mandatory for short options too.
  -a, --all                  do not ignore entries starting with .
  -A, --almost-all           do not list implied . and ..
      --author               with -l, print the author of each file
  -b, --escape               print C-style escapes for nongraphic characters
      --block-size=SIZE      scale sizes by SIZE before printing them; e.g.,
                               '--block-size=M' prints sizes in units of
                               1,048,576 bytes; see SIZE format below
  -B, --ignore-backups       do not list implied entries ending with ~
  -c                         with -lt: sort by, and show, ctime (time of last
                               modification of file status information);
                               with -l: show ctime and sort by name;
                               otherwise: sort by ctime, newest first
  -C                         list entries by columns
      --color[=WHEN]         colorize the output; WHEN can be 'always' (default
                               if omitted), 'auto', or 'never'; more info below
  -d, --directory            list directories themselves, not their contents
  -D, --dired                generate output designed for Emacs' dired mode
  -f                         do not sort, enable -aU, disable -ls --color
  -F, --classify             append indicator (one of */=>@|) to entries
      --file-type            likewise, except do not append '*'
      --format=WORD          across -x, commas -m, horizontal -x, long -l,
                               single-column -1, verbose -l, vertical -C
      --full-time            like -l --time-style=full-iso
  -g                         like -l, but do not list owner
      --group-directories-first
                             group directories before files;
                               can be augmented with a --sort option, but any
                               use of --sort=none (-U) disables grouping
  -G, --no-group             in a long listing, don't print group names
  -h, --human-readable       with -l and/or -s, print human readable sizes
                               (e.g., 1K 234M 2G)
      --si                   likewise, but use powers of 1000 not 1024
  -H, --dereference-command-line
                             follow symbolic links listed on the command line
      --dereference-command-line-symlink-to-dir
                             follow each command line symbolic link
                               that points to a directory
      --hide=PATTERN         do not list implied entries matching shell PATTERN
                               (overridden by -a or -A)
      --indicator-style=WORD  append indicator with style WORD to entry names:
                               none (default), slash (-p),
                               file-type (--file-type), classify (-F)
  -i, --inode                print the index number of each file
  -I, --ignore=PATTERN       do not list implied entries matching shell PATTERN
  -k, --kibibytes            default to 1024-byte blocks for disk usage
  -l                         use a long listing format
  -L, --dereference          when showing file information for a symbolic
                               link, show information for the file the link
                               references rather than for the link itself
  -m                         fill width with a comma separated list of entries
  -n, --numeric-uid-gid      like -l, but list numeric user and group IDs
  -N, --literal              print raw entry names (don't treat e.g. control
                               characters specially)
  -o                         like -l, but do not list group information
  -p, --indicator-style=slash
                             append / indicator to directories
  -q, --hide-control-chars   print ? instead of nongraphic characters
      --show-control-chars   show nongraphic characters as-is (the default,
                               unless program is 'ls' and output is a terminal)
  -Q, --quote-name           enclose entry names in double quotes
      --quoting-style=WORD   use quoting style WORD for entry names:
                               literal, locale, shell, shell-always,
                               shell-escape, shell-escape-always, c, escape
  -r, --reverse              reverse order while sorting
  -R, --recursive            list subdirectories recursively
  -s, --size                 print the allocated size of each file, in blocks
  -S                         sort by file size, largest first
      --sort=WORD            sort by WORD instead of name: none (-U), size (-S),
                               time (-t), version (-v), extension (-X)
      --time=WORD            with -l, show time as WORD instead of default
                               modification time: atime or access or use (-u);
                               ctime or status (-c); also use specified time
                               as sort key if --sort=time (newest first)
      --time-style=STYLE     with -l, show times using style STYLE:
                               full-iso, long-iso, iso, locale, or +FORMAT;
                               FORMAT is interpreted like in 'date'; if FORMAT
                               is FORMAT1<newline>FORMAT2, then FORMAT1 applies
                               to non-recent files and FORMAT2 to recent files;
                               if STYLE is prefixed with 'posix-', STYLE
                               takes effect only outside the POSIX locale
  -t                         sort by modification time, newest first
  -T, --tabsize=COLS         assume tab stops at each COLS instead of 8
  -u                         with -lt: sort by, and show, access time;
                               with -l: show access time and sort by name;
                               otherwise: sort by access time, newest first
  -U                         do not sort; list entries in directory order
  -v                         natural sort of (version) numbers within text
  -w, --width=COLS           set output width to COLS.  0 means no limit
  -x                         list entries by lines instead of by columns
  -X                         sort alphabetically by entry extension
  -Z, --context              print any security context of each file
  -1                         list one file per line.  Avoid '\n' with -q or -b
      --help     display this help and exit
      --version  output version information and exit

The SIZE argument is an integer and optional unit (example: 10K is 10*1024).
Units are K,M,G,T,P,E,Z,Y (powers of 1024) or KB,MB,... (powers of 1000).

Using color to distinguish file types is disabled both by default and
with --color=never.  With --color=auto, ls emits color codes only when
standard output is connected to a terminal.  The LS_COLORS environment
variable can change the settings.  Use the dircolors command to set it.

Exit status:
 0  if OK,
 1  if minor problems (e.g., cannot access subdirectory),
 2  if serious trouble (e.g., cannot access command-line argument).

GNU coreutils online help: <http://www.gnu.org/software/coreutils/>
Full documentation at: <http://www.gnu.org/software/coreutils/ls>
or available locally via: info '(coreutils) ls invocation'
```

> **Unsupported command-line options**
>
> If you try to use an option that is not supported, `ls` and other commands
> will usually print an error message similar to:
>
> ```shell
> $ ls -j
> ```
>
> ```text
> ls: invalid option -- 'j'
> Try 'ls --help' for more information.
> ```

#### The `man` command

The other way to learn about `ls` is to type
```shell
$ man ls
```

This will display a description
of the `ls` command and its options and some examples of how to use it.
Likewise, `man man` will show the manual for the `man` command.

To navigate through the `man` pages,
you may use <kbd>↑</kbd> and <kbd>↓</kbd> to move line-by-line,
or try <kbd>Ctrl+Spacebar</kbd> and <kbd>Spacebar</kbd> to skip up and down by a full page.
To search for a character or word in the `man` pages,
use <kbd>/</kbd> followed by the character or word you are searching for.
Sometimes a search will result in multiple hits.
If so, you can move between hits using <kbd>n</kbd>
(for moving forward) and <kbd>N</kbd> (for moving backward).

To quit the `man` pages, press <kbd>Q</kbd>.

> **Assistance on the web**
>
> Of course there is a third way to access help for commands:
> searching the internet via your web browser.
> In many cases the first results from your search will be
> [Stack Overflow](https://stackoverflow.com/questions/tagged/bash)
> pages where someone has already asked (and hopefully had answered)
> a question similar to yours.
>
> GNU provides links to its
> [manuals](http://www.gnu.org/manual/manual.html) including the
> [core GNU utilities](http://www.gnu.org/software/coreutils/manual/coreutils.html),
> which covers many commands introduced within this lesson.
> There's also a community effort known as the [TLDR pages](https://tldr.sh/)
> that aims to simplify the default man pages with practical examples.

### Looking around and moving about
`
We can also use `ls` to see the contents of a different directory.  Let's take a
look at our `Desktop` directory by running `ls -F Desktop`,
i.e.,
the command `ls` with the `-F` **option** and the **argument**  `Desktop`.
The argument `Desktop` tells `ls` that
we want a listing of something other than our current working directory:

```shell
$ ls -F Desktop
```

```text
data-shell/
```

Your output should be a list of all the files and sub-directories on your
Desktop, including the `data-shell` directory you downloaded at
the [setup for this lesson](http://swcarpentry.github.io/shell-novice/setup.html).
Take a look at your Desktop to confirm that your output is accurate.

Now that we know the `data-shell` directory is located on our Desktop, we
can do two things.
First, we can look at its contents, using the same strategy as before, passing
a directory name to `ls`:

```shell
$ ls -F Desktop/data-shell
```

```text
creatures/          molecules/          notes.txt           solar.pdf
data/               north-pacific-gyre/ pizza.cfg           writing/
```

Second, we can actually change our location to a different directory, so
we are no longer located in
our home directory.

The command to change locations is `cd` followed by a
directory name to change our working directory.
`cd` stands for "change directory",
which is a bit misleading:
the command doesn't change the directory,
it changes the shell's idea of what directory we are in.

Let's say we want to move to the `data` directory we saw above.  We can
use the following series of commands to get there:

```shell
$ cd Desktop
$ cd data-shell
$ cd data
```

These commands will move us from our home directory onto our Desktop, then into
the `data-shell` directory, then into the `data` directory.
You will notice that `cd` doesn't print anything.
This is normal.
Many shell commands will not output anything to the screen when successfully executed.
But if we run `pwd` after it, we can see that we are now in `/Users/amira/Desktop/data-shell/data`.
If we run `ls` without arguments now,
it lists the contents of `/Users/amira/Desktop/data-shell/data`,
because that's where we now are:

```shell
$ pwd
```

```text
/Users/amira/Desktop/data-shell/data
```

```shell
$ ls -F
```

```text
amino-acids.txt   elements/     pdb/	        salmon.txt
animals.txt       morse.txt     planets.txt     sunspot.txt
```

We now know how to go down the directory tree, but
how do we go up?  We might try the following:

```shell
$ cd data-shell
```

```text
-bash: cd: data-shell: No such file or directory
```

But we get an error!  Why is this?

With our methods so far,
`cd` can only see sub-directories inside your current directory.  There are
different ways to see directories above your current location; we'll start
with the simplest.

There is a shortcut in the shell to move up one directory level
that looks like this:

```shell
$ cd ..
```

`..` is a special directory name meaning
"the directory containing this one",
or more succinctly,
the **parent** of the current directory or the directory **above**.
Sure enough,
if we run `pwd` after running `cd ..`, we're back in `/Users/amira/Desktop/data-shell`:

```shell
$ pwd
```

```text
/Users/amira/Desktop/data-shell
```

The special directory `..` doesn't usually show up when we run `ls`.  If we want
to display it, we can give `ls` the `-a` option:

```shell
$ ls -F -a
```

```text
./   .bash_profile  data/       north-pacific-gyre/  pizza.cfg  thesis/
../  creatures/     molecules/  notes.txt            solar.pdf  writing/
```

`-a` stands for "show all";
it forces `ls` to show us file and directory names that begin with `.`,
such as `..` (which, if we're in `/Users/amira`, refers to the `/Users` directory)
As you can see,
it also displays another special directory that's just called `.`,
which means "the current working directory".
It may seem redundant to have a name for it,
but we'll see some uses for it soon.

Note that in most command line tools, multiple options can be combined
with a single `-` and no spaces between the options: `ls -F -a` is
equivalent to `ls -Fa`.

> **Other Hidden Files**
>
> In addition to the hidden directories `..` and `.`, you may also see a file
> called `.bash_profile`. This file usually contains shell configuration
> settings. You may also see other files and directories beginning
> with `.`. These are usually files and directories that are used to configure
> different programs on your computer. The prefix `.` is used to prevent these
> configuration files from cluttering the terminal when a standard `ls` command
> is used.

> **Orthogonality**
>
> The special names `.` and `..` don't belong to `cd`;
> they are interpreted the same way by every program.
> For example,
> if we are in `/Users/amira/data`,
> the command `ls ..` will give us a listing of `/Users/amira`.
> When the meanings of the parts are the same no matter how they're combined,
> programmers say they are **orthogonal**:
> Orthogonal systems tend to be easier for people to learn
> because there are fewer special cases and exceptions to keep track of.
>
> **Moving beyond the parent directory**
>
> Several `..` can be joined by the path separator to move higher than the
> parent directory. To move two directories up, you would type `cd ../..`. This
> means you can also move to a directory on the same level at the current
> directory. For example, to move from `/Users/amira/data` to
> `/Users/amira/molecules` you can type `cd ../molecules`.

So these are the basic commands for navigating the filesystem on your computer:
`pwd`, `ls` and `cd`.  Let's explore some variations on those commands.
What happens if you type `cd` on its own, without giving a directory?

```shell
$ cd
```

How can you check what happened? `pwd` gives us the answer:

```shell
$ pwd
```

```text
/Users/amira
```

It turns out that `cd` without an argument will return you to your home directory,
which is great if you've gotten lost in your own filesystem.

Let's try returning to the `data` directory from before.  Last time, we used
three commands, but we can actually string together the list of directories
to move to `data` in one step:

```shell
$ cd Desktop/data-shell/data
```

Check that we've moved to the right place by running `pwd` and `ls -F`

If we want to move up one level from the data directory, we could use `cd ..`.  But
there is another way to move to any directory, regardless of your
current location.

So far, when specifying directory names, or even a directory path (as above),
we have been using **relative paths**.  When you use a relative path with a command
like `ls` or `cd`, it tries to find that location  from where we are,
rather than from the root of the filesystem.

However, it is possible to specify the **absolute path** to a directory by
including its entire path from the root directory, which is indicated by a
leading slash.  The leading `/` tells the computer to follow the path from
the root of the filesystem, so it always refers to exactly one directory,
no matter where we are when we run the command.

This allows us to move to our `data-shell` directory from anywhere on
the filesystem (including from inside `data`).  To find the absolute path
we're looking for, we can use `pwd` and then extract the piece we need
to move to `data-shell`.

```shell
$ pwd
```

```text
/Users/amira/Desktop/data-shell/data
```

```shell
$ cd /Users/amira/Desktop/data-shell
```

Run `pwd` and `ls -F` to ensure that we're in the directory we expect.

> **Two More Shortcuts**
>
> The shell interprets the character `~` (tilde) at the start of a path to
> mean "the current user's home directory". For example, if Amira's home
> directory is `/Users/amira`, then `~/data` is equivalent to
> `/Users/amira/data`. This only works if it is the first character in the
> path: `here/there/~/elsewhere` is *not* `here/there/Users/amira/elsewhere`.
>
> Another shortcut is the `-` (dash) character.  `cd` will translate `-` into
> *the previous directory you were in*, which is faster than having to remember,
> then type, the full path.  This is a *very* efficient way of moving back
> and forth between directories. The difference between `cd ..` and `cd -` is
> that the former brings you *up*, while the latter brings you *back*. You can
> think of it as the *Last Channel* button on a TV remote.


## Working With Files and Directories {#rse-bash-working}

### Creating directories 
 
We now know how to explore files and directories,
but how do we create them in the first place?

Step one: see where we are and what we already have.
Let's go back to our `data-shell` directory on the Desktop
and use `ls -F` to see what it contains:

```shell
$ pwd
```

```text
/Users/amira/Desktop/data-shell
```

```shell
$ ls -F
```

```text
creatures/  data/  molecules/  north-pacific-gyre/  notes.txt  pizza.cfg  solar.pdf  writing/
```

Let's create a new directory called `thesis` using the command `mkdir thesis`
(which has no output):

```shell
$ mkdir thesis
```

As you might guess from its name,
`mkdir` means "make directory".
Since `thesis` is a relative path
(i.e., does not have a leading slash, like `/what/ever/thesis`),
the new directory is created in the current working directory:

```shell
$ ls -F
```

```text
creatures/  data/  molecules/  north-pacific-gyre/  notes.txt  pizza.cfg  solar.pdf  thesis/  writing/
```

> **Two ways of doing the same thing**
>
> Using the shell to create a directory is no different than using a file explorer.
> If you open the current directory using your operating system's graphical file explorer,
> the `thesis` directory will appear there too.
> While the shell and the file explorer are two different ways of interacting with the files,
> the files and directories themselves are the same.


> **Good names for files and directories**
>
> Complicated names of files and directories can make your life painful
> when working on the command line. Here we provide a few useful
> tips for the names of your files.
>
> 1. Don't use spaces.
>
>    Spaces can make a name more meaningful,
>    but since spaces are used to separate arguments on the command line
>    it is better to avoid them in names of files and directories.
>    You can use `-` or `_` instead (e.g. `north-pacific-gyre/` rather than `north pacific gyre/`).
>
> 2. Don't begin the name with `-` (dash).
>
>    Commands treat names starting with `-` as options.
>
> 3. Stick with letters, numbers, `.` (period or 'full stop'), `-` (dash) and `_` (underscore).
>
>    Many other characters have special meanings on the command line.
>    We will learn about some of these during this lesson.
>    There are special characters that can cause your command to not work as
>    expected and can even result in data loss.
>
> If you need to refer to names of files or directories that have spaces
> or other special characters, you should surround the name in quotes (`""`).


Since we've just created the `thesis` directory, there's nothing in it yet:

```shell
$ ls -F thesis
```

### Create a text file

Let's change our working directory to `thesis` using `cd`,
then run a text editor called Nano to create a file called `draft.txt`:

```shell
$ cd thesis
$ nano draft.txt
```

We could have created this file without using `cd` to first change our current
directory, and instead directly have typed `nano thesis/draft.txt`.

> **Which Editor?**
>
> When we say, "`nano` is a text editor," we really do mean "text": it can
> only work with plain character data, not spreadsheets, images, Word documents
> (these are saved as XML-files, a markup language similar to HTML which makes
> them difficult to edit outside a Word editor), or
> any other format. We use it in examples because it is one of the
> least complex and readily available text editors. However, because of this trait, it may 
> not be powerful enough or flexible enough for the work you need to do
> after this workshop. On Unix systems (such as Linux and Mac OS X),
> many programmers use [Emacs](http://www.gnu.org/software/emacs/) or
> [Vim](http://www.vim.org/) (both of which require more time to learn),
> or a graphical editor such as
> [Sublime Text](https://www.sublimetext.com/). On Windows, a popular editor is
> [Notepad++](http://notepad-plus-plus.org/). Windows also has a built-in
> editor called `notepad` and macOS has `TextEdit` which both can be run from
> the command line (`start notepad <file_name>` and `open -e <file_name>`,
> respectively).
>
> No matter what editor you use, you will need to know where it searches
> for and saves files. If you start it from the shell, it will (probably)
> use your current working directory as its default location. If you use
> your computer's start menu, it may want to save files in your desktop or
> documents directory instead. You can change this by navigating to
> another directory the first time you "Save As..."


Let's type in a few lines of text.
Once we're happy with our text,
we can press <kbd>Ctrl</kbd>+<kbd>O</kbd>
(press the Ctrl or Control key and, while holding it down, press the O key)
to write our data to disk
(we'll be asked what file we want to save this to:
press <kbd>Return</kbd> to accept the suggested default of `draft.txt`).

*TODO: Figure - Nano in Action [../fig/nano-screenshot.png](https://github.com/swcarpentry/shell-novice/blob/gh-pages/fig/nano-screenshot.png)*

Once our file is saved, we can use `Ctrl-X` to quit the editor and
return to the shell.

> **Control, Ctrl, or ^ Key**
>
> The Control key is also called the "Ctrl" key. There are various ways
> in which using the Control key may be described. For example, you may
> see an instruction to press the Control key and, while holding it down,
> press the X key, described as any of:
>
> * `Control-X`
> * `Control+X`
> * `Ctrl-X`
> * `Ctrl+X`
> * `^X`
> * `C-x`
>
> In nano, along the bottom of the screen you'll see `^G Get Help ^O WriteOut`.
> This means that you can use `Control-G` to get help and `Control-O` to save your
> file.

`nano` doesn't leave any output on the screen after it exits,
but `ls` now shows that we have created a file called `draft.txt`:

```shell
$ ls
```

```text
draft.txt
```

> **What's In A Name?**
>
> You may have noticed that all of Amira's files are named "something dot
> something", and in this part of the lesson, we always used the extension
> `.txt`.  This is just a convention: we can call a file `mythesis` or
> almost anything else we want. However, most people use two-part names
> most of the time to help them (and their programs) tell different kinds
> of files apart. The second part of such a name is called the
> **filename extension**, and indicates
> what type of data the file holds: `.txt` signals a plain text file, `.pdf`
> indicates a PDF document, `.cfg` is a configuration file full of parameters
> for some program or other, `.png` is a PNG image, and so on.
>
> This convention is important as it can give us hints of what is expected to
> be in a files. Files contain
> bytes: it's up to us and our programs to interpret those bytes
> according to the rules for plain text files, PDF documents, configuration
> files, images, and so on.
>
> Naming a PNG image of a whale as `whale.mp3` doesn't somehow
> magically turn it into a recording of whalesong, though it *might*
> cause the operating system to try to open it with a music player
> when someone double-clicks it.

### Moving files and directories

Returning to the `data-shell` directory,

```shell
cd ~/Desktop/data-shell/
```

In our `thesis` directory we have a file `draft.txt`
which isn't a particularly informative name,
so let's change the file's name using `mv`,
which is short for "move":

```shell
$ mv thesis/draft.txt thesis/quotes.txt
```

The first argument tells `mv` what we're "moving",
while the second is where it's to go.
In this case,
we're moving `thesis/draft.txt` to `thesis/quotes.txt`,
which has the same effect as renaming the file.
A filename is just a pointer to a location on the storage drive where the bytes of that file are kept,
so all that `mv` does is to change the name of the pointer.
The physical location of the file's bytes on the drive remains the same as long as the
new pointer name is on the same drive as where the bytes are stored.
Only if the new pointer name is on a removable storage media such as flash drive,
the bytes need to be copied from the internal computer drive and their original
space on the computer drive is marked as a writable place to put the bytes of new files).
Sure enough,
`ls` shows us that `thesis` now contains one file called `quotes.txt`:

```shell
$ ls thesis
```

```text
quotes.txt
```

One has to be careful when specifying the target file name, since `mv` will
silently overwrite any existing file with the same name, which could
lead to data loss. An additional option, `mv -i` (or `mv --interactive`),
can be used to make `mv` ask you for confirmation before overwriting.
`mv` also works on directories.

Let's move `quotes.txt` into the current working directory.
We use `mv` once again,
but this time we'll just use the name of a directory as the second argument
to tell `mv` that we want to keep the filename,
but put the file somewhere new.
(This is why the command is called "move".)
In this case,
the directory name we use is the special directory name `.` that we mentioned earlier.

```shell
$ mv thesis/quotes.txt .
```

The effect is to move the file from the directory it was in to the current working directory.
`ls` now shows us that `thesis` is empty:

```shell
$ ls thesis
```

Further,
`ls` with a filename or directory name as an argument only lists that file or directory.
We can use this to see that `quotes.txt` is still in our current directory:

```shell
$ ls quotes.txt
```

```text
quotes.txt
```

### Copying files and directories

The `cp` command works very much like `mv`,
except it copies a file instead of moving it.
We can check that it did the right thing using `ls`
with two paths as arguments --- like most Unix commands,
`ls` can be given multiple paths at once:

```shell
$ cp quotes.txt thesis/quotations.txt
$ ls quotes.txt thesis/quotations.txt
```

```text
quotes.txt   thesis/quotations.txt
```

We can also copy a directory and all its contents by using the
[recursive](https://en.wikipedia.org/wiki/Recursion) option `-r`,
e.g. to back up a directory:

```shell
$ cp -r thesis thesis_backup
```

We can check the result by listing the contents of both the `thesis` and `thesis_backup` directory:

```shell
$ ls thesis thesis_backup
```

```text
thesis:
quotations.txt

thesis_backup:
quotations.txt
```


### Removing files and directories

Returning to the `data-shell` directory,
let's tidy up this directory by removing the `quotes.txt` file we created.
The Unix command we'll use for this is `rm` (short for 'remove'):

```shell
$ rm quotes.txt
```

We can confirm the file has gone using `ls`:

```shell
$ ls quotes.txt
```

```text
ls: cannot access 'quotes.txt': No such file or directory
```

> **Deleting Is Forever**
>
> The Unix shell doesn't have a trash bin that we can recover deleted
> files from (though most graphical interfaces to Unix do).  Instead,
> when we delete files, they are unlinked from the filesystem so that
> their storage space on disk can be recycled. Tools for finding and
> recovering deleted files do exist, but there's no guarantee they'll
> work in any particular situation, since the computer may recycle the
> file's disk space right away.

If we try to remove the `thesis` directory using `rm thesis`,
we get an error message:

```shell
$ rm thesis
```

```text
rm: cannot remove `thesis': Is a directory
```

This happens because `rm` by default only works on files, not directories.

`rm` can remove a directory *and all its contents* if we use the 
recursive option `-r`, and it will do so *without any confirmation prompts*:

```shell
$ rm -r thesis
```

Given that there is no way to retrieve files deleted using the shell,
`rm -r` *should be used with great caution* (you might consider adding the interactive option `rm -r -i`).

> **Verbose output**
>
> To see what happens when we run an `rm` command,
> we can append the verbose `-v`.
> This will print each deleted filename,
> which can be immensely helpful when something goes awry.
> The `-v` flag works the same with `mv` and `cp`.

### Operations with multiple files and directories

Oftentimes one needs to copy or move several files at once.
This can be done by providing a list of individual filenames,
or specifying a naming pattern using wildcards.

**Wildcards**
>
> `*` is a **wildcard**, which matches zero or more characters.
> Let's consider the `data-shell/molecules` directory:
> `*.pdb` matches `ethane.pdb`, `propane.pdb`, and every
> file that ends with '.pdb'. On the other hand, `p*.pdb` only matches
> `pentane.pdb` and `propane.pdb`, because the 'p' at the front only
> matches filenames that begin with the letter 'p'.
>
> `?` is also a wildcard, but it matches exactly one character.
> So `?ethane.pdb` would match `methane.pdb` whereas
> `*ethane.pdb` matches both `ethane.pdb`, and `methane.pdb`.
>
> Wildcards can be used in combination with each other
> e.g. `???ane.pdb` matches three characters followed by `ane.pdb`,
> giving `cubane.pdb  ethane.pdb  octane.pdb`.
>
> When the shell sees a wildcard, it expands the wildcard to create a
> list of matching filenames *before* running the command that was
> asked for. As an exception, if a wildcard expression does not match
> any file, Bash will pass the expression as an argument to the command
> as it is. For example typing `ls *.pdf` in the `molecules` directory
> (which contains only files with names ending with `.pdb`) results in
> an error message that there is no file called `*.pdf`.
> However, generally commands like `ls` see the lists of
> file names matching these expressions, but not the wildcards
> themselves. It is the shell, not the other programs, that deals with
> expanding wildcards, and this is another example of orthogonal design.

## Pipes and Filters {#rse-bash-pipes}

Now that we know a few basic commands,
we can finally look at one of the shell's most powerful features:
the ease with which it lets us combine existing programs in new ways.
We'll start with `molecules` directory
that contains six files describing some simple organic molecules.
The `.pdb` extension indicates that these files are in Protein Data Bank format,
a simple text format that specifies the type and position of each atom in the molecule.

```shell
$ ls molecules
```

```text
cubane.pdb    ethane.pdb    methane.pdb
octane.pdb    pentane.pdb   propane.pdb
```

Let's go into that directory with `cd` and run the command `wc *.pdb`.
`wc` is the "word count" command:
it counts the number of lines, words, and characters in files (from left to right, in that order).

The `*` in `*.pdb` matches zero or more characters,
so the shell turns `*.pdb` into a list of all `.pdb` files in the current directory:

```shell
$ cd molecules
$ wc *.pdb
```

```text
  20  156  1158  cubane.pdb
  12  84   622   ethane.pdb
   9  57   422   methane.pdb
  30  246  1828  octane.pdb
  21  165  1226  pentane.pdb
  15  111  825   propane.pdb
 107  819  6081  total
```

If we run `wc -l` instead of just `wc`,
the output shows only the number of lines per file:

```shell
$ wc -l *.pdb
```

```text
  20  cubane.pdb
  12  ethane.pdb
   9  methane.pdb
  30  octane.pdb
  21  pentane.pdb
  15  propane.pdb
 107  total
```

> **Why Isn't It Doing Anything?**
>
> What happens if a command is supposed to process a file, but we
> don't give it a filename? For example, what if we type:
>
> ```shell
> $ wc -l
> ```
>
> but don't type `*.pdb` (or anything else) after the command?
> Since it doesn't have any filenames, `wc` assumes it is supposed to
> process standard input, so it just sits there and waits for us to give
> it some data interactively. From the outside, all we see is it
> sitting there: the command doesn't appear to do anything.
>
> If you make this kind of mistake, you can escape out of this state by holding down
> the control key (<kbd>Ctrl</kbd>) and typing the letter <kbd>C</kbd>
> once and letting go of the <kbd>Ctrl</kbd> key.
> <kbd>Ctrl</kbd>+<kbd>C</kbd>

We can also use `-w` to get only the number of words,
or `-c` to get only the number of characters.

Which of these files contains the fewest lines?
It's an easy question to answer when there are only six files,
but what if there were 6000?
Our first step toward a solution is to run the command:

```shell
$ wc -l *.pdb > lengths.txt
```

The greater than symbol, `>`, tells the shell to **redirect** the command's output
to a file instead of printing it to the screen. (This is why there is no screen output:
everything that `wc` would have printed has gone into the
file `lengths.txt` instead.)  The shell will create
the file if it doesn't exist. If the file exists, it will be
silently overwritten, which may lead to data loss and thus requires
some caution.
`ls lengths.txt` confirms that the file exists:

```shell
$ ls lengths.txt
```

```text
lengths.txt
```

We can now send the content of `lengths.txt` to the screen using `cat lengths.txt`.
`cat` stands for "concatenate":
it prints the contents of files one after another.
There's only one file in this case,
so `cat` just shows us what it contains:

```shell
$ cat lengths.txt
```

```text
  20  cubane.pdb
  12  ethane.pdb
   9  methane.pdb
  30  octane.pdb
  21  pentane.pdb
  15  propane.pdb
 107  total
```

> **Output Page by Page**
>
> We'll continue to use `cat` in this lesson, for convenience and consistency,
> but it has the disadvantage that it always dumps the whole file onto your screen.
> More useful in practice is the command `less`,
> which you use with `less lengths.txt`.
> This displays a screenful of the file, and then stops.
> Just like in the man pages, you can go forward one screenful by pressing
> `space` or `f`,
> or back one by pressing `ctrl+space` or `b`.
> Press `/` to search, `n` and `N` to navigate between search hits,
> and `q` to quit `less`.

Now let's use the `sort` command to sort its contents.

We will also use the `-n` option to specify that the sort is
numerical instead of alphanumerical
(e.g. the name `10.txt` should be sorted after `2.txt` even though the first character `1` comes before `2`).
This does *not* change the file;
instead, it sends the sorted result to the standard output which is printed to the screen:

```shell
$ sort -n lengths.txt
```

```text
  9  methane.pdb
 12  ethane.pdb
 15  propane.pdb
 20  cubane.pdb
 21  pentane.pdb
 30  octane.pdb
107  total
```

We can put the sorted list of lines in another temporary file called `sorted-lengths.txt`
by putting `> sorted-lengths.txt` after the command,
just as we used `> lengths.txt` to put the output of `wc` into `lengths.txt`.
Once we've done that,
we can run another command called `head` to get the first few lines in `sorted-lengths.txt`:

```shell
$ sort -n lengths.txt > sorted-lengths.txt
$ head -n 1 sorted-lengths.txt
```

```text
  9  methane.pdb
```

Using `-n 1` with `head` tells it that
we only want the first line of the file;
`-n 20` would get the first 20,
and so on.
Since `sorted-lengths.txt` contains the lengths of our files ordered from least to greatest,
the output of `head` must be the file with the fewest lines.

> **Redirecting to the same file**
>
> It's a very bad idea to try redirecting
> the output of a command that operates on a file
> to the same file. For example:
>
> ```shell
> $ sort -n lengths.txt > lengths.txt
> ```
>
> Doing something like this may give you
> incorrect results and/or delete
> the contents of `lengths.txt`.

If you think this is confusing,
you're in good company:
even once you understand what `wc`, `sort`, and `head` do,
all those intermediate files make it hard to follow what's going on.
We can make it easier to understand by running `sort` and `head` together:

```shell
$ sort -n lengths.txt | head -n 1
```

```text
  9  methane.pdb
```

The vertical bar, `|`, between the two commands is called a **pipe**.
It tells the shell that we want to use
the output of the command on the left
as the input to the command on the right.
The computer might create a temporary file if it needs to,
or copy data from one program to the other in memory,
or something else entirely;
we don't have to know or care.

Nothing prevents us from chaining pipes consecutively.
That is, we can for example send the output of `wc` directly to `sort`,
and then the resulting output to `head`.
Thus we first use a pipe to send the output of `wc` to `sort`:

```shell
$ wc -l *.pdb | sort -n
```

```text
   9 methane.pdb
  12 ethane.pdb
  15 propane.pdb
  20 cubane.pdb
  21 pentane.pdb
  30 octane.pdb
 107 total
```

And now we send the output of this pipe, through another pipe, to `head`, so that the full pipeline becomes:

```shell
$ wc -l *.pdb | sort -n | head -n 1
```

```text
   9  methane.pdb
```

This is exactly like a mathematician nesting functions like *log(3x)*
and saying "the log of three times *x*".
In our case,
the calculation is "head of sort of line count of `*.pdb`".

Here's what actually happens behind the scenes when we create a pipe.
When a computer runs a program --- any program --- it creates a **process**
in memory to hold the program's software and its current state.
Every process has an input channel called **standard input**.
(By this point, you may be surprised that the name is so memorable, but don't worry:
most Unix programmers call it "stdin").
Every process also has a default output channel called **standard output**
(or "stdout"). A second output channel called **standard error** (stderr) also
exists. This channel is typically used for error or diagnostic messages, and it
allows a user to pipe the output of one program into another while still receiving
error messages in the terminal.

The shell is actually just another program.
Under normal circumstances,
whatever we type on the keyboard is sent to the shell on its standard input,
and whatever it produces on standard output is displayed on our screen.
When we tell the shell to run a program,
it creates a new process
and temporarily sends whatever we type on our keyboard to that process's standard input,
and whatever the process sends to standard output to the screen.

When we run `wc -l *.pdb > lengths.txt`,
the shell tells the computer to create a new process to run the `wc` program.
Since we've provided some filenames as arguments,
`wc` reads from them instead of from standard input.
And since we've used `>` to redirect output to a file,
the shell connects the process's standard output to that file.

If we run `wc -l *.pdb | sort -n` instead,
the shell creates two processes
(one for each process in the pipe)
so that `wc` and `sort` run simultaneously.
The standard output of `wc` is fed directly to the standard input of `sort`;
since there's no redirection with `>`,
`sort`'s output goes to the screen.
And if we run `wc -l *.pdb | sort -n | head -n 1`,
we get three processes with data flowing from the files,
through `wc` to `sort`,
and from `sort` through `head` to the screen.

*TODO: Figure - Redirects and Pipes [../fig/redirects-and-pipes.png](https://github.com/swcarpentry/shell-novice/blob/gh-pages/fig/redirects-and-pipes.svg)*

This simple idea is why Unix has been so successful.
Instead of creating enormous programs that try to do many different things,
Unix programmers focus on creating lots of simple tools that each do one job well,
and that work well with each other.
This programming model is called "pipes and filters".
We've already seen pipes;
a **filter** is a program like `wc` or `sort`
that transforms a stream of input into a stream of output.
Almost all of the standard Unix tools can work this way:
unless told to do otherwise,
they read from standard input,
do something with what they've read,
and write to standard output.

The key is that any program that reads lines of text from standard input
and writes lines of text to standard output
can be combined with every other program that behaves this way as well.
You can *and should* write your programs this way
so that you can put those programs into pipes to multiply their power.

## Loops {#rse-bash-loops}

**Loops** are a programming construct which allow us to repeat a command or set of commands
for each item in a list.
As such they are key to productivity improvements through automation.
Similar to wildcards and tab completion, using loops also reduces the
amount of typing required (and hence reduces the number of typing mistakes).

For this example,
we'll use the `creatures` directory which contains genome data files of mythological creatures.
The directory only has two example files,
but the principles can be applied to many more files at once.
We would like to print out the classification for each species, which is given on the second line of the file.
For each file, we would need to execute the command `head -n 2` and pipe this to `tail -n 1`.
We'll use a loop to solve this problem, but first let's look at the general form of a loop:

```shell
for thing in list_of_things
do
    operation_using $thing    # Indentation within the loop is not required, but aids legibility
done
```

and we can apply this to our example like this:

```shell
$ for filename in basilisk.dat unicorn.dat
> do
>    head -n 2 $filename | tail -n 1
> done
```

```text
CLASSIFICATION: basiliscus vulgaris
CLASSIFICATION: equus monoceros
```

> **Follow the Prompt**
>
> The shell prompt changes from `$` to `>` and back again as we were
> typing in our loop. The second prompt, `>`, is different to remind
> us that we haven't finished typing a complete command yet. A semicolon, `;`,
> can be used to separate two commands written on a single line.

When the shell sees the keyword `for`,
it knows to repeat a command (or group of commands) once for each item in a list.
Each time the loop runs (called an iteration), an item in the list is assigned in sequence to
the **variable**, and the commands inside the loop are executed, before moving on to
the next item in the list.
Inside the loop,
we call for the variable's value by putting `$` in front of it.
The `$` tells the shell interpreter to treat
the variable as a variable name and substitute its value in its place,
rather than treat it as text or an external command.

In this example, the list is two filenames: `basilisk.dat` and `unicorn.dat`.
Each time the loop iterates, it will assign a file name to the variable `filename`
and run the `head` command.
The first time through the loop,
`$filename` is `basilisk.dat`.
The interpreter runs the command `head` on `basilisk.dat`,
and then prints the
first three lines of `basilisk.dat`.
For the second iteration, `$filename` becomes
`unicorn.dat`. This time, the shell runs `head` on `unicorn.dat`
and prints the first three lines of `unicorn.dat`.
Since the list was only two items, the shell exits the `for` loop.

> **Same Symbols, Different Meanings**
>
> Here we see `>` being used as a shell prompt, whereas `>` is also
> used to redirect output.
> Similarly, `$` is used as a shell prompt, but, as we saw earlier,
> it is also used to ask the shell to get the value of a variable.
>
> If the *shell* prints `>` or `$` then it expects you to type something,
> and the symbol is a prompt.
>
> If *you* type `>` or `$` yourself, it is an instruction from you that
> the shell should redirect output or get the value of a variable.

When using variables it is also
possible to put the names into curly braces to clearly delimit the variable
name: `$filename` is equivalent to `${filename}`, but is different from
`${file}name` (the latter appends "name" to the `$file` variable).
You may find this notation in other people's programs.

We have called the variable in this loop `filename`
in order to make its purpose clearer to human readers.
The shell itself doesn't care what the variable is called;
if we wrote this loop as:

```shell
$ for x in basilisk.dat unicorn.dat
> do
>    head -n 2 $x | tail -n 1
> done
```

or:

```shell
$ for temperature in basilisk.dat unicorn.dat
> do
>    head -n 2 $temperature | tail -n 1
> done
```

it would work exactly the same way.
*Don't do this.*
Programs are only useful if people can understand them,
so meaningless names (like `x`) or misleading names (like `temperature`)
increase the odds that the program won't do what its readers think it does.

Let's continue with our example in the `data-shell/creatures` directory.
Here's a slightly more complicated loop:

```shell
$ for filename in *.dat
> do
>     echo $filename
>     head -n 100 $filename | tail -n 20
> done
```

The shell starts by expanding `*.dat` to create the list of files it will process.
The **loop body**
then executes two commands for each of those files.
The first, `echo`, just prints its command-line arguments to standard output.
For example:

```shell
$ echo hello there
```

prints:

```text
hello there
```

In this case,
since the shell expands `$filename` to be the name of a file,
`echo $filename` just prints the name of the file.
Note that we can't write this as:

```shell
$ for filename in *.dat
> do
>     $filename
>     head -n 100 $filename | tail -n 20
> done
```

because then the first time through the loop,
when `$filename` expanded to `basilisk.dat`, the shell would try to run `basilisk.dat` as a program.
Finally,
the `head` and `tail` combination selects lines 81-100
from whatever file is being processed
(assuming the file has at least 100 lines).

> **Spaces in Names**
>
> Spaces are used to separate the elements of the list
> that we are going to loop over. If one of those elements
> contains a space character, we need to surround it with
> quotes, and do the same thing to our loop variable.
> Suppose our data files are named:
>
> ```text
> red dragon.dat
> purple unicorn.dat
> ```
>
> To loop over these files, we would need to add double quotes like so:
>
> ```shell
> $ for filename in "red dragon.dat" "purple unicorn.dat"
> do
>     head -n 100 "$filename" | tail -n 20
> done
> ```
>
> It is simpler just to avoid using spaces (or other special characters) in filenames.
>
> The files above don't exist, so if we run the above code, the `head` command will be unable
> to find them, however the error message returned will show the name of the files it is
> expecting:
> ```text
> head: cannot open ‘red dragon.dat’ for reading: No such file or directory
> head: cannot open ‘purple unicorn.dat’ for reading: No such file or directory
> ```
>
> Try removing the quotes around `$filename` in the loop above
> to see the effect of the quote marks on spaces.
> Note that we get a result from the loop command for unicorn.dat
> when we run this code in the `creatures` directory:
>
> ```text
> head: cannot open ‘red’ for reading: No such file or directory
> head: cannot open ‘dragon.dat’ for reading: No such file or directory
> head: cannot open ‘purple’ for reading: No such file or directory
> CGGTACCGAA
> AAGGGTCGCG
> CAAGTGTTCC
> ```

We would like to modify each of the files in `data-shell/creatures`, but also save a version
of the original files, naming the copies `original-basilisk.dat` and `original-unicorn.dat`.
We can't use:

```shell
$ cp *.dat original-*.dat
```

because that would expand to:

```shell
$ cp basilisk.dat unicorn.dat original-*.dat
```

This wouldn't back up our files, instead we get an error:

```text
cp: target `original-*.dat' is not a directory
```

This problem arises when `cp` receives more than two inputs. When this happens, it
expects the last input to be a directory where it can copy all the files it was passed.
Since there is no directory named `original-*.dat` in the `creatures` directory we get an
error.

Instead, we can use a loop:

```shell
$ for filename in *.dat
> do
>     cp $filename original-$filename
> done
```

This loop runs the `cp` command once for each filename.
The first time,
when `$filename` expands to `basilisk.dat`,
the shell executes:

```shell
cp basilisk.dat original-basilisk.dat
```

The second time, the command is:

```shell
cp unicorn.dat original-unicorn.dat
```

Before running a loop,
it can be a good idea to check that the loop is doing the correct thing.
We learned earlier how to print strings using `echo`,
and we can modify the loop
to use `echo` to print our commands without actually executing them.
As such we can check what commands *would be* run in the unmodified loop.

The following diagram
shows what happens when the modified loop is executed, and demonstrates how the
judicious use of `echo` is a good debugging technique.

*TODO: Figure - For Loop in Action [../fig/shell_script_for_loop_flow_chart.svg](https://github.com/swcarpentry/shell-novice/blob/gh-pages/fig/shell_script_for_loop_flow_chart.svg)*


> **Those Who Know History Can Choose to Repeat It**
>
> Another way to repeat previous work is to use the `history` command to
> get a list of the last few hundred commands that have been executed, and
> then to use `!123` (where "123" is replaced by the desired command number) to
> repeat one of those commands. For example, if Amira types this:
>
> ```shell
> $ history | tail -n 5
> ```
>
> ```shell
>   456  ls -l NENE0*.txt
>   457  rm stats-NENE01729B.txt.txt
>   458  bash goostats NENE01729B.txt stats-NENE01729B.txt
>   459  ls -l NENE0*.txt
>   460  history
> ```
>
> then she can re-run `goostats` on `NENE01729B.txt` simply by typing
> `!458`.

> **Other History Commands**
>
> There are a number of other shortcut commands for getting at the history.
>
> - `Ctrl-R` enters a history search mode "reverse-i-search" and finds the
> most recent command in your history that matches the text you enter next.
> Press `Ctrl-R` one or more additional times to search for earlier matches.
> - `!!` retrieves the immediately preceding command
> (you may or may not find this more convenient than using the up-arrow
> one common usecase is to preface a previous command, e.g. `sudo !!`)
> - `!!` is shorthand for `!-1`, so `!-2` would go back two steps in history instead of one.
> - `!$` retrieves the last word of the last command.
> That's useful more often than you might expect: after
> `bash goostats NENE01729B.txt stats-NENE01729B.txt`, you can type
> `less !$` to look at the file `stats-NENE01729B.txt`, which is
> quicker than doing up-arrow and editing the command-line.
> `less !!:3` would do the same thing but it more flexible, since the number
> can be changed to grab any argument from the previous command.


## Shell Scripts {#rse-bash-scripts}
`
We are going to take the commands we repeat frequently and save them in files
so that we can re-run all those operations again later by typing a single command.
For historical reasons,
a bunch of commands saved in a file is usually called a shell **script**,
but make no mistake:
these are actually small programs.

Let's start by going back to `molecules/` and creating a new file, `middle.sh` which will
become our shell script:

```shell
$ cd molecules
$ nano middle.sh
```

The command `nano middle.sh` opens the file `middle.sh` within the text editor "nano"
(which runs within the shell).
If the file does not exist, it will be created.
We can use the text editor to directly edit the file -- we'll simply insert the following line:

```text
head -n 15 octane.pdb | tail -n 5
```

This is a variation on the pipe we constructed earlier:
it selects lines 11-15 of the file `octane.pdb`.
Remember, we are *not* running it as a command just yet:
we are putting the commands in a file.

Then we save the file (`Ctrl-O` in nano),
and exit the text editor (`Ctrl-X` in nano).
Check that the directory `molecules` now contains a file called `middle.sh`.

Once we have saved the file,
we can ask the shell to execute the commands it contains.
Our shell is called `bash`, so we run the following command:

```shell
$ bash middle.sh
```

```text
ATOM      9  H           1      -4.502   0.681   0.785  1.00  0.00
ATOM     10  H           1      -5.254  -0.243  -0.537  1.00  0.00
ATOM     11  H           1      -4.357   1.252  -0.895  1.00  0.00
ATOM     12  H           1      -3.009  -0.741  -1.467  1.00  0.00
ATOM     13  H           1      -3.172  -1.337   0.206  1.00  0.00
```

Sure enough,
our script's output is exactly what we would get if we ran that pipeline directly.

What if we want to select lines from an arbitrary file?
We could edit `middle.sh` each time to change the filename,
but that would probably take longer than just retyping the command.
Instead, let's edit `middle.sh` and make it more versatile:

```shell
$ nano middle.sh
```

Now, within "nano", replace the text `octane.pdb` with the special variable called `$1`:

```shell
head -n 15 "$1" | tail -n 5
```

Inside a shell script,
`$1` means "the first argument on the command line".
We can now run our script like this:

```shell
$ bash middle.sh octane.pdb
```

```text
ATOM      9  H           1      -4.502   0.681   0.785  1.00  0.00
ATOM     10  H           1      -5.254  -0.243  -0.537  1.00  0.00
ATOM     11  H           1      -4.357   1.252  -0.895  1.00  0.00
ATOM     12  H           1      -3.009  -0.741  -1.467  1.00  0.00
ATOM     13  H           1      -3.172  -1.337   0.206  1.00  0.00
```


or on a different file like this:

```shell
$ bash middle.sh pentane.pdb
```

```text
ATOM      9  H           1       1.324   0.350  -1.332  1.00  0.00
ATOM     10  H           1       1.271   1.378   0.122  1.00  0.00
ATOM     11  H           1      -0.074  -0.384   1.288  1.00  0.00
ATOM     12  H           1      -0.048  -1.362  -0.205  1.00  0.00
ATOM     13  H           1      -1.183   0.500  -1.412  1.00  0.00
```

> **Double-Quotes Around Arguments**
>
> For the same reason that we put the loop variable inside double-quotes,
> in case the filename happens to contain any spaces,
> we surround `$1` with double-quotes.


We still need to edit `middle.sh` each time we want to adjust the range of lines,
though.
Let's fix that by using the special variables `$2` and `$3` for the
number of lines to be passed to `head` and `tail` respectively:

```shell
$ nano middle.sh
```

```shell
head -n $2 "$1" | tail -n $3
```

We can now run:

```shell
$ bash middle.sh pentane.pdb 15 5
```

```text
ATOM      9  H           1       1.324   0.350  -1.332  1.00  0.00
ATOM     10  H           1       1.271   1.378   0.122  1.00  0.00
ATOM     11  H           1      -0.074  -0.384   1.288  1.00  0.00
ATOM     12  H           1      -0.048  -1.362  -0.205  1.00  0.00
ATOM     13  H           1      -1.183   0.500  -1.412  1.00  0.00
```

By changing the arguments to our command we can change our script's
behaviour:

```shell
$ bash middle.sh pentane.pdb 20 5
```


```text
ATOM     14  H           1      -1.259   1.420   0.112  1.00  0.00
ATOM     15  H           1      -2.608  -0.407   1.130  1.00  0.00
ATOM     16  H           1      -2.540  -1.303  -0.404  1.00  0.00
ATOM     17  H           1      -3.393   0.254  -0.321  1.00  0.00
TER      18              1
```

This works,
but it may take the next person who reads `middle.sh` a moment to figure out what it does.
We can improve our script by adding some **comments** at the top:

```shell
$ nano middle.sh
```

```text
# Select lines from the middle of a file.
# Usage: bash middle.sh filename end_line num_lines
head -n $2 "$1" | tail -n $3
```

A comment starts with a `#` character and runs to the end of the line.
The computer ignores comments,
but they're invaluable for helping people (including your future self) understand and use scripts.
The only caveat is that each time you modify the script,
you should check that the comment is still accurate:
an explanation that sends the reader in the wrong direction is worse than none at all.

What if we want to process many files in a single pipeline?
For example, if we want to sort our `.pdb` files by length, we would type:

```shell
$ wc -l *.pdb | sort -n
```

because `wc -l` lists the number of lines in the files
(recall that `wc` stands for 'word count', adding the `-l` option means 'count lines')
and `sort -n` sorts things numerically.
We could put this in a file,
but then it would only ever sort a list of `.pdb` files in the current directory.
If we want to be able to get a sorted list of other kinds of files,
we need a way to get all those names into the script.
We can't use `$1`, `$2`, and so on
because we don't know how many files there are.
Instead, we use the special variable `$@`,
which means,
"All of the command-line arguments to the shell script."
We also should put `$@` inside double-quotes
to handle the case of arguments containing spaces
(`"$@"` is equivalent to `"$1"` `"$2"` ...)
Here's an example:

```shell
$ nano sorted.sh
```

```text
# Sort filenames by their length.
# Usage: bash sorted.sh one_or_more_filenames
wc -l "$@" | sort -n
```

```shell
$ bash sorted.sh *.pdb ../creatures/*.dat
```

```text
9 methane.pdb
12 ethane.pdb
15 propane.pdb
20 cubane.pdb
21 pentane.pdb
30 octane.pdb
163 ../creatures/basilisk.dat
163 ../creatures/unicorn.dat
433 total
```


Suppose we have just run a series of commands that did something useful --- for example,
that created a graph we'd like to use in a paper.
We'd like to be able to re-create the graph later if we need to,
so we want to save the commands in a file.
Instead of typing them in again
(and potentially getting them wrong)
we can do this:

```shell
$ history 5 > redo-figure-3.sh
```

The file `redo-figure-3.sh` now contains:

```text
297 bash goostats NENE01729B.txt stats-NENE01729B.txt
298 bash goodiff stats-NENE01729B.txt /data/validated/01729.txt > 01729-differences.txt
299 cut -d ',' -f 2-3 01729-differences.txt > 01729-time-series.txt
300 ygraph --format scatter --color bw --borders none 01729-time-series.txt figure-3.png
301 history | tail -n 5 > redo-figure-3.sh
```

After a moment's work in an editor to remove the serial numbers on the commands,
and to remove the final line where we called the `history` command,
we have a completely accurate record of how we created that figure.

In practice, most people develop shell scripts by running commands at the shell prompt a few times
to make sure they're doing the right thing,
then saving them in a file for re-use.
This style of work allows people to recycle
what they discover about their data and their workflow with one call to `history`
and a bit of editing to clean up the output
and save it as a shell script.

## Finding Things {#rse-bash-finding}

Similarly to how many use "Google" as a
verb meaning "to find online", Unix programmers often use the
word "grep" to mean "search for a text pattern" (in a file).
"grep" is a contraction of "global/regular expression/print",
a common sequence of operations in early Unix text editors.
It is also the name of a very useful command-line program.

`grep` finds and prints lines in files that match a pattern.
For our examples,
we will use a file that contains three haikus taken from a
1998 competition in *Salon* magazine. For this set of examples,
we're going to be working in the writing subdirectory:

```shell
$ cd ~/Desktop/data-shell/writing
$ cat haiku.txt
```

```text
The Tao that is seen
Is not the true Tao, until
You bring fresh toner.

With searching comes loss
and the presence of absence:
"My Thesis" not found.

Yesterday it worked
Today it is not working
Software is like that.
```

> **Forever, or Five Years**
>
> We haven't linked to the original haikus because they don't appear to be on *Salon*'s site any longer.
> As [Jeff Rothenberg said](https://www.clir.org/wp-content/uploads/sites/6/ensuring.pdf),
> "Digital information lasts forever --- or five years, whichever comes first."
> Luckily, popular content often [has backups](http://wiki.c2.com/?ComputerErrorHaiku).

Let's find lines that contain the word "not":

```shell
$ grep not haiku.txt
```

```text
Is not the true Tao, until
"My Thesis" not found
Today it is not working
```

Here, `not` is the pattern we're searching for.
The grep command searches through the file,
looking for matches to the pattern specified.
To use it type `grep`,
then the pattern we're searching for
and finally the name of the file (or files) we're searching in.
The output is the three lines in the file that contain the letters "not".

By default, grep searches for a pattern in a case-sensitive way.
In addition, the search pattern we have selected does not have to form a complete word,
as we will see in the next example.

Let's search for the pattern: "The".

```shell
$ grep The haiku.txt
```

```text
The Tao that is seen
"My Thesis" not found.
```

This time, two lines that include the letters "The" are outputted,
one of which contained our search pattern within a larger word, "Thesis".

To restrict matches to lines containing the word "The" on its own,
we can give `grep` with the `-w` option.
This will limit matches to word boundaries.

Later in this lesson,
we will also see how we can change the search behavior of grep with respect to its case sensitivity.

```shell
$ grep -w The haiku.txt
```

```text
The Tao that is seen
```

Note that a "word boundary" includes the start and end of a line, so not
just letters surrounded by spaces.
Sometimes we don't
want to search for a single word, but a phrase. This is also easy to do with
`grep` by putting the phrase in quotes.

```shell
$ grep -w "is not" haiku.txt
```

```text
Today it is not working
```

We've now seen that you don't have to include quotes around single words,
but it is useful to use quotes when searching for multiple words.
It also helps to make it easier to distinguish between the search term or phrase
and the file being searched.
We will use quotes in the remaining examples.

Another useful option is `-n`, which numbers the lines that match:

```shell
$ grep -n "it" haiku.txt
```

```text
5:With searching comes loss
9:Yesterday it worked
10:Today it is not working
```

Here, we can see that lines 5, 9, and 10 contain the letters "it".

We can combine options (i.e. flags) as we do with other Unix commands.
For example, let's find the lines that contain the word "the". We can combine
the option `-w` to find the lines that contain the word "the" and `-n` to number the lines that match:

```shell
$ grep -n -w "the" haiku.txt
```

```text
2:Is not the true Tao, until
6:and the presence of absence:
```

Now we want to use the option `-i` to make our search case-insensitive:

```shell
$ grep -n -w -i "the" haiku.txt
```

```text
1:The Tao that is seen
2:Is not the true Tao, until
6:and the presence of absence:
```

Now, we want to use the option `-v` to invert our search, i.e., we want to output
the lines that do not contain the word "the".

```shell
$ grep -n -w -v "the" haiku.txt
```

```text
1:The Tao that is seen
3:You bring fresh toner.
4:
5:With searching comes loss
7:"My Thesis" not found.
8:
9:Yesterday it worked
10:Today it is not working
11:Software is like that.
```

To search for text within multiple files in the same directory,
wildcards can be used when specifying the file names.
For example,
`grep "the" *.txt` would search all `.txt` files in the current directory.
To search all files within a directory and its subdirectories,
the recursive `-r` flag can be used: `grep -r "the"`.
When multiple files are searched,
the filename will precede the printed matching line.

`grep` has lots of other options. To find out what they are, we can type:

```shell
$ grep --help
```

```text
Usage: grep [OPTION]... PATTERN [FILE]...
Search for PATTERN in each FILE or standard input.
PATTERN is, by default, a basic regular expression (BRE).
Example: grep -i 'hello world' menu.h main.c

Regexp selection and interpretation:
  -E, --extended-regexp     PATTERN is an extended regular expression (ERE)
  -F, --fixed-strings       PATTERN is a set of newline-separated fixed strings
  -G, --basic-regexp        PATTERN is a basic regular expression (BRE)
  -P, --perl-regexp         PATTERN is a Perl regular expression
  -e, --regexp=PATTERN      use PATTERN for matching
  -f, --file=FILE           obtain PATTERN from FILE
  -i, --ignore-case         ignore case distinctions
  -w, --word-regexp         force PATTERN to match only whole words
  -x, --line-regexp         force PATTERN to match only whole lines
  -z, --null-data           a data line ends in 0 byte, not newline

Miscellaneous:
...        ...        ...
```

> **Wildcards**
>
> `grep`'s real power doesn't come from its options, though; it comes from
> the fact that patterns can include wildcards. (The technical name for
> these is **regular expressions**, which
> is what the "re" in "grep" stands for.) Regular expressions are both complex
> and powerful; if you want to do complex searches, please look at the lesson
> on [the website](http://v4.software-carpentry.org/regexp/index.html).
> As a sample,
> we can find lines that have an 'o' in the second position like this:
>
> ```shell
> $ grep -E '^.o' haiku.txt
> ```
>
> ```text
> You bring fresh toner.
> Today it is not working
> Software is like that.
> ```
>
> We use the `-E` option and put the pattern in quotes to prevent the shell
> from trying to interpret it. (If the pattern contained a `*`, for
> example, the shell would try to expand it before running `grep`.) The
> `^` in the pattern anchors the match to the start of the line. The `.`
> matches a single character (just like `?` in the shell), while the `o`
> matches an actual 'o'.

While `grep` finds lines in files,
the `find` command finds files themselves.
Again,
it has a lot of options;
to show how the simplest ones work, we'll use the directory tree shown below.

*TODO: Figure - File Tree for Find Example [../fig/find-file-tree.svg](https://github.com/swcarpentry/shell-novice/blob/gh-pages/fig/find-file-tree.svg)*

Amira's `writing` directory contains one file called `haiku.txt` and three subdirectories:
`thesis` (which contains a sadly empty file, `empty-draft.md`);
`data` (which contains three files `LittleWomen.txt`, `one.txt` and `two.txt`);
and a `tools` directory that contains the programs `format` and `stats`,
and a subdirectory called `old`, with a file `oldtool`.

For our first command,
let's run `find .` (remember to run this command from the `data-shell/writing` folder).

```shell
$ find .
```

```text
.
./data
./data/one.txt
./data/LittleWomen.txt
./data/two.txt
./tools
./tools/format
./tools/old
./tools/old/oldtool
./tools/stats
./haiku.txt
./thesis
./thesis/empty-draft.md
```

As always,
the `.` on its own means the current working directory,
which is where we want our search to start.
`find`'s output includes the names of files **and** directories
under the current working directory.
This can seem useless at first but `find` has many options
to filter the output and in this lesson we will discover some
of them.

The first option in our list is
`-type d` that means "directories".
Sure enough,
`find`'s output is the names of the five directories in our little tree
(including `.`):

```shell
$ find . -type d
```

```text
./
./data
./thesis
./tools
./tools/old
```

If we change `-type d` to `-type f`,
we get a listing of all the files instead:

```shell
$ find . -type f
```

```text
./haiku.txt
./tools/stats
./tools/old/oldtool
./tools/format
./thesis/empty-draft.md
./data/one.txt
./data/LittleWomen.txt
./data/two.txt
```

Now let's try matching by name:

```shell
$ find . -name *.txt
```

```text
./haiku.txt
```

We expected it to find all the text files,
but it only prints out `./haiku.txt`.
The problem is that the shell expands wildcard characters like `*` *before* commands run.
Since `*.txt` in the current directory expands to `haiku.txt`,
the command we actually ran was:

```shell
$ find . -name haiku.txt
```

`find` did what we asked; we just asked for the wrong thing.

To get what we want,
let's do what we did with `grep`:
put `*.txt` in single quotes to prevent the shell from expanding the `*` wildcard.
This way,
`find` actually gets the pattern `*.txt`, not the expanded filename `haiku.txt`:

```shell
$ find . -name '*.txt'
```

```text
./data/one.txt
./data/LittleWomen.txt
./data/two.txt
./haiku.txt
```

> **Listing vs. Finding**
>
> `ls` and `find` can be made to do similar things given the right options,
> but under normal circumstances,
> `ls` lists everything it can,
> while `find` searches for things with certain properties and shows them.

As we said earlier,
the command line's power lies in combining tools.
We've seen how to do that with pipes;
let's look at another technique.
As we just saw,
`find . -name '*.txt'` gives us a list of all text files in or below the current directory.
How can we combine that with `wc -l` to count the lines in all those files?

The simplest way is to put the `find` command inside `$()`:

```shell
$ wc -l $(find . -name '*.txt')
```

```text
11 ./haiku.txt
300 ./data/two.txt
21022 ./data/LittleWomen.txt
70 ./data/one.txt
21403 total
```

When the shell executes this command,
the first thing it does is run whatever is inside the `$()`.
It then replaces the `$()` expression with that command's output.
Since the output of `find` is the four filenames `./data/one.txt`,
`./data/LittleWomen.txt`, `./data/two.txt`, and `./haiku.txt`,
the shell constructs the command:

```shell
$ wc -l ./data/one.txt ./data/LittleWomen.txt ./data/two.txt ./haiku.txt
```

which is what we wanted.
This expansion is exactly what the shell does when it expands wildcards like `*` and `?`,
but lets us use any command we want as our own "wildcard".

It's very common to use `find` and `grep` together.
The first finds files that match a pattern;
the second looks for lines inside those files that match another pattern.
Here, for example, we can find PDB files that contain iron atoms
by looking for the string "FE" in all the `.pdb` files above the current directory:

```shell
$ grep "FE" $(find .. -name '*.pdb')
```

```text
../data/pdb/heme.pdb:ATOM     25 FE           1      -0.924   0.535  -0.518
```

> **Binary Files**
>
> We have focused exclusively on finding patterns in text files. What if
> your data is stored as images, in databases, or in some other format?
>
> A handful of tools extend `grep` to handle a few non text formats. But a
> more generalizable approach is to convert the data to text, or
> extract the text-like elements from the data. On the one hand, it makes simple
> things easy to do. On the other hand, complex things are usually impossible. For
> example, it's easy enough to write a program that will extract X and Y
> dimensions from image files for `grep` to play with, but how would you
> write something to find values in a spreadsheet whose cells contained
> formulas?
>
> A last option is to recognize that the shell and text processing have
> their limits, and to use another programming language.
> When the time comes to do this, don't be too hard on the shell: many
> modern programming languages have borrowed a lot of
> ideas from it, and imitation is also the sincerest form of praise.

## Summary {#rse-bash-summary}

What have we learned in these lessons on the bash shell?
The [Navigating Files and Directories](#bash-navigation) and
[Working With Files and Directories](#bash-working) content
means that we can now get around in a command line environment.
This is especially useful if we are doing our data analysis
by logging onto a high performance computing facility, for instance,
as the command line interface is often the only way to interact with such a facility.

The [Loops](#bash-loops), [Shell Scripts](#bash-scripts) and
[Finding Things](#bash-finding) content means that we can now save time
by getting the computer to do repetitive tasks for us.

Finally, the [Pipes and Filters](#bash-pipes) content means we've been introduced
to a fundamentally important concept:
rather than writing monolithic functions/programs that do many things at once,
it's better to write small functions/programs that do perform a well-defined, discrete task,
and then combine them in myriad different ways to conduct your data analysis.

## Exercises {#rse-bash-exercises}


### Navigating Files and Directories

**Exploring More `ls` Flags**

You can also use two options at the same time. What does the command `ls` do when used
with the `-l` option? What about if you use both the `-l` and the `-h` option?

Some of its output is about properties that we do not cover in this lesson (such
as file permissions and ownership), but the rest should be useful
nevertheless.

> *Solution*  
>
> The `-l` option makes `ls` use a **l**ong listing format, showing not only
> the file/directory names but also additional information such as the file size
> and the time of its last modification. If you use both the `-h` option and the `-l` option,
> this makes the file size "**h**uman readable", i.e. displaying something like `5.3K`
> instead of `5369`.


**Listing Recursively and By Time**

The command `ls -R` lists the contents of directories recursively, i.e., lists
their sub-directories, sub-sub-directories, and so on at each level. The command
`ls -t` lists things by time of last change, with most recently changed files or
directories first.

In what order does `ls -R -t` display things? Hint: `ls -l` uses a long listing
format to view timestamps.

> *Solution*  
>
> The files/directories in each directory are sorted by time of last change.


**Absolute vs Relative Paths**

Starting from `/Users/amanda/data`,
which of the following commands could Amanda use to navigate to her home directory,
which is `/Users/amanda`?

1. `cd .`
2. `cd /`
3. `cd /home/amanda`
4. `cd ../..`
5. `cd ~`
6. `cd home`
7. `cd ~/data/..`
8. `cd`
9. `cd ..`

> *Solution*
>
> 1. No: `.` stands for the current directory.
> 2. No: `/` stands for the root directory.
> 3. No: Amanda's home directory is `/Users/amanda`.
> 4. No: this goes up two levels, i.e. ends in `/Users`.
> 5. Yes: `~` stands for the user's home directory, in this case `/Users/amanda`.
> 6. No: this would navigate into a directory `home` in the current directory if it exists.
> 7. Yes: unnecessarily complicated, but correct.
> 8. Yes: shortcut to go back to the user's home directory.
> 9. Yes: goes up one level.

**Relative Path Resolution**

Using the filesystem diagram below, if `pwd` displays `/Users/thing`,
what will `ls -F ../backup` display?

1.  `../backup: No such file or directory`
2.  `2012-12-01 2013-01-08 2013-01-27`
3.  `2012-12-01/ 2013-01-08/ 2013-01-27/`
4.  `original/ pnas_final/ pnas_sub/`

TODO: Figure - Filesystem for Challenge Questions [../fig/filesystem-challenge.svg](https://github.com/swcarpentry/shell-novice/blob/gh-pages/fig/filesystem-challenge.svg)

> *Solution*
>
> 1. No: there *is* a directory `backup` in `/Users`.
> 2. No: this is the content of `Users/thing/backup`,
>    but with `..` we asked for one level further up.
> 3. No: see previous explanation.
> 4. Yes: `../backup/` refers to `/Users/backup/`.


**`ls` Reading Comprehension**

Using the filesystem diagram below,
if `pwd` displays `/Users/backup`,
and `-r` tells `ls` to display things in reverse order,
what command(s) will result in the following output:

```shell
pnas_sub/ pnas_final/ original/
```

TODO: Figure - Filesystem for Challenge Questions [../fig/filesystem-challenge.svg](https://github.com/swcarpentry/shell-novice/blob/gh-pages/fig/filesystem-challenge.svg)

1.  `ls pwd`
2.  `ls -r -F`
3.  `ls -r -F /Users/backup`

> *Solution*
>
>  1. No: `pwd` is not the name of a directory.
>  2. Yes: `ls` without directory argument lists files and directories
>     in the current directory.
>  3. Yes: uses the absolute path explicitly.


### Working With Files and Directories

**Creating Files a Different Way**

We have seen how to create text files using the `nano` editor.
Now, try the following command:

```shell
$ touch my_file.txt
```

1.  What did the `touch` command do?
    When you look at your current directory using the GUI file explorer,
    does the file show up?

2.  Use `ls -l` to inspect the files.  How large is `my_file.txt`?

3.  When might you want to create a file this way?

> *Solution*
>
> 1.  The `touch` command generates a new file called `my_file.txt` in
>     your current directory.  You
>     can observe this newly generated file by typing `ls` at the
>     command line prompt. `my_file.txt` can also be viewed in your
>     GUI file explorer.
>
> 2.  When you inspect the file with `ls -l`, note that the size of
>     `my_file.txt` is 0 bytes.  In other words, it contains no data.
>     If you open `my_file.txt` using your text editor it is blank.
>
> 3.  Some programs do not generate output files themselves, but
>     instead require that empty files have already been generated.
>     When the program is run, it searches for an existing file to
>     populate with its output.  The touch command allows you to
>     efficiently generate a blank text file to be used by such
>     programs.

**Moving to the Current Folder**

After running the following commands,
Jamie realizes that she put the files `sucrose.dat` and `maltose.dat` into the wrong folder:

```shell
$ ls -F
  analyzed/ raw/
$ ls -F analyzed
  fructose.dat glucose.dat maltose.dat sucrose.dat
$ cd raw/
```

Fill in the blanks to move these files to the current folder
(i.e., the one she is currently in):

```shell
$ mv ___/sucrose.dat  ___/maltose.dat ___
```

> *Solution*
>
> ```shell
> $ mv ../analyzed/sucrose.dat ../analyzed/maltose.dat .
> ```
>
> Recall that `..` refers to the parent directory (i.e. one above the current directory)
> and that `.` refers to the current directory.


**Renaming Files**

Suppose that you created a plain-text file in your current directory to contain a list of the
statistical tests you will need to do to analyze your data, and named it: `statstics.txt`

After creating and saving this file you realize you misspelled the filename! You want to
correct the mistake, which of the following commands could you use to do so?

1. `cp statstics.txt statistics.txt`
2. `mv statstics.txt statistics.txt`
3. `mv statstics.txt .`
4. `cp statstics.txt .`

> *Solution*
>
> 1. No.  While this would create a file with the correct name,
> the incorrectly named file still exists in the directory and would need to be deleted.
> 2. Yes, this would work to rename the file.
> 3. No, the period(.) indicates where to move the file, but does not provide a new file name;
> identical file names cannot be created.
> 4. No, the period(.) indicates where to copy the file, but does not provide a new file name;
> identical file names cannot be created.


**Moving and Copying**

What is the output of the closing `ls` command in the sequence shown below?

```shell
$ pwd
```
 
```text
/Users/jamie/data
```
 
```shell
$ ls
```
 
```text
proteins.dat
```
 
```shell
$ mkdir recombine
$ mv proteins.dat recombine/
$ cp recombine/proteins.dat ../proteins-saved.dat
$ ls
```

1.   `proteins-saved.dat recombine`
2.   `recombine`
3.   `proteins.dat recombine`
4.   `proteins-saved.dat`

> *Solution*
>
> We start in the `/Users/jamie/data` directory, and create a new folder called `recombine`.
> The second line moves (`mv`) the file `proteins.dat` to the new folder (`recombine`).
> The third line makes a copy of the file we just moved.  The tricky part here is where the file was
> copied to.  Recall that `..` means "go up a level", so the copied file is now in `/Users/jamie`.
> Notice that `..` is interpreted with respect to the current working
> directory, **not** with respect to the location of the file being copied.
> So, the only thing that will show using ls (in `/Users/jamie/data`) is the recombine folder.
>
> 1. No, see explanation above.  `proteins-saved.dat` is located at `/Users/jamie`
> 2. Yes
> 3. No, see explanation above.  `proteins.dat` is located at `/Users/jamie/data/recombine`
> 4. No, see explanation above.  `proteins-saved.dat` is located at `/Users/jamie`


**Using `rm` Safely**

What happens when we execute `rm -i thesis_backup/quotations.txt`?
Why would we want this protection when using `rm`?

> *Solution*
>
> ```shell
> $ rm: remove regular file 'thesis_backup/quotations.txt'? y
> ```
> 
> The `-i` option will prompt before (every) removal
> (use <kbd>Y</kbd> to confirm deletion or <kbd>N</kbd> to keep the file).
> The Unix shell doesn't have a trash bin, so all the files removed will disappear forever.
> By using the `-i` option, we have the chance to check that we are deleting
> only the files that we want to remove.


**Copy with Multiple Filenames**

For this exercise, you can test the commands in the `data-shell/data` directory.

In the example below, what does `cp` do when given several filenames and a directory name?

```shell
$ mkdir backup
$ cp amino-acids.txt animals.txt backup/
```

In the example below, what does `cp` do when given three or more file names?

```shell
$ ls -F
```

```text
amino-acids.txt  animals.txt  backup/  elements/  morse.txt  pdb/  planets.txt  salmon.txt  sunspot.txt
```

```shell
$ cp amino-acids.txt animals.txt morse.txt 
```

> *Solution*
>
> If given more than one file name followed by a directory name (i.e. the destination directory must 
> be the last argument), `cp` copies the files to the named directory.
>
> If given three file names, `cp` throws an error such as the one below, because it is expecting a directory
> name as the last argument.
>
> ```text
> cp: target ‘morse.txt’ is not a directory
> ```


**List filenames matching a pattern**

When run in the `molecules` directory, which `ls` command(s) will
produce this output?

`ethane.pdb   methane.pdb`

1. `ls *t*ane.pdb`
2. `ls *t?ne.*`
3. `ls *t??ne.pdb`
4. `ls ethane.*`

> *Solution*
>
>  The solution is `3.`
>
> `1.` shows all files whose names contain zero or more characters (`*`) followed by the letter `t`,
> then zero or more characters (`*`) followed by `ane.pdb`.
> This gives `ethane.pdb  methane.pdb  octane.pdb  pentane.pdb`. 
>
> `2.` shows all files whose names start with zero or more characters (`*`) followed by the letter `t`,
> then a single character (`?`), then `ne.` followed by zero or more characters (`*`).
> This will give us `octane.pdb` and `pentane.pdb` but doesn't match anything which ends in `thane.pdb`.
>
> `3.` fixes the problems of option 2 by matching two characters (`??`) between `t` and `ne`.
> This is the solution.
>
> `4.` only shows files starting with `ethane.`.


**More on Wildcards**

Sam has a directory containing calibration data, datasets, and descriptions of
the datasets:

```text
.
├── 2015-10-23-calibration.txt
├── 2015-10-23-dataset1.txt
├── 2015-10-23-dataset2.txt
├── 2015-10-23-dataset_overview.txt
├── 2015-10-26-calibration.txt
├── 2015-10-26-dataset1.txt
├── 2015-10-26-dataset2.txt
├── 2015-10-26-dataset_overview.txt
├── 2015-11-23-calibration.txt
├── 2015-11-23-dataset1.txt
├── 2015-11-23-dataset2.txt
├── 2015-11-23-dataset_overview.txt
├── backup
│   ├── calibration
│   └── datasets
└── send_to_bob
    ├── all_datasets_created_on_a_23rd
    └── all_november_files
```

Before heading off to another field trip, she wants to back up her data and
send some datasets to her colleague Bob. Sam uses the following commands
to get the job done:

```shell
$ cp *dataset* backup/datasets
$ cp ____calibration____ backup/calibration
$ cp 2015-____-____ send_to_bob/all_november_files/
$ cp ____ send_to_bob/all_datasets_created_on_a_23rd/
```

Help Sam by filling in the blanks.

The resulting directory structure should look like this
```text
.
├── 2015-10-23-calibration.txt
├── 2015-10-23-dataset1.txt
├── 2015-10-23-dataset2.txt
├── 2015-10-23-dataset_overview.txt
├── 2015-10-26-calibration.txt
├── 2015-10-26-dataset1.txt
├── 2015-10-26-dataset2.txt
├── 2015-10-26-dataset_overview.txt
├── 2015-11-23-calibration.txt
├── 2015-11-23-dataset1.txt
├── 2015-11-23-dataset2.txt
├── 2015-11-23-dataset_overview.txt
├── backup
│   ├── calibration
│   │   ├── 2015-10-23-calibration.txt
│   │   ├── 2015-10-26-calibration.txt
│   │   └── 2015-11-23-calibration.txt
│   └── datasets
│       ├── 2015-10-23-dataset1.txt
│       ├── 2015-10-23-dataset2.txt
│       ├── 2015-10-23-dataset_overview.txt
│       ├── 2015-10-26-dataset1.txt
│       ├── 2015-10-26-dataset2.txt
│       ├── 2015-10-26-dataset_overview.txt
│       ├── 2015-11-23-dataset1.txt
│       ├── 2015-11-23-dataset2.txt
│       └── 2015-11-23-dataset_overview.txt
└── send_to_bob
    ├── all_datasets_created_on_a_23rd
    │   ├── 2015-10-23-dataset1.txt
    │   ├── 2015-10-23-dataset2.txt
    │   ├── 2015-10-23-dataset_overview.txt
    │   ├── 2015-11-23-dataset1.txt
    │   ├── 2015-11-23-dataset2.txt
    │   └── 2015-11-23-dataset_overview.txt
    └── all_november_files
        ├── 2015-11-23-calibration.txt
        ├── 2015-11-23-dataset1.txt
        ├── 2015-11-23-dataset2.txt
        └── 2015-11-23-dataset_overview.txt
```

> *Solution*
>
> ```shell
> $ cp *calibration.txt backup/calibration
> $ cp 2015-11-* send_to_bob/all_november_files/
> $ cp *-23-dataset* send_to_bob/all_datasets_created_on_a_23rd/
> ```


**Organizing Directories and Files**

Jamie is working on a project and she sees that her files aren't very well
organized:

```shell
$ ls -F
```

```text
analyzed/  fructose.dat    raw/   sucrose.dat
```

The `fructose.dat` and `sucrose.dat` files contain output from her data
analysis. What command(s) covered in this lesson does she need to run so that the commands below will
produce the output shown?

```shell
$ ls -F
```

```text
analyzed/   raw/
```
 
```shell
$ ls analyzed
```

```text
fructose.dat    sucrose.dat
```

> *Solution*
>
> ```shell
> mv *.dat analyzed
> ```
> 
> Jamie needs to move her files `fructose.dat` and `sucrose.dat` to the `analyzed` directory.
> The shell will expand *.dat to match all .dat files in the current directory.
> The `mv` command then moves the list of .dat files to the "analyzed" directory.


**Reproduce a folder structure**

You're starting a new experiment, and would like to duplicate the directory
structure from your previous experiment so you can add new data.

Assume that the previous experiment is in a folder called '2016-05-18',
which contains a `data` folder that in turn contains folders named `raw` and
`processed` that contain data files.  The goal is to copy the folder structure
of the `2016-05-18-data` folder into a folder called `2016-05-20`
so that your final directory structure looks like this:

	2016-05-20/
	└── data
	    ├── processed
	    └── raw
 
Which of the following set of commands would achieve this objective?

What would the other commands do?

```shell
$ mkdir 2016-05-20
$ mkdir 2016-05-20/data
$ mkdir 2016-05-20/data/processed
$ mkdir 2016-05-20/data/raw
```

```shell
$ mkdir 2016-05-20
$ cd 2016-05-20
$ mkdir data
$ cd data
$ mkdir raw processed
```

```shell
$ mkdir 2016-05-20/data/raw
$ mkdir 2016-05-20/data/processed
```
 
```shell
$ mkdir 2016-05-20
$ cd 2016-05-20
$ mkdir data
$ mkdir raw processed
```
 
> *Solution*
>
> The first two sets of commands achieve this objective.
> The first set uses relative paths to create the top level directory before
> the subdirectories.
>
> The third set of commands will give an error because `mkdir` won't create a subdirectory
> of a non-existant directory: the intermediate level folders must be created first.
> 
> The final set of commands generates the 'raw' and 'processed' directories at the same level
> as the 'data' directory.


### Pipes and Filters

**What Does `sort -n` Do?**

If we run `sort` on a file containing the following lines:

```text
10
2
19
22
6
```

the output is:

```text
10
19
2
22
6
```

If we run `sort -n` on the same input, we get this instead:

```text
2
6
10
19
22
```

Explain why `-n` has this effect.

> *Solution*
>
> The `-n` option specifies a numerical rather than an alphanumerical sort.


**What Does `>>` Mean?**

We have seen the use of `>`, but there is a similar operator `>>` which works slightly differently.
We'll learn about the differences between these two operators by printing some strings.
We can use the `echo` command to print strings e.g.

```shell
$ echo The echo command prints text
```

```text
The echo command prints text
```

Now test the commands below to reveal the difference between the two operators:

```shell
$ echo hello > testfile01.txt
```

and:

```shell
$ echo hello >> testfile02.txt
```

Hint: Try executing each command twice in a row and then examining the output files.

> *Solution*
>
> In the first example with `>`, the string "hello" is written to `testfile01.txt`,
> but the file gets overwritten each time we run the command.
>
> We see from the second example that the `>>` operator also writes "hello" to a file
> (in this case`testfile02.txt`),
> but appends the string to the file if it already exists (i.e. when we run it for the second time).


**Appending Data**

We have already met the `head` command, which prints lines from the start of a file.
`tail` is similar, but prints lines from the end of a file instead.

Consider the file `data-shell/data/animals.txt`.
After these commands, select the answer that
corresponds to the file `animals-subset.txt`:

```shell
$ head -n 3 animals.txt > animals-subset.txt
$ tail -n 2 animals.txt >> animals-subset.txt
```

1. The first three lines of `animals.txt`
2. The last two lines of `animals.txt`
3. The first three lines and the last two lines of `animals.txt`
4. The second and third lines of `animals.txt`

> *Solution*
> Option 3 is correct.
> For option 1 to be correct we would only run the `head` command.
> For option 2 to be correct we would only run the `tail` command.
> For option 4 to be correct we would have to pipe the output of `head` into `tail -n 2`
> by doing `head -n 3 animals.txt | tail -n 2 > animals-subset.txt`


**Piping Commands Together**

In our current directory, we want to find the 3 files which have the least number of
lines. Which command listed below would work?

1. `wc -l * > sort -n > head -n 3`
2. `wc -l * | sort -n | head -n 1-3`
3. `wc -l * | head -n 3 | sort -n`
4. `wc -l * | sort -n | head -n 3`

> *Solution*
>
> Option 4 is the solution.
> The pipe character `|` is used to feed the standard output from one process to
> the standard input of another.
> `>` is used to redirect standard output to a file.
> Try it in the `data-shell/molecules` directory!


**Why Does `uniq` Only Remove Adjacent Duplicates?**

The command `uniq` removes adjacent duplicated lines from its input.
For example, the file `data-shell/data/salmon.txt` contains:

```text
coho
coho
steelhead
coho
steelhead
steelhead
```

Running the command `uniq salmon.txt` from the `data-shell/data` directory produces:

```text
coho
steelhead
coho
steelhead
```

Why do you think `uniq` only removes *adjacent* duplicated lines?
(Hint: think about very large data sets.) What other command could
you combine with it in a pipe to remove all duplicated lines?

> *Solution*
> ```shell
> $ sort salmon.txt | uniq
> ```


**Pipe Reading Comprehension**

A file called `animals.txt` (in the `data-shell/data` folder) contains the following data:

```text
2012-11-05,deer
2012-11-05,rabbit
2012-11-05,raccoon
2012-11-06,rabbit
2012-11-06,deer
2012-11-06,fox
2012-11-07,rabbit
2012-11-07,bear
```

What text passes through each of the pipes and the final redirect in the pipeline below?

```shell
$ cat animals.txt | head -n 5 | tail -n 3 | sort -r > final.txt
```
 
Hint: build the pipeline up one command at a time to test your understanding

> *Solution*
>
> The `head` command extracts the first 5 lines from `animals.txt`.
> Then, the last 3 lines are extracted from the previous 5 by using the `tail` command.
> With the `sort -r` command those 3 lines are sorted in reverse order and finally,
> the output is redirected to a file `final.txt`.
> The content of this file can be checked by executing `cat final.txt`.
> The file should contain the following lines:
> ```text
> 2012-11-06,rabbit
> 2012-11-06,deer
> 2012-11-05,raccoon
> ```


**Pipe Construction**

For the file `animals.txt` from the previous exercise, consider the following command:

```shell
$ cut -d , -f 2 animals.txt
```

The `cut` command is used to remove or "cut out" certain sections of each line in the file.
The `-d` option is used to define the delimiter.
A **delimiter** is a character that is used to separate each line of text into columns.
The default delimiter is <kbd>Tab</kbd>,
meaning that the `cut` command will automatically assume that
values in different columns will be separated by a tab.
The `-f` option is used to specify the field (column) to cut out.
The command above uses the `-d` option to split each line by comma,
and the `-f` option to print the second field in each line, to give the following output:

```text
deer
rabbit
raccoon
rabbit
deer
fox
rabbit
bear
```

What other command(s) could be added to this in a pipeline to find
out what animals the file contains (without any duplicates in their
names)?

> *Solution*
>
> ```shell
> $ cut -d , -f 2 animals.txt | sort | uniq
> ```


**Which Pipe?**

The file `animals.txt` contains 8 lines of data formatted as follows:

```text
2012-11-05,deer
2012-11-05,rabbit
2012-11-05,raccoon
2012-11-06,rabbit
...
```

The `uniq` command has a `-c` option which gives a count of the
number of times a line occurs in its input.  Assuming your current
directory is `data-shell/data/`, what command would you use to produce
a table that shows the total count of each type of animal in the file?

1.  `sort animals.txt | uniq -c`
2.  `sort -t, -k2,2 animals.txt | uniq -c`
3.  `cut -d, -f 2 animals.txt | uniq -c`
4.  `cut -d, -f 2 animals.txt | sort | uniq -c`
5.  `cut -d, -f 2 animals.txt | sort | uniq -c | wc -l`

> *Solution*
>
> Option 4. is the correct answer.
> If you have difficulty understanding why, try running the commands, or sub-sections of
> the pipelines (make sure you are in the `data-shell/data` directory).


**Wildcard Expressions**

Wildcard expressions can be very complex, but you can sometimes write
them in ways that only use simple syntax, at the expense of being a bit
more verbose.
Consider the directory `data-shell/north-pacific-gyre/2012-07-03` :
the wildcard expression `*[AB].txt`
matches all files ending in `A.txt` or `B.txt`. Imagine you forgot about
this.

1.  Can you match the same set of files with basic wildcard expressions
    that do not use the `[]` syntax? *Hint*: You may need more than one
    expression.

2.  The expression that you found and the expression from the lesson match the
    same set of files in this example. What is the small difference between the
    outputs?

3.  Under what circumstances would your new expression produce an error message
    where the original one would not?
    
> *Solution*
>
> 1. A solution using two wildcard expressions:
>     ```shell
>     $ ls *A.txt
>     $ ls *B.txt
>     ```
> 2. The output from the new commands is separated because there are two commands.
> 3. When there are no files ending in `A.txt`, or there are no files ending in
> `B.txt`.


**Removing Unneeded Files**

Suppose you want to delete your processed data files, and only keep
your raw files and processing script to save storage.
The raw files end in `.dat` and the processed files end in `.txt`.
Which of the following would remove all the processed data files,
and *only* the processed data files?

1. `rm ?.txt`
2. `rm *.txt`
3. `rm * .txt`
4. `rm *.*`

> *Solution*
>
> 1. This would remove `.txt` files with one-character names
> 2. This is correct answer
> 3. The shell would expand `*` to match everything in the current directory,
> so the command would try to remove all matched files and an additional
> file called `.txt`
> 4. The shell would expand `*.*` to match all files with any extension,
> so this command would delete all files


### Loops

**Doing a Dry Run**

A loop is a way to do many things at once --- or to make many mistakes at
once if it does the wrong thing. One way to check what a loop *would* do
is to `echo` the commands it would run instead of actually running them.

Suppose we want to preview the commands the following loop will execute
without actually running those commands:

```shell
$ for file in *.pdb
> do
>   analyze $file > analyzed-$file
> done
```

What is the difference between the two loops below, and which one would we
want to run?

```shell
# Version 1
$ for file in *.pdb
> do
>   echo analyze $file > analyzed-$file
> done
```

```shell
# Version 2
$ for file in *.pdb
> do
>   echo "analyze $file > analyzed-$file"
> done
```

> *Solution*
>
> The second version is the one we want to run.
> This prints to screen everything enclosed in the quote marks, expanding the
> loop variable name because we have prefixed it with a dollar sign.
>
> The first version redirects the output from the command `echo analyze $file` to
> a file, `analyzed-$file`. A series of files is generated: `analyzed-cubane.pdb`,
> `analyzed-ethane.pdb` etc.
>
> Try both versions for yourself to see the output! Be sure to open the
> `analyzed-*.pdb` files to view their contents.


**Nested Loops**

Suppose we want to set up up a directory structure to organize
some experiments measuring reaction rate constants with different compounds
*and* different temperatures.  What would be the
result of the following code:

```shell
$ for species in cubane ethane methane
> do
>     for temperature in 25 30 37 40
>     do
>         mkdir $species-$temperature
>     done
> done
```

> *Solution*
>
> We have a nested loop, i.e. contained within another loop, so for each species
> in the outer loop, the inner loop (the nested loop) iterates over the list of
> temperatures, and creates a new directory for each combination.
>
> Try running the code for yourself to see which directories are created!


**Variables in Loops**

This exercise refers to the `data-shell/molecules` directory.
`ls` gives the following output:

```text
cubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb
```

What is the output of the following code?

```shell
$ for datafile in *.pdb
> do
>    ls *.pdb
> done
```

Now, what is the output of the following code?

```shell
$ for datafile in *.pdb
> do
>	ls $datafile
> done
```

Why do these two loops give different outputs?

> *Solution*
>
> The first code block gives the same output on each iteration through
> the loop.
> Bash expands the wildcard `*.pdb` within the loop body (as well as
> before the loop starts) to match all files ending in `.pdb`
> and then lists them using `ls`.
> The expanded loop would look like this:
> ```shell
> $ for datafile in cubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb
> > do
> >	ls cubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb
> > done
> ```
>
> ```text
> cubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb
> cubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb
> cubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb
> cubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb
> cubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb
> cubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb
> ```
>
> The second code block lists a different file on each loop iteration.
> The value of the `datafile` variable is evaluated using `$datafile`,
> and then listed using `ls`.
>
> ```text
> cubane.pdb
> ethane.pdb
> methane.pdb
> octane.pdb
> pentane.pdb
> propane.pdb
> ```


**Limiting Sets of Files**

What would be the output of running the following loop in the `data-shell/molecules` directory?

```shell
$ for filename in c*
> do
>    ls $filename
> done
```

1.  No files are listed.
2.  All files are listed.
3.  Only `cubane.pdb`, `octane.pdb` and `pentane.pdb` are listed.
4.  Only `cubane.pdb` is listed.

> *Solution*
>
> 4 is the correct answer. `*` matches zero or more characters, so any file name starting with
> the letter c, followed by zero or more other characters will be matched.

How would the output differ from using this command instead?

```shell
$ for filename in *c*
> do
>    ls $filename
> done
```

1.  The same files would be listed.
2.  All the files are listed this time.
3.  No files are listed this time.
4.  The files `cubane.pdb` and `octane.pdb` will be listed.
5.  Only the file `octane.pdb` will be listed.

> *Solution*
>
> 4 is the correct answer. `*` matches zero or more characters, so a file name with zero or more
> characters before a letter c and zero or more characters after the letter c will be matched.


**Saving to a File in a Loop - Part One**

In the `data-shell/molecules` directory, what is the effect of this loop?

```shell
for alkanes in *.pdb
> do
>     echo $alkanes
>     cat $alkanes > alkanes.pdb
> done
```

1.  Prints `cubane.pdb`, `ethane.pdb`, `methane.pdb`, `octane.pdb`, `pentane.pdb` and `propane.pdb`,
    and the text from `propane.pdb` will be saved to a file called `alkanes.pdb`.
2.  Prints `cubane.pdb`, `ethane.pdb`, and `methane.pdb`, and the text from all three files would be
    concatenated and saved to a file called `alkanes.pdb`.
3.  Prints `cubane.pdb`, `ethane.pdb`, `methane.pdb`, `octane.pdb`, and `pentane.pdb`, and the text
    from `propane.pdb` will be saved to a file called `alkanes.pdb`.
4.  None of the above.

> *Solution*
>
> 1. The text from each file in turn gets written to the `alkanes.pdb` file.
> However, the file gets overwritten on each loop interation, so the final content of `alkanes.pdb`
> is the text from the `propane.pdb` file.


**Saving to a File in a Loop - Part Two**

Also in the `data-shell/molecules` directory, what would be the output of the following loop?

```shell
for datafile in *.pdb
> do
>     cat $datafile >> all.pdb
> done
```

1.  All of the text from `cubane.pdb`, `ethane.pdb`, `methane.pdb`, `octane.pdb`, and
    `pentane.pdb` would be concatenated and saved to a file called `all.pdb`.
2.  The text from `ethane.pdb` will be saved to a file called `all.pdb`.
3.  All of the text from `cubane.pdb`, `ethane.pdb`, `methane.pdb`, `octane.pdb`, `pentane.pdb`
    and `propane.pdb` would be concatenated and saved to a file called `all.pdb`.
4.  All of the text from `cubane.pdb`, `ethane.pdb`, `methane.pdb`, `octane.pdb`, `pentane.pdb`
    and `propane.pdb` would be printed to the screen and saved to a file called `all.pdb`.

> *Solution*
>
> 3 is the correct answer. `>>` appends to a file, rather than overwriting it with the redirected
> output from a command.
> Given the output from the `cat` command has been redirected, nothing is printed to the screen.


### Shell Scripts

**List Unique Species**

Leah has several hundred data files, each of which is formatted like this:

```text
2013-11-05,deer,5
2013-11-05,rabbit,22
2013-11-05,raccoon,7
2013-11-06,rabbit,19
2013-11-06,deer,2
2013-11-06,fox,1
2013-11-07,rabbit,18
2013-11-07,bear,1
```

An example of this type of file is given in `data-shell/data/animal-counts/animals.txt`.
 
We can use the command `cut -d , -f 2 animals.txt | sort | uniq`
to produce the unique species in `animals.txt`.
In order to avoid having to type out this series of commands every time,
a scientist may choose to write a shell script instead.

Write a shell script called `species.sh` that takes any number of
filenames as command-line arguments,
and uses a variation of the above command to print a list
of the unique species appearing in each of those files separately.

> *Solution*
>
> ```text
> # Script to find unique species in csv files where species is the second data field
> # This script accepts any number of file names as command line arguments
>
> # Loop over all files
> for file in $@ 
> do
> 	echo "Unique species in $file:"
> 	# Extract species names
> 	cut -d , -f 2 $file | sort | uniq
> done
> ```


**Why Record Commands in the History Before Running Them?**

If you run the command:

```shell
$ history | tail -n 5 > recent.sh
```

the last command in the file is the `history` command itself, i.e.,
the shell has added `history` to the command log before actually
running it. In fact, the shell *always* adds commands to the log
before running them. Why do you think it does this?

> *Solution*
>
> If a command causes something to crash or hang, it might be useful
> to know what that command was, in order to investigate the problem.
> Were the command only be recorded after running it, we would not
> have a record of the last command run in the event of a crash.


**Variables in Shell Scripts**

In the `molecules` directory, imagine you have a shell script called `script.sh` containing the
following commands:

```shell
head -n $2 $1
tail -n $3 $1
```

While you are in the `molecules` directory, you type the following command:

```shell
bash script.sh '*.pdb' 1 1
```

Which of the following outputs would you expect to see?

1. All of the lines between the first and the last lines of each file ending in `.pdb`
    in the `molecules` directory
2. The first and the last line of each file ending in `.pdb` in the `molecules` directory
3. The first and the last line of each file in the `molecules` directory
4. An error because of the quotes around `*.pdb`

> *Solution*
>
> The correct answer is 2. 
>
> The special variables $1, $2 and $3 represent the command line arguments given to the
> script, such that the commands run are:
>
> ```shell
> $ head -n 1 cubane.pdb ethane.pdb octane.pdb pentane.pdb propane.pdb
> $ tail -n 1 cubane.pdb ethane.pdb octane.pdb pentane.pdb propane.pdb
> ```
> 
> The shell does not expand `'*.pdb'` because it is enclosed by quote marks.
> As such, the first argument to the script is `'*.pdb'` which gets expanded within the
> script by `head` and `tail`.


**Find the Longest File With a Given Extension**

Write a shell script called `longest.sh` that takes the name of a
directory and a filename extension as its arguments, and prints
out the name of the file with the most lines in that directory
with that extension. For example:

```shell
$ bash longest.sh /tmp/data pdb
```

would print the name of the `.pdb` file in `/tmp/data` that has
the most lines.

> *Solution*
>
> ```text
> # Shell script which takes two arguments: 
> #    1. a directory name
> #    2. a file extension
> # and prints the name of the file in that directory
> # with the most lines which matches the file extension.
> 
> wc -l $1/*.$2 | sort -n | tail -n 2 | head -n 1
> ```


**Script Reading Comprehension**

For this question, consider the `data-shell/molecules` directory once again.
This contains a number of `.pdb` files in addition to any other files you
may have created.
Explain what each of the following three scripts would do when run as
`bash script1.sh *.pdb`, `bash script2.sh *.pdb`, and `bash script3.sh *.pdb` respectively.

```shell
# Script 1
echo *.*
```

```shell
# Script 2
for filename in $1 $2 $3
> do
>     cat $filename
> done
```

```shell
# Script 3
echo $@.pdb
```

> *Solution*
>
> In each case, the shell expands the wildcard in `*.pdb` before passing the resulting
> list of file names as arguments to the script.
>
> Script 1 would print out a list of all files containing a dot in their name.
> The arguments passed to the script are not actually used anywhere in the script.
>
> Script 2 would print the contents of the first 3 files with a `.pdb` file extension.
> `$1`, `$2`, and `$3` refer to the first, second, and third argument respectively.
> 
> Script 3 would print all the arguments to the script (i.e. all the `.pdb` files),
> followed by `.pdb`.
> `$@` refers to *all* the arguments given to a shell script.
> ```text
> cubane.pdb ethane.pdb methane.pdb octane.pdb pentane.pdb propane.pdb.pdb
> ```


### Finding Things

**Using `grep`**

Which command would result in the following output:

```text
and the presence of absence:
```

1. `grep "of" haiku.txt`
2. `grep -E "of" haiku.txt`
3. `grep -w "of" haiku.txt`
4. `grep -i "of" haiku.txt`

> *Solution*
>
> The correct answer is 3, because the `-w` option looks only for whole-word matches.
> The other options will also match "of" when part of another word.


**Tracking a Species**
 
Leah has several hundred 
data files saved in one directory, each of which is formatted like this:
 
```text
2013-11-05,deer,5
2013-11-05,rabbit,22
2013-11-05,raccoon,7
2013-11-06,rabbit,19
2013-11-06,deer,2
```

She wants to write a shell script that takes a species as the first command-line argument 
and a directory as the second argument. The script should return one file called `species.txt` 
containing a list of dates and the number of that species seen on each date.
For example using the data shown above, `rabbit.txt` would contain:
 
```text
2013-11-05,22
2013-11-06,19
```

Put these commands and pipes in the right order to achieve this:
 
```shell
cut -d : -f 2  
>  
|  
grep -w $1 -r $2  
|  
$1.txt  
cut -d , -f 1,3  
```

Hint: use `man grep` to look for how to grep text recursively in a directory
and `man cut` to select more than one field in a line.

An example of such a file is provided in `data-shell/data/animal-counts/animals.txt`

> *Solution*
>
> ```shell
> grep -w $1 -r $2 | cut -d : -f 2 | cut -d , -f 1,3  > $1.txt
> ```
>
> You would call the script above like this:
>
> ```shell
> $ bash count-species.sh bear .
> ```

**Little Women**

You and your friend, having just finished reading *Little Women* by
Louisa May Alcott, are in an argument.  Of the four sisters in the
book, Jo, Meg, Beth, and Amy, your friend thinks that Jo was the
most mentioned.  You, however, are certain it was Amy.  Luckily, you
have a file `LittleWomen.txt` containing the full text of the novel
(`data-shell/writing/data/LittleWomen.txt`).
Using a `for` loop, how would you tabulate the number of times each
of the four sisters is mentioned?

Hint: one solution might employ
the commands `grep` and `wc` and a `|`, while another might utilize
`grep` options.
There is often more than one way to solve a programming task, so a
particular solution is usually chosen based on a combination of
yielding the correct result, elegance, readability, and speed.

> *Solution*
>
> ```text
> for sis in Jo Meg Beth Amy
> do
> 	echo $sis:
>	grep -ow $sis LittleWomen.txt | wc -l
> done
> ```
>
> Alternative, slightly inferior solution:
> ```text
> for sis in Jo Meg Beth Amy
> do
> 	echo $sis:
>	grep -ocw $sis LittleWomen.txt
> done
> ```
>
> This solution is inferior because `grep -c` only reports the number of lines matched.
> The total number of matches reported by this method will be lower if there is more
> than one match per line.


**Matching and Subtracting**

The `-v` option to `grep` inverts pattern matching, so that only lines
which do *not* match the pattern are printed. Given that, which of
the following commands will find all files in `/data` whose names
end in `s.txt` (e.g., `animals.txt` or `planets.txt`), but do
*not* contain the word `net`?
Once you have thought about your answer, you can test the commands in the `data-shell`
directory.

1.  `find data -name '*s.txt' | grep -v net`
2.  `find data -name *s.txt | grep -v net`
3.  `grep -v "temp" $(find data -name '*s.txt')`
4.  None of the above.

> *Solution*
>
> The correct answer is 1. Putting the match expression in quotes prevents the shell
> expanding it, so it gets passed to the `find` command.
>
> Option 2 is incorrect because the shell expands `*s.txt` instead of passing the wildcard
> expression to `find`.
>
> Option 3 is incorrect because it searches the contents of the files for lines which
> do not match "temp", rather than searching the file names.


**`find` Pipeline Reading Comprehension**

Write a short explanatory comment for the following shell script:

```shell
wc -l $(find . -name '*.dat') | sort -n
```

> *Solution*
>
> 1. Find all files with a `.dat` extension recursively from the current directory
> 2. Count the number of lines each of these files contains
> 3. Sort the output from step 2. numerically


**Finding Files With Different Properties**
 
The `find` command can be given several other criteria known as "tests"
to locate files with specific attributes, such as creation time, size,
permissions, or ownership.  Use `man find` to explore these, and then
write a single command to find all files in or below the current directory
that are owned by the user `ahmed` and were modified in the last 24 hours.

Hint 1: you will need to use three tests: `-type`, `-mtime`, and `-user`.

Hint 2: The value for `-mtime` will need to be negative---why?

> *Solution*
>
> Assuming that Nelle’s home is our working directory we type:
>
> ```shell
> $ find ./ -type f -mtime -1 -user ahmed
> ```


## Key Points {#rse-bash-keypoints}

```{r, child="keypoints/bash.md"}
```

```{r, child="./links.md"}
```
