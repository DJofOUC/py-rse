# Bash Shell {#rse-bash}

At a high level, computers do four things:
run programs,
store data,
communicate with each other,
and interact with people.
They can do the last of these in many different ways,
of which a [graphical user interface][gui] (GUI) is the most widely used way.
The computer displays icons to show us our files and programs,
and we tell it to copy or run those by clicking with a mouse.
This style of interaction is easy to learn but hard to automate,
and doesn't create a record of what we did.

In contrast,
when we use a [command-line interface][cli] (CLI)
we communicate with the computer by typing commands,
and the computer responds by displaying text.
CLIs existed long before GUIs,
and have survived because they are efficient,
easy to automate,
and automatically create a record of what we did.

The heart of every CLI is a [read-evaluate-print loop][repl].
Its name comes from its basic operating cycle:
when we type a command and press <kbd>Return</kbd> (also known as <kbd>Enter</kbd>)
the CLI <strong>r</strong>eads the command,
<strong>e</strong>valuates it (i.e., executes it),
<strong>p</strong>rints its output,
then <strong>l</strong>oops around and waits for another command.

If you have used an interactive console for R or Python,
then you have already used a REPL and a simple CLI.
This lesson will introduce another kind of CLI that lets us interact with our computer's operating system.
It is called a [command shell][command-shell],
or just "shell" for short,
and in essence is a program that runs other programs on our behalf (FIXME: diagram).
Those "other programs" can do things as simple as telling us what time it is
or as complex as modeling global climate change;
as long as they obey a few simple rules,
the shell can run them without having to know what language they are written in
or how they do what they do.

Programmers have written many different shells over the last forty years.
The most popular on Unix systems today,
and in most packages that provide Unix-like tools for Windows,
is called Bash
(an acronym of <strong>B</strong>ourne <strong>A</strong>gain <strong>SH</strong>ell,
which is a pun on the name of its predecessor,
the Bourne shell).
Like other shells,
it allows us to combine programs to create pipelines
that can handle large volumes of data.
Sequences of commands can be saved in a [script][script],
just as commands for R or Python can be saved in programs,
which makes our workflows more reproducible.
Finally,
the command line is often the easiest way to interact with remote machines and supercomputers—in fact,
the shell is practically essential for working with clusters and the cloud.

## How can I explore my files and directories using the shell? {#rse-bash-explore}

When Bash runs it presents us with a [prompt][prompt] to indicate that it is waiting for input.
By default,
this prompt is a simple dollar sign:

```shell
$
```

However,
different shells may use a different symbol,
and as we'll see later,
we can customize the prompt to give us more information.

> **Don't Type the Dollar Sign**
>
> We show the `$` prompt so that it's clear what you are supposed to type,
> particularly when several commands appear in a row,
> but you should *not* type it yourself.

Let's run a command to find out who the shell thinks we are:

```shell
$ whoami
```

```text
amira
```

Amira is one of the learners described in Section \@ref(rse-intro-personas);
we will use her in the examples that follow.

Now that we know who we are,
we can explore where we are and what we have.
The part of the operating system responsible for managing files and directories (also called [folders][folder])
is called the [filesystem][filesystem].
Some of the most commonly-used commands in the shell create, inspect, rename, and delete files and directories.
Let's start exploring them by running the command `pwd`,
which stands for <strong>p</strong>rint <strong>w</strong>orking <strong>d</strong>irectory.
The "print" part of its name is straightforward;
the "working directory" part refers to the fact that
the shell keeps track of our [current working directory][current-working-directory] at all times.
Most commands read and write files in the current working directory
unless we tell them to do something else,
so knowing where we are before running a command is important.

```shell
$ pwd
```

```text
/Users/amira
```

Here,
the computer's response is `/Users/amira`,
which tells us that we are in a directory called `amira` that is contained in a top-level directory called `User`.
This directory is Amira's [home directory][home-directory],
and to understand what that phrase means,
we must first understand how the filesystem is organized.
On Amira's computer, it looks like this:

FIXME: figure (filesystem)

At the top is the [root directory][root-directory] that holds everything else,
which we can refer to using a slash character, `/` on its own.
Inside that directory are several other directories,
including `bin` (where some built-in programs are stored),
`data` (for miscellaneous data files),
`tmp` (for temporary files that don't need to be stored long-term),
and `Users` (where users' personal directories are located).
We know that `/Users` is stored inside the root directory `/` because its name begins with `/`,
and that our current working directory `/Users/amira` is stored inside `/Users`
because `/Users` is the first part of its name.
A string like this is called a [path][path] because it tells us
how to get from one place in the filesystem (in this case, the root directory)
to another (in this case, Amira's home directory).

> **Slashes**
>
> The `/` character means two different things in a path.
> At the front of a path or on its own,
> it refers to the root directory.
> When it appears inside a name, it is a separator.
> Windows uses backslashes (`\\ `) instead of forward slashes as separators.

Underneath `/Users`,
we find one directory for each user with an account on this machine.
Jun's files are stored in `/Users/jun`,
Sami's in `/Users/sami`,
and Amira's in `/Users/amira`.
This is where the name "home directory" comes from:
when we log in as a particular user,
the shell starts us off in the directory that holds that user's files.

> **Home Directory Variations**
>
> Our home directory will be different on different operating systems.
> On Linux it may look like `/home/amira`,
> and on Windows it may be `C:\Documents and Settings\amira` or `C:\Users\amira`
> (depending on the version of Windows).
> Our examples show what we would see on MacOS.

Now that we know where we are,
let's see what we have using the command `ls`,
which stands for "listing":

```shell
$ ls
```

```text
Applications Documents    Library      Music        Public         todo.txt
Desktop      Downloads    Movies       Pictures     climate-data
```

`ls` prints the names of the files and directories in the current directory.
(Again,
our results may be different depending on our operating system
and what files or directories we have.)

We can make the output of `ls` more comprehensible by using the `-F` [option][command-option]
(also sometimes called a [switch][command-switch] or a [flag][command-flag]).
Options are exactly like arguments to a function in R or Python;
in this case,
`-F` tells `ls` to add markers to its output to tell us what things are.
A trailing `/` indicates a directory,
while a trailing `*` tell us that something is a runnable program.
Depending on our setup,
the shell might also use colors to indicate whether each entry is a file or directory.

```shell
$ ls -F
```

```text
Applications/ Documents/    Library/      Music/        Public/        todo.txt
Desktop/      Downloads/    Movies/       Pictures/     climate-data/
```

Here,
we can see that almost everything in our home directory is a [sub-directory][sub-directory];
the only thing that isn't is a file called `todo.tt`.

> **Spaces Matter**
>
> `1+2` and `1 + 2` mean the same thing in mathematics,
> but `ls -F` and `ls-F` are very different things in the shell.
> The shell splits whatever we type into pieces based on spaces,
> so if we forget to separate `ls` and `-F` with at least one space,
> the shell will try to find a program called `ls-F` and (quite sensibly)
> give an error message like `ls-F: command not found`.

Some options tell a command how to behave,
but others tell it what to act on.
For example,
if we want to see what's in the `/Users` directory,
we can type:

```shell
$ ls /Users
```

```text
Amira   Jun     Sami
```

We often call the file and directory names that we give to commands [arguments][command-argument]
to distinguish them from the built-in options.
We can combine options and arguments:

```shell
$ ls -F /Users
```

```text
Amira/  Jun/    Sami/
```

but we must put the command's options (like `-F`)
before the names of any files or directories we want to work on
because once the command encounters one of the latter
it assumes that we are done giving it built-in options:

```shell
$ ls /Users -F
```

```text
ls: -F: No such file or directory
Amira   Jun     Sami
```

FIXME: behaves differently on Linux (?)

## How can I move around in the shell? {#rse-bash-navigate}

Let's run `ls` again to look at what's in our current working directory,
which is what the command shows if we don't ask it to show us anything else:

```shell
$ ls -F
```

```text
Applications/ Documents/    Library/      Music/        Public/        todo.txt
Desktop/      Downloads/    Movies/       Pictures/     climate-data/
```

If we want to see what's in `climate-data`,
we can ask `ls` to list its contents:

```shell
$ ls -F climate-data
```

```text
NOTES           bin/            docs/           thesis.md       tofu.config
README.md       cleaned/        raw/            thesis.pdf
```

Notice that `climate-data` doesn't have a leading slash before its name.
This absence tells the shell that it is a [relative path][relative-path],
i.e.,
that it should be interpreted starting from our current working directory.
In contrast,
a path like `/Users/amira` is an [absolute path][absolute-path]:
it is always interpreted from the root directory down,
so it always refers to the same thing.
Using a relative path is like telling someone to go two kilometers north and then half a kilometer east;
using an absolute path is like giving them the latitude and longitude of their destination.

We can use whichever kind of path is easiest to type,
but if we are going to do a lot of work with our climate data,
the easiest thing would be to change our current working directory
so that we don't have to type `climate-data` at all.
The command to do this is `cd`,
which stands for <strong>c</strong>hange <strong>d</strong>irectory.
This name is a bit misleading:
the command doesn't change the directory,
it changes the shell's idea of what directory we are in.
Let's try it out:

```shell
$ cd climate-data
```

`cd` doesn't print anything.
This is normal:
many shell commands run silently unless something goes wrong,
on the theory that they should only ask for our attention when they need it.
To confirm that `cd` has done what we asked,
we can use `pwd`:

```shell
$ pwd
```

```text
/Users/amira/climate-data
```

```shell
$ ls -F
```

```text
NOTES           bin/            docs/           thesis.md       tofu.config
README.md       cleaned/        raw/            thesis.pdf
```

> **Missing Directories and Unknown Options**
>
> If we give a command an option that it doesn't understand,
> it will usually print an error message and (if we're lucky)
> tersely remind us of what we should have done:
>
> ```shell
> $ cd -j
> ```
>
> ```text
> -bash: cd: -j: invalid option
> cd: usage: cd [-L|-P] [dir]
> ```
>
> If, on the other hand,
> we get the syntax right but make a mistake in the name of a file or directory,
> it will tell us that:
>
> ```shell
> $ cd whoops
> ```
>
> ```text
> -bash: cd: whoops: No such file or directory
> ```

We now know how to go down the directory tree,
but how do we go up?
This doesn't work:

```shell
$ cd amira
```

```text
cd: amira: No such file or directory
```

because `amira` on its own is a relative path meaning
"a file or directory called `amira` *below our current working directory*".
To get back home,
we can either use an absolute path:

```shell
$ cd /Users/amira
```

or a special relative path called `..` (two periods in a row with no spaces),
which always means "the directory that contains the current one".
The directory one level above the one we are in is called the [parent directory][parent-directory],
and sure enough,
`..` gets us there:

```shell
$ cd ..
$ pwd
```

```text
/Users/amira
```

`ls` usually doesn't show us this special directory—since it's always there,
displaying it every time would be a distraction.
We can ask `ls` to include it using the `-a` option,
which stands for "all":

```shell
$ ls -F -a
```

```text
./              Applications/   Documents/      Library/        Music/          Public/         todo.txt
../             Desktop/        Downloads/      Movies/         Pictures/       climate-data/
```

(Remember,
we are now in `/Users/amira`.)
The output also shows another special directory called `.` (a single period),
which refers to the current working directory.
It may seem redundant to have a name for it,
but we'll see some uses for it soon.

The special names `.` and `..` don't belong to `cd`:
they are interpreted the same way by every program.
For example,
if we are in `/Users/amira/climate-data`,
the command `ls ..` will display a listing of `/Users/amira`.
When the meanings of the parts are the same no matter how they're combined,
programmers say they are [orthogonal][orthogonality].
Orthogonal systems tend to be easier for people to learn
because there are fewer interactions and exceptions to keep track of.

> **Other Hidden Files**
>
> In addition to the hidden directories `..` and `.`,
> we may also comes across files with names like `.jupyter` or `.Rhistory`.
> These usually contain settings or other data for particular programs;
> the prefix `.` is used to prevent `ls` from cluttering up the output
> when we run `ls`.
> We can always use the `-a` option to display them.

`cd` is a simple command,
but it allows us to explore several new ideas.
First,
several `..` can be joined by the path separator
to move higher than the parent directory in a single step.
For example, `cd ../..` will move us up two directories
(e.g., from `/Users/amira/climate-data` to `/Users`),
while `cd ../Movies` will move us up from `climate-data` and back down into `Movies`.

What happens if we type `cd` on its own without giving a directory?

```shell
$ pwd
```

```text
/Users/amira/Movies
```

```shell
$ cd
$ pwd
```

```text
/Users/amira
```

This works no matter where we are:
`cd` on its own always returns us to our home directory.
We can achieve the same thing using the special directory name `~`,
which is a shortcut for our home directory:

```shell
$ ls ~
```

```text
Applications    Documents       Library         Music           Public          todo.txt
Desktop         Downloads       Movies          Pictures        climate-data 
```

(`ls` doesn't show any trailing slashes here because we haven't used `-F`.)
We can use `~` in paths,
so that (for example) `~/Downloads` always refers to our download directory.

Finally,
`cd` interprets the shortcut `-` (a single dash) to mean the last directory we were in.
Using this is usually faster and more reliable than trying to remember and type the path,
but unlike `~`,
it only works with `cd`:
`ls -` tries to print a listing of a directory called `-`
rather than showing us the contents of our previous directory.

## How can I create new files and directories? {#rse-bash-filedir}
 
We now know how to explore files and directories,
but how do we create them?
To find out,
let's go back to our `climate-data` directory:

```shell
$ cd ~/climate-data
$ ls -F
```

```text
NOTES           bin/            docs/           thesis.md       tofu.config
README.md       cleaned/        raw/            thesis.pdf
```

To create a new directory,
we use the command `mkdir` (short for <strong>m</strong>a<strong>k</strong>e <strong>dir</strong>irectory):

```shell
$ mkdir analysis
```

Since `analysis` is a relative path
(i.e., does not have a leading slash)
the new directory is created in the current working directory:

```shell
$ ls -F
```

```text
NOTES           analysis/       cleaned/        raw/            thesis.pdf
README.md       bin/            docs/           thesis.md       tofu.config
```

Using the shell to create a directory is no different than using a graphical tool.
If we look at the current directory with our computer's file browser
we will see the `analysis` directory there too.
The shell and the file explorer are two different ways of interacting with the files;
the files and directories themselves are the same.

> **Naming Files and Directories**
>
> Complicated names of files and directories can make our life painful.
> Following a few simple rules can save a lot of headaches:
>
> 1. **Don't use spaces.**
>    Spaces can make a name easier to read,
>    but since they are used to separate arguments on the command line,
>    most shell commands interpret a name like `My Thesis` as two names `My` and `Thesis`.
>    Use `-` or `_` instead,
>    e.g, `climate-data` or `climate_data`.
>
> 2. **Don't begin the name with `-` (dash)**
>    to avoid confusion with command options like `-F`.
>
> 3. **Stick with letters, digits, `.` (period or 'full stop'), `-` (dash) and `_` (underscore).**
>    Many other characters mean special things in the shell.
>    We will learn about some of these during this lesson,
>    but these are always safe.
>
> If we need to refer to files or directories that have spaces or other special characters in their names,
> we can surround the name in quotes (`""`).
> For example, `ls "My Thesis"` will work where `ls My Thesis` does not.

Since we just created the `thesis` directory,
it doesn't contain anything:

```shell
$ ls -F analysis
```

Let's change our working directory to `analysis` using `cd`,
then use a very simple text editor called [Nano][nano] to create a file called `draft.txt`:

```shell
$ cd analysis
$ nano draft.txt
```

We could just as easily have run `nano analysis/draft.txt` to edit the file.
What's more important is that when we say "Nano is a text editor" we really do mean "text":
it can only work with plain character data,
not spreadsheets, images, Microsoft Word files, or anything else invented after 1970.
We use it in this lesson because it runs everywhere
and because it is as simple as something can be and still be called an editor.
However,
that last trait means that we *shouldn't* use it for larger tasks,
such as writing a program or a paper.
Chapter \@ref(tools) discusses some alternatives.

> **Recycling Pixels**
>
> Unlike most modern editors,
> Nano runs *inside* the shell window instead of opening a new window of its own.
> This is a holdover from an era when graphical terminals were a rarity
> and different applications had to share a single character-only screen.

Once Nano is open we can type in a few lines of text,
then press <kbd>Ctrl</kbd>+<kbd>O</kbd>
(the Control key and the letter 'O' at the same time)
to save our work.
Nano will ask us what file we want to save it to;
press <kbd>Return</kbd> to accept the suggested default of `draft.txt`.
Once our file is saved,
we can use <kbd>Ctrl</kbd>+<kbd>X</kbd> to exit the editor and return to the shell.

FIXME: figure (Nano)

> **Control, Ctrl, or ^ Key**
>
> The Control key,
> also called the "Ctrl" key,
> can be described in a bewildering variety of ways.
> For example,
> holding down <kbd>Control</kbd> and <kbd>X</kbd> at the same time
> may be written as any of:
>
> -   `Control-X`
> -   `Control+X`
> -   `Ctrl-X`
> -   `Ctrl+X`
> -   `C-x`
> -   `^X`
>
> When Nano runs
> it displays some help in the bottom two lines of the screen
> using the last of these notations:
> for example,
> `^G Get Help` means "use <kbd>Control</kbd>+<kbd>G</kbd> to get help"
> and `^O WriteOut` means "use <kbd>Control</kbd>+<kbd>O</kbd> to write out the current file".

Nano doesn't leave any output on the screen after it exits,
but we can use `ls` to show that we have indeed created a new file `draft.txt`:
but `ls` now shows that we have created a file called `draft.txt`:

```shell
$ ls
```

```text
draft.txt
```

> **What's In A Name?**
>
> All of Amira's files are named "something dot something".
> This is just a convention:
> we can call a file `mythesis` or almost anything else.
> However,
> most people use two-part names to help them (and their programs)
> tell different kinds of files apart.
> The second part of such a name is called the [filename extension][filename-extension]
> and indicates what type of data the file holds:
> `.txt` for plain text,
> `.pdf` for a PDF document,
> `.png` for a PNG image, and so on.
> This is just a convention:
> saving a PNG image of a whale as `whale.mp3`
> doesn't somehow magically turn it into a recording of whalesong,
> though it *might* cause the operating system to try to open it with a music player
> when someone double-clicks it.

## How can move files and directories around? {#rse-bash-move}

Let's go back to our `climate-data` directory:

```shell
cd ~/climate-data
```

The `analysis` directory contains a file called `draft.txt`.
That isn't a particularly informative name,
so let's change it using `mv` (short for <strong>m</strong>o<strong>v</strong>e):

```shell
$ mv analysis/draft.txt analysis/prior-work.txt
```

The first argument tells `mv` what we are "moving",
while the second is where it's to go:
here,
moving `analysis/draft.txt` to `analysis/prior-work.txt`
has the same effect as renaming the file:

```shell
$ ls analysis
```

```text
prior-work.txt
```

We must be careful when specifying the destination
because `mv` will silently overwrite any existing file with the same name.
An option `-i` (for "interactive") makes `mv` ask us for confirmation before overwriting.
`mv` also works on directories,
so `mv analysis first-paper` would rename the directory without changing its contents.

Now suppose we want to move `prior-work.txt` into the current working directory.
If we don't want to change the file's name,
just its location,
we can provide `mv` with a directory as a destination
and it will move the file there.
In this case,
the directory we want is the special name `.` that we mentioned earlier:

```shell
$ mv analysis/prior-work.txt .
```

`ls` now shows us that `analysis` is empty:

```shell
$ ls analysis
```

and that our current directory now contains our file:

```shell
$ ls
```

```text
NOTES           analysis/       cleaned/        prior-work.txt  thesis.md       tofu.config
README.md       bin/            docs/           raw/            thesis.pdf
```

If we only want to check that the file exists,
we can give its name to `ls`
just like we can give the command the name of a directory:

```shell
$ ls prior-work.txt
```

```text
prior-work.txt
```

## How can I copy files and directories? {#rse-bash-copy}

The `cp` command
(for <strong>c</strong>o<strong>p</strong>y,
and no, we don't know why the creators of Unix seemed to be allergic to vowels)
works like `mv` except it copies a file instead of moving it.
We can check that it did the right thing using `ls` with two arguments:

```shell
$ cp prior-work.txt analysis/section-1.txt
$ ls prior-work.txt analysis/section-1.txt
```

```text
analysis/section-1.txt  prior-work.txt
```

Notice that `ls` shows the output in alphabetical order.
If we leave off the second filename and ask it to show us a file and a directory
(or multiple directories)
it lists them one by one:

```shell
$ ls prior-work.txt analysis
```

```text
prior-work.txt

analysis:
section-1.txt
```

Copying a directory and its content is a little more complicated.
If we use `cp` on its own,
we get an error message:

```shell
$ cp analysis backup
```

```text
cp: analysis is a directory (not copied).
```

If we really want to copy everything,
we must give `cp` the `-r` option (meaning [<strong>r</strong>ecursive][recursion]:

```shell
$ cp -r analysis backup
```

Once again we can check the result with `ls`:

```shell
$ ls analysis backup
```

```text
analysis/:
section-1.txt

backup/:
section-1.txt
```

## How can I delete files and directories? {#rse-bash-rm}

Let's tidy up by removing the `prior-work.txt` file we created in our `climate-data` directory.
The command to do this is `rm` (for <strong>r</strong>e<strong>m</strong>ove):

```shell
$ rm prior-work.txt
```

We can confirm the file has gone using `ls`:

```shell
$ ls prior-work.txt
```

```text
ls: prior-work.txt: No such file or directory
```

Deleting is forever:
the Unix shell doesn't have a trash bin that we can recover deleted files from
(though most graphical interfaces for Unix do),
so when we delete a file,
it really is gone—or at least gone-ish.
Tools for finding and recovering deleted files do exist,
but there is no guarantee they will work,
since the computer may recycle the file's disk space at any time.

In a half-hearted attempt to stop us from erasing things accidentally,
`rm` refuses to delete directories:

```shell
$ rm analysis
```

```text
rm: analysis: is a directory
```

`rm` can remove a directory and everything it contains
if we use the recursive option `-r`:

```shell
$ rm -r analysis
```

`rm -r` should be used with great caution:
in most cases,
it's safest to add the `-i` option (for <strong>i</strong>nteractive)
to get `rm` to ask us to confirm each deletion.
As a halfway measure,
we can use `-v` (for <strong>v</strong>erbose)
to get `rm` to print a message for each file it deletes.
This options works the same way with `mv` and `cp`.

## How can I run commands on lots of files at once? {#rse-bashwildcard}

The `cleaned` directory in our climate data project contains
precipitation and temperature records from four weather stations in Australia:

```shell
$ ls cleaned
```

```text
andamooka_prec.csv      badingarra_prec.csv     bellambi_prec.csv       tuggeranong_prec.csv
andamooka_temp.csv      badingarra_temp.csv     bellambi_temp.csv       tuggeranong_temp.csv
```

The `wc` command (short for <strong>w</strong>ord <strong>c</strong>ount)
can tell us how many lines, words, and letters there are in one of these files:

```shell
$ wc cleaned/andamooka_prec.csv
```

```text
20003   20004  401858 cleaned/andamooka_prec.csv
```

> **What's in a Word?**
>
> The number of lines and number of words in this file are almost the same
> because `wc` only considers spaces to be word breaks, not commas,
> and the only space in this file occurs in the name of one of the columns.

We could run `wc` seven more times to count find out how many lines there are in the other files,
but that would be a lot of typing
and we would probably make at least one mistake.
We can't just give `wc` the name of the directory as we do with `ls`:

```shell
$ wc cleaned
```

```text
wc: cleaned: read: Is a directory
```

Instead,
we can use [wildcards][wildcard] to select a set of filenames at once.
The most commonly-used wildcard is `*`;
it matches zero or more characters,
so `cleaned/*.csv` matches all of the files in the `cleaned` directory:

```text
$ ls cleaned/*.csv
```

```text
cleaned/andamooka_prec.csv       cleaned/badingarra_prec.csv      cleaned/bellambi_prec.csv        cleaned/tuggeranong_prec.csv
cleaned/andamooka_temp.csv       cleaned/badingarra_temp.csv      cleaned/bellambi_temp.csv        cleaned/tuggeranong_temp.csv
```

while `cleaned/b*.csv` only matches the four whose names begin with a 'b':

```shell
$ ls cleaned/b*.csv
```

```text
cleaned/badingarra_prec.csv      cleaned/badingarra_temp.csv      cleaned/bellambi_prec.csv        cleaned/bellambi_temp.csv
```

Wildcards are expanded to match filenames *before* commands are run,
so they work exactly the same way for every command.
This means that we can use them with `wc` to (for example)
count the number of records in the precipitation files:

```shell
$ wc cleaned/*_prec.csv
```

```text
   20003   20004  401858 cleaned/andamooka_prec.csv
   21099   21100  406584 cleaned/badingarra_prec.csv
    8315    8316  169740 cleaned/bellambi_prec.csv
    8681    8682  175998 cleaned/tuggeranong_prec.csv
   58098   58102 1154180 total
```

or the number of records in the files from Tuggeranong:

```shell
$ wc cleaned/tug*.csv
```

```text
    8681    8682  175998 cleaned/tuggeranong_prec.csv
    8680    8681  196212 cleaned/tuggeranong_temp.csv
   17361   17363  372210 total
```

The exercises will introduce and explore other wildcards
(such as `?`, which matches exactly one character).
For now,
the only other thing we need to know is that
it's possible for a wildcard expression to *not* match anything.
In this case,
the command will usually print an error message:

```shell
$ wc cleaned/*.txt
```

```text
wc: cleaned/*.txt: open: No such file or directory
```

## How can I find out what commands there are and how to use them? {#rse-bash-help}

By default,
`wc` displays lines, words, and characters,
but we can ask it to display only a count of lines:

```shell
$ wc -l cleaned/*_prec.csv
```

```text
   20003 cleaned/andamooka_prec.csv
   21099 cleaned/badingarra_prec.csv
    8315 cleaned/bellambi_prec.csv
    8681 cleaned/tuggeranong_prec.csv
   58098 total
```

`wc` has other options as well.
We can use the `man` command (short for <strong>man</strong>ual)
to find out what they are:

```shell
$ man wc
```

FIXME: typeset the text below as a figure with callouts pointing at its parts.

```text
WC(1)                     BSD General Commands Manual                    WC(1)

NAME
     wc -- word, line, character, and byte count

SYNOPSIS
     wc [-clmw] [file ...]

DESCRIPTION
     The wc utility displays the number of lines, words, and bytes contained
     in each input file, or standard input (if no file is specified) to the
     standard output.  A line is defined as a string of characters delimited
     by a <newline> character.  Characters beyond the final <newline> charac-
     ter will not be included in the line count.

     A word is defined as a string of characters delimited by white space
     characters.  White space characters are the set of characters for which
     the iswspace(3) function returns true.  If more than one input file is
     specified, a line of cumulative counts for all the files is displayed on
     a separate line after the output for the last file.

     The following options are available:

     -c      The number of bytes in each input file is written to the standard
             output.  This will cancel out any prior usage of the -m option.

     -l      The number of lines in each input file is written to the standard
             output.

     -m      The number of characters in each input file is written to the
             standard output.  If the current locale does not support multi-
             byte characters, this is equivalent to the -c option.  This will
             cancel out any prior usage of the -c option.

     -w      The number of words in each input file is written to the standard
             output.

     When an option is specified, wc only reports the information requested by
     that option.  The order of output always takes the form of line, word,
     byte, and file name.  The default action is equivalent to specifying the
     -c, -l and -w options.

     If no files are specified, the standard input is used and no file name is
     displayed.  The prompt will accept input until receiving EOF, or [^D] in
     most environments.

ENVIRONMENT
     The LANG, LC_ALL and LC_CTYPE environment variables affect the execution
     of wc as described in environ(7).

EXIT STATUS
     The wc utility exits 0 on success, and >0 if an error occurs.

EXAMPLES
     Count the number of characters, words and lines in each of the files
     report1 and report2 as well as the totals for both:

           wc -mlw report1 report2

COMPATIBILITY
     Historically, the wc utility was documented to define a word as a "maxi-
     mal string of characters delimited by <space>, <tab> or <newline> charac-
     ters".  The implementation, however, did not handle non-printing charac-
     ters correctly so that "  ^D^E  " counted as 6 spaces, while
     "foo^D^Ebar" counted as 8 characters.  4BSD systems after 4.3BSD modi-
     fied the implementation to be consistent with the documentation.  This
     implementation defines a "word" in terms of the iswspace(3) function,
     as required by IEEE Std 1003.2 ("POSIX.2").

SEE ALSO
     iswspace(3)

STANDARDS
     The wc utility conforms to IEEE Std 1003.1-2001 ("POSIX.1").

HISTORY
     A wc command appeared in Version 1 AT&T UNIX.

BSD                            February 23, 2005                           BSD
```

> **Navigating the Manual**
>
> If our screen is too small to display an entire manual page at once,
> the shell will use a [pager][pager] called `less` to show it piece by piece.
> We can use <kbd>↑</kbd> and <kbd>↓</kbd> to move line-by-line
> or <kbd>Ctrl</kbd>+<kbd>Spacebar</kbd> and <kbd>Spacebar</kbd>
> to skip up and down one page at a time.
> (<kbd>B</kbd> and <kbd>F</kbd> also work.)
> 
> To search for a character or word,
> use <kbd>/</kbd> followed by the character or word to search for.
> If the search produces multiple hits,
> we can move between them using <kbd>N</kbd> (for "next").
> To quit, press <kbd>Q</kbd>.

This is a lot of information,
most of which isn't really useful.
Some commands have a `--help` option to provide a succinct summary of possibilites,
but the best place to go for help these days is probably the [TLDR][tldr] website.
The acronym stands for "too long, didn't read",
and its help for `wc` displays this:

```
wc
Count words, bytes, or lines.

Count lines in file:
wc -l {{file}}

Count words in file:
wc -w {{file}}

Count characters (bytes) in file:
wc -c {{file}}

Count characters in file (taking multi-byte character sets into account):
wc -m {{file}}

edit this page on github
```

As the last line suggests,
all of its examples are in a public GitHub repository
so that users like you can add the ones you wish it had.
For more information,
we can search on [Stack Overflow][so-bash]
or browse the [GNU manuals][gnu-man]
(particularly those for the [core GNU utilities][gnu-man-coreutils],
which include many of the commands introduced in this lesson).
In all cases,
though,
we need to have some idea of what we're looking for in the first place:
someone who wants to know how many lines there are in a data file
is unlikely to think to look for `wc`.

## How can I combine commands? {#rse-bash-pipe}

Now that we know a few basic commands,
we can introduce one of the shell's most powerful features:
the ease with which it lets us combine existing programs in new ways.
Let's go into the `climate-data/cleaned` directory
and count the number of lines in each file once again:

```shell
$ cd ~/climate-data/cleaned
$ wc -l *.csv
```

```text
   20003 andamooka_prec.csv
   18541 andamooka_temp.csv
   21099 badingarra_prec.csv
   20002 badingarra_temp.csv
    8315 bellambi_prec.csv
    8314 bellambi_temp.csv
    8681 tuggeranong_prec.csv
    8680 tuggeranong_temp.csv
  113635 total
```

Which of these files is shortest?
It's easy to see when there are only eight files,
but what if there were eight thousand?
Our first step toward a solution is to run the command:

```shell
$ wc -l *.csv > lengths.txt
```

The greater than symbol `>` tells the shell to [redirect][redirection] the command's output to a file
instead of printing it.
Nothing appears on the screen:
everything that would have appeared has gone into the file `lengths.txt` instead.
The shell creates this file if it doesn't exist,
or overwrites it if it already exists.
`ls lengths.txt` confirms that the file exists:

```shell
$ ls lengths.txt
```

```text
lengths.txt
```

We can print the contents of `lengths.txt` using `cat`
(short for con<strong>cat</strong>enate).
Its name comes from the fact that it will print as many files as we give it,
one after the other.
If we give it just one name,
all we get is that file:

```shell
$ cat lengths.txt
```

```text
   20003 andamooka_prec.csv
   18541 andamooka_temp.csv
   21099 badingarra_prec.csv
   20002 badingarra_temp.csv
    8315 bellambi_prec.csv
    8314 bellambi_temp.csv
    8681 tuggeranong_prec.csv
    8680 tuggeranong_temp.csv
  113635 total
```

We can now use `sort` to sort the lines in this file:

```shell
$ sort lengths.txt
```

```text
    8314 bellambi_temp.csv
    8315 bellambi_prec.csv
    8680 tuggeranong_temp.csv
    8681 tuggeranong_prec.csv
   18541 andamooka_temp.csv
   20002 badingarra_temp.csv
   20003 andamooka_prec.csv
   21099 badingarra_prec.csv
  113635 total
```

Just to be safe,
we should use `sort`'s `-n` option to specify that we want to sort numerically;
without this,
`sort` would sort things alphabetically
and `10` would come before `2`.

`sort` does not change `lengths.txt`,
but it sends its output to the screen just as `wc` did.
We can therefore put the sorted list of lines in another temporary file called `sorted-lengths.txt` using `>`:

```shell
$ sort lengths.txt > sorted-lengths.txt
```

> **Redirecting to the Same File**
>
> It's tempting to send the output of `sort` back to the file it reads:
>
> ```shell
> $ sort -n lengths.txt > lengths.txt
> ```
>
> However, all this does is wipe out the contents of `lengths.txt`.
> The reason is that when the shell sees the redirection,
> it opens the file on the right of the `>` for writing,
> which erases anything that file contained.
> It then runs `sort`, which finds itself reading from a newly-empty file.

Creating intermediate files with names like `lengths.txt` and `sorted-lengths.txt` works,
but keeping track of those files and cleaning them up when they're no longer needed is a burden.
We can produce the same result more safely and with less typing using a [pipe][unix-pipe]:

```shell
$ wc -l *.csv | sort -n
```

```text
    8314 bellambi_temp.csv
    8315 bellambi_prec.csv
    8680 tuggeranong_temp.csv
    8681 tuggeranong_prec.csv
   18541 andamooka_temp.csv
   20002 badingarra_temp.csv
   20003 andamooka_prec.csv
   21099 badingarra_prec.csv
  113635 total
```

The vertical bar `|` between the `wc` and `sort` commands
tells the shell that we want to use the output of the command on the left
as the input to the command on the right.
Putting it another way,
the downstream command doesn't read from a file;
instead,
it reads the output of the upstream command.

We can use `|` to build pipes of any length.
For example,
we can use the command `head` to get just the first three lines of sorted data:

```shell
$ wc -l *.csv | sort -n | head -n 3
```

```text
    8314 bellambi_temp.csv
    8315 bellambi_prec.csv
    8680 tuggeranong_temp.csv
```

If we want the last line of this,
we can add a call to the `tail` command:

```shell
$ wc -l *.csv | sort -n | head -n 3 | tail -n 1
```

```text
    8680 tuggeranong_temp.csv
```

and finally save our result:

```shell
$ wc -l *.csv | sort -n | head -n 3 | tail -n 1 > results.txt
```

In practice,
most Unix users would create this pipeline exactly as we have,
by starting with a single command and adding others one by one,
checking the output after each change.
The shell makes this easy by letting us move up and down in our [command history][unix-command-history]
using the <kbd>↑</kbd> and <kbd>↓</kbd> keys
and editing old commands to create new ones.

## How do pipes work (and why do I need to know)? {#rse-bash-stdio}

In order to use pipes and redirection effectively,
we need to know just a little about how they work.
When a computer runs a program—any program—it creates a [process][process] in memory
to hold the program's instructions and data.
Every process in Unix has an input channel called [standard input][standard-input]
and an output channel called [standard output][standard-output].
(By this point you may be surprised that their names are so memorable, but don't worry:
most Unix programmers call them [stdin][stdin] and [stdout][stdout]).

The shell is a program like any other,
and like any other,
it runs inside a process.
Under normal circumstances its standard input is connected to our keyboard
and its standard output to our screen,
so it reads what we type and displays its output for us to see (FIXME: diagram).
When we tell the shell to run a program
it creates a new process
and temporarily reconnects the keyboard and stream
to that process's standard input and output (FIXME: diagram).

Redirection with `>` tells the shell to connect the program's standard output to a file
instead of to the screen (FIXME: diagram).
If we provide one or more files for the command to read,
as with `sort lengths.txt`,
the program reads data from those files.
If we don't provide any filenames,
though,
the Unix convention is for the program to read from standard input.
We can test this by running `sort` on its own,
typing in a few lines of text,
and then pressing <kbd>Ctrl</kbd>+<kbd>D</kbd> to signal the end of input.
`sort` will then sort and print whatever we typed:

```shell
$ sort
one
two
three
four
^D
```

```text
four
one
three
two
```

When we create a pipe like `wc *.csv | sort`,
the shell creates one process for each command so that `wc` and `sort` will run simultaneously,
and then connects the standard output of `wc` directly to the standard input of `sort` (FIXME: diagram).
`wc` doesn't know whether its output is going to the screen,
another program,
or to a file via `>`.
Equally,
`sort` doesn't know if its input is coming from the keyboard or another process;
it just knows that it has to read, sort, and print.

> **Why Isn't It Doing Anything?**
>
> What happens if a command is supposed to process a file
> but we don't give it a filename?
> For example, what if we type:
>
> ```shell
> $ wc -l
> ```
>
> but don't type `*.csv` (or anything else) after the command?
> Since it doesn't have any filenames,
> `wc` assumes it is supposed to read from the keyboard,
> so it waits for us to type in some data.
> It doesn't tell us this:
> it just sits and waits.
>
> This mistake can be hard to spot,
> particularly if we put the filename at the end of the pipeline:
>
> ```shell
> $ wc -l | sort bellambi_temp.csv
> ```
>
> In this case,
> `sort` ignores standard input and reads the data in the file,
> but `wc` still just sits there waiting for input.
>
> If we make this mistake,
> we can end the program by typing <kbd>Ctrl</kbd>+<kbd>C</kbd>.
> We can also use this to interrupt programs that are taking a long time to run
> or are trying to connect to a website that isn't responding.

Just as we can redirect standard output with `>`,
we can connect standard input to a file using `<`.
In the case of a single file,
this has the same effect as providing the file's name to the command:

```shell
$ wc < bellambi_temp.csv
```

```text
    8314    8316  184203
```

If we try to use redirection with a wildcard,
though,
the shell *doesn't* concatenate all of the matching files:

```shell
$ wc < *.csv
```

```text
-bash: *.csv: ambiguous redirect
```

It also doesn't print the error message to standard output,
which we can prove by redirecting:

```shell
$ wc < *.csv > all.txt
```

```text
-bash: *.csv: ambiguous redirect
```

```shell
$ cat all.txt
```

```text
cat: all.txt: No such file or directory
```

Every process has a second output channel called [standard error][standard-error] (or [stderr][stderr]).
Programs use it for error messages
so that their attempts to tell us something has gone wrong don't vanish silently into an output file.
There are ways to redirect standard error,
but doing so is almost always a bad idea.

## How can I do the same operations on many files? {#rse-bash-loops}

A [loop][unix-loop] is a way to repeat a set of commands for each item in a list.
We can use them to build complex workflows out of simple pieces,
and,
like wildcards,
they reduce the typing we have to do and the number of mistakes we might make.

Let's suppose that we want to take a slice out of each temperature data file in the `cleaned` directory—
more specifically,
that we want to get the first three rows of each file
*without* the header row.
If we only cared about one file,
we could use a simple pipeline to take the first four lines
and then take the last three of those:

```shell
$ head -n 4 andamooka_temp.csv | tail -n 3
```

```text
16065,1969,1,1,,
16065,1969,1,2,,
16065,1969,1,3,,
```

If we try to use a wildcard to select files,
we only get three lines of output,
not the 12 we expect:

```shell
$ head -n 4 *.csv | tail -n 3
```

```text
70339,1996,1,1,,
70339,1996,1,2,,
70339,1996,1,3,,
```

The problem is that `head` is producing a single stream of output
containing four lines for each file
(along with a header telling us the file's name):

```shell
$ head -n 4 *.csv
```

```text
==> andamooka_temp.csv <==
Station,Year,Month,Day,Max Temp (C),Quality
16065,1969,1,1,,
16065,1969,1,2,,
16065,1969,1,3,,

==> badingarra_temp.csv <==
Station,Year,Month,Day,Max Temp (C),Quality
9037,1965,1,1,40.6,Y
9037,1965,1,2,40,Y
9037,1965,1,3,41.1,Y

==> bellambi_temp.csv <==
Station,Year,Month,Day,Max Temp (C),Quality
68228,1997,1,1,,
68228,1997,1,2,,
68228,1997,1,3,,

==> tuggeranong_temp.csv <==
Station,Year,Month,Day,MaxTemp (C),Quality
70339,1996,1,1,,
70339,1996,1,2,,
70339,1996,1,3,,
```

Let's try this instead:

```shell
$ for filename in andamooka_temp.csv badingarra_temp.csv
> do
>   head -n 4 $filename
> done
```

```text
Station,Year,Month,Day,Max Temp (C),Quality
16065,1969,1,1,,
16065,1969,1,2,,
16065,1969,1,3,,
Station,Year,Month,Day,Max Temp (C),Quality
9037,1965,1,1,40.6,Y
9037,1965,1,2,40,Y
9037,1965,1,3,41.1,Y
```

There is a lot going on here,
so we will break it down into pieces:

1.  The keywords `for`…`in`…`do`…`done` create the loop,
    and must always appear in that order.

2.  `filename` is a [variable][variable].
    At any moment it contains a value,
    which can change over time.

3.  The loop runs once for each item in the list.
    Each time it runs,
    it assigns the next item to the variable,
    so in this case,
    `filename` will be `andamooka_temp.csv` the first time around
    and `badingarra_temp.csv` the second time.

4.  The commands that the loop executes (which are called the [body][loop-body] of the loop)
    appear between `do` and `done`.
    Those commands use the current value of the variable `filename`,
    but to get it,
    we must put a dollar sign `$` in front of the variable's name.
    If we forget and use `filename` instead of `$filename`,
    the shell will think that we are actually referring to something called `filename`.

5.  The shell prompt changes from `$` to `>` and back again as we type in our loop.
    The second prompt is called a [continuation prompt][continuation-prompt];
    it is different to remind us that we haven't finished typing a complete command yet.

So what does the loop actually do?
As the output shows,
it runs our little pipeline separately for each file.
Let's use a wildcard expression to select all four temperature files
and add another stage to the pipeline:

```shell
$ for filename in *_temp.csv
> do
>   head -n 4 $filename | tail -n 3
> done
```

```text
16065,1969,1,1,,
16065,1969,1,2,,
16065,1969,1,3,,
9037,1965,1,1,40.6,Y
9037,1965,1,2,40,Y
9037,1965,1,3,41.1,Y
68228,1997,1,1,,
68228,1997,1,2,,
68228,1997,1,3,,
70339,1996,1,1,,
70339,1996,1,2,,
70339,1996,1,3,,
```

The weather station IDs in the first column show that
we are in fact getting exactly three rows from each of four files.

> **Same Symbols, Different Meanings**
>
> The computer displays `>` as a shell continuation prompt when we are typing,
> but when we type `>` ourselves,
> it means "redirect output".
> Similarly,
> `$` is a regular prompt when the computer prints it
> but means "get the value of a variable" when we type.
> This [overloading][overloading] is sometimes confusing,
> but there are only so many symbols on a standard keyboard.

## What do variable names mean? {#rse-bash-meaningless}

The short answer to this section's title is "nothing".
We have called the variable in this loop `filename`
to make its purpose clear to human readers,
but the shell doesn't care what the variable is called.
We could equally well write our loop as:

```shell
$ for f in *_temp.csv
> do
>   head -n 4 $f | tail -n 3
> done
```

or:

```shell
$ for username in *_temp.csv
> do
>   head -n 4 $username | tail -n 3
> done
```

*Don't do this.*
Programs are only useful if people can understand them,
so meaningless names like `f` and misleading names like `username`
increase the odds of misunderstanding.

## How can I re-do things I have done recently? {#rse-bash-history}

Loops are useful if we know in advance what we want to repeat,
but if we have already run commands,
we can still repeat.
One way is to use <kbd>↑</kbd> and <kbd>↓</kbd> to go up and down in our command history as described earlier.
Another is to use the `history` command to get a list of the last few hundred commands that have been executed:

```shell
$ history
```

```text
  ...
  571  wc -l *.csv | sort -n | head n 3
  572  wc -l *.csv | sort -n | head -n 3
  573  wc -l *.csv | sort -n | head -n 3 | tail -n 1
  ...
```

and then use an exclamation mark `!` followed by a number to repeat one of those commands:

```shell
$ !572
```

```shell
wc -l *.csv | sort -n | head -n 3
    8314 bellambi_temp.csv
    8315 bellambi_prec.csv
    8680 tuggeranong_temp.csv
```

The shell prints the command it is going to run to standard error rather than to standard output,
so that (for example) `!572 > results.txt` puts the command's output into a file
*without* also writing the command.

Having an accurate record of what we have done
and a simple way to repeat it
are yet another reason why people use the Unix shell.
In fact,
being able to repeat history is such a powerful idea
that the shell gives us several ways to do it.

-   `!head` re-runs the most recent command starting with `head`,
    while `!wc` re-runs the most recent starting with `wc`.
-   If we type <kbd>Ctrl</kbd>+<kbd>R</kbd>
    the shell does an interactive reverse search for whatever we type next.
    If we don't like the first thing it finds,
    we can type <kbd>Ctrl</kbd>+<kbd>R</kbd> again to go further back.

If we use `history`, <kbd>↑</kbd>, or <kbd>Ctrl</kbd>+<kbd>R</kbd>,
we will quickly notice that loops don't have to be broken across lines.
Instead,
their parts can be separated with semi-colons:

```shell
$ for filename in *_temp.csv; do head -n 4 $filename | tail -n 3; done
```

This is fairly readable,
although even experienced users have a tendency to put the semi-colon after `do` instead of before it.
If our loop contains multiple commands,
though,
the multi-line format is much easier to read—compare this:

```shell
$ for filename in *_temp.csv
> do
>   echo $filename
>   head -n 4 $filename | tail -n 3
> done
```

with this:

```shell
$ for filename in *_temp.csv; do echo $filename; head -n 4 $filename | tail -n 3; done
```

## How can I create new filenames automatically? {#rse-bash-autoname}

Suppose we want to create a backup copy of each precipitation data file.
If we don't want to change the files' names,
we can do this with `cp`:

```shell
$ cd ~/climate-data
$ mkdir backup
$ cp cleaned/*_prec.csv backup
```

But what if we want the backup files to have the [extension][filename-extension] `.bak` instead of `.csv`?
`cp` can do this for a single file:

```shell
$ cp cleaned/andamooka_prec.csv backup/andamooka_prec.bak
```

but can't do all the files at once:

```shell
$ cp cleaned/*_prec.csv backup/*_prec.bak
```

```text
usage: cp [-R [-H | -L | -P]] [-fi | -n] [-apvXc] source_file target_file
       cp [-R [-H | -L | -P]] [-fi | -n] [-apvXc] source_file ... target_directory
```

`backup/*_prec.bak` doesn't match anything—those files don't yet exist—so
what we are actually asking `cp` to do is:

```shell
$ cp cleaned/andamooka_prec.csv cleaned/badingarra_prec.csv cleaned/bellambi_prec.csv cleaned/tuggeranong_prec.csv
```

This doesn't work because `cp` only understands how to do two things:
copy a single file to create another file,
or copy a bunch of files into a directory.
If we give it more than two names as arguments,
it expects the last one to be a directory;
since it isn't,
it reports an error.

Instead,
let's copy all the files and then rename them:

```shell
$ cp cleaned/*_prec.csv backup
$ for filename in backup/*.csv
> do
>   mv $filename $filename.bak
> done
$ ls backup
```

```text
andamooka_prec.csv.bak badingarra_prec.csv.bak bellambi_prec.csv.bak tuggeranong_prec.csv.bak
```

## How can I create new commands of my own? {#rse-bash-script}

Loops and history let us do tasks repeatedly,
but we can go even further and save commands in files
so that we can re-run complex sequences of operations with a few keystrokes.
For historical reasons,
a file full of shell commands is usually called a [shell script][shell-script],
but it is really just another kind of program.

Let's start by going into `climate-data` and creating a new file called `years.sh`
to hold our shell script:

```shell
$ cd ~/climate-data
$ nano years.sh
```

Insert this line:

```text
cut -d , -f 2 cleaned/bellambi_temp.csv
```

This uses the `cut` command to split the CSV file on commas
and select the second field from each line.
(The option `-d` stands for [delimiter][delimiter]:
we can provide any character we want to split lines on colons, spaces, and so on.)
Note that we do *not* put a dollar sign `$` at the front of the line:
we have been showing that for interactive commands,
but in this case we are putting the command in a file rather than running it immediately.

Once we have added this line to the file,
we can write it out with <kbd>Ctrl</kbd>+<kbd>O</kbd>
and exit with <kbd>Ctrl</kbd>+<kbd>O</kbd>.
`ls` shows that our file now exists:

```shell
$ ls
```

```text
NOTES           backup/         cleaned/        raw/            thesis.pdf      years.sh
README.md       bin/            docs/           thesis.md       tofu.config
```

and we can check its contents using `cat years.sh`.
More importantly,
we can now ask the shell to run this file:

```shell
$ bash years.sh
```

```text
Year
1997
1997
...many more lines...
2019
2019
2019
```

Sure enough,
our script's output is exactly what we would get if we ran the command directly.
For example,
we can count how many lines of output there are by putting our script in a pipeline:

```shell
$ bash years.sh | wc -l
```

```text
    8314
```

What if we want to remove duplicates from the output?
The command `uniq` will do what we want,
so let's edit our script,
add it,
look at our changes,
and then run the modified script:

```shell
$ nano years.sh
$ cat years.sh
```

```text
cut -d , -f 2 cleaned/bellambi_temp.csv | uniq
```

```shell
$ bash years.sh
```

```text
Year
1997
1998
1999
...one line per year...
2017
2018
2019
```

Once again,
we can pipe the output of our script into other commands
just as we would pipe the output from any other program:

```shell
$ bash years.sh | wc -l
```

```text
      24
```

## How can I make my scripts more versatile? {#rse-bash-params}

Creating a list of distinct years in a single specific data file isn't all that useful.
What we really want is a way to get the years from any of our files.
Let's edit `years.sh` again and replace `cleaned/bellambi_temp.csv`
with a special variable `$1`.
Once our change is made,
`years.sh` should contain:

```text
cut -d , -f 2 $1 | uniq
```

Inside a shell script,
`$1` means "the first argument on the command line".
We can now run our script like this:

```shell
$ bash years.sh cleaned/bellambi_temp.csv
```

and get exactly the same output as before,
or give it a different filename:

```shell
$ bash years.sh cleaned/andamooka_prec.csv 
```

and get the years from that file instead.

Our little script is now doing something useful,
but it may take the next person who reads it a moment to figure out exactly what that is.
We can improve our script by adding [comments][comment] at the top:

```text
# Select distinct years from column 2 of climate data file.
# Usage: bash years.sh /path/to/file.csv
cut -d , -f 2 $1 | uniq
```

As in R and Python,
a comment starts with a `#` character and runs to the end of the line.
The computer ignores comments,
but they help people (including your future self) understand and use scripts.

Let's make one more change to our script.
Instead of always selecting the second column,
let's have it select whatever column the user specified:

```text
# Select distinct years from column 2 of climate data file.
# Usage: bash years.sh /path/to/file.csv
cut -d , -f $2 $1 | uniq
```

The change is very small:
we have replaced the fixed column number `2` with a reference to the special variable `$2`,
which is assigned the value of the second command-line argument we give the script when we run it.
Let's check that it works by asking for column 1,
which is the weather station ID:

```shell
$ bash years.sh cleaned/bellambi_prec.csv 1
```

```text
Station
68228
```

But we have made a common mistake:
we have changed the script without changing the comment.
A description that sends readers in the wrong direction is worse than none at all,
so we should go back and update it.
We should probably also change the script's name from `years.sh` to `column.sh`,
since a program's name is the first piece of documentation anyone sees.

And finally,
we should add one more command to our pipeline.
If we run the script as-is for column 3,
which holds months,
we get this:

```shell
$ bash years.sh cleaned/bellambi_prec.csv 3
```
Month
1
2
...
11
12
1
2
...
8
9
10
```

Duplicate months aren't removed because `uniq` only removes *adjacent* duplicates.
If we want to get rid of them all,
we must sort the data so that redundant lines are next to one another.
Here's our final script:

```text
# Select distinct values from a column of a climate data file.
# Usage: bash years.sh /path/to/file.csv column_number
cut -d , -f $2 $1 | uniq
```

## How can I turn interactive work into a script? {#rse-bash-capture}

Suppose we have just run a series of commands that did something useful,
such as creating a plot for a paper.
Instead of typing those commands into a file in an editor
(and potentially getting them wrong)
we can run this:

```shell
$ history 6 > make-figure-3.sh
```

to put the most recent five commands in `make-figure-3.sh`.

```text
297 bash stats.sh cleaned/*_temp.csv > temperature_stats.csv
298 bash trim-outliers.sh temperature_stats.csv > plot_data.txt
299 date
300 ygraph --format scatter --color bw --borders none plot_data.txt figure-3.png
301 rm temperature_stats.csv plot_data.txt
302 history 6 > make-figure-3.sh
```

It only takes a few moments in an editor to remove the serial numbers
and delete the use of `date`
(which prints the current time and date)
to create a script that accurately captures what we actually did.
This is how we usually develop shell scripts:
run commands interactively a few times to make sure they are doing the right thing,
then save our recent history to a file and turn that into a reusable script.

## How can I find things in a file? {#rse-bash-grep}

We can use `head` and `tail` to select lines from a file by position,
but we also often want to select lines that contain certain values.
This operation is called [filtering][filter] When we are working with database tables or dataframes,
and in the shell,
we usually do it using a command called `grep`.
The name comes from "global regular expression print",
which was a common sequence of operations in early Unix text editors.
To show how `grep` works,
we will use a file that contains three haikus
taken from a 1998 competition in *Salon* magazine.

```shell
$ cat haiku.txt
```

```text
The Tao that is seen
Is not the true Tao, until
You bring fresh toner.

With searching comes loss
and the presence of absence:
"My Thesis" not found.

Yesterday it worked
Today it is not working
Software is like that.
```

> **Forever, or Five Years**
>
> We haven't linked to the original haikus because they don't appear to be on *Salon*'s site any longer.
> As [Jeff Rothenberg said][rothenberg-quote],
> "Digital information lasts forever—or five years, whichever comes first."
> Luckily, popular content often [has backups][rothenberg-backup].

Let's find lines that contain the word "not":

```shell
$ grep not haiku.txt
```

```text
Is not the true Tao, until
"My Thesis" not found
Today it is not working
```

Here, `not` is our (very simple) pattern.
`grep` searches the file line by line
and shows those lines that contain matches.
Let's search for the pattern `The`:

```shell
$ grep The haiku.txt
```

```text
The Tao that is seen
"My Thesis" not found.
```

Two lines match,
but in one of them,
our pattern is part of a larger word `Thesis`.
To restrict matching to lines containing `The` on its own,
we can give `grep` with the `-w` option:

```shell
$ grep -w The haiku.txt
```

```text
The Tao that is seen
```

What if we want to search for a phrase rather than a single word?

```shell
$ grep is not haiku.txt
```

```text
grep: not: No such file or directory
haiku.txt:The Tao that is seen
haiku.txt:"My Thesis" not found.
haiku.txt:Today it is not working
haiku.txt:Software is like that.
```

In this case,
`grep` uses `is` as the pattern
and tries to find it in the files `not` and `haiku.txt`.
It then tells us that the file `not` cannot be found,
but prints `haiku.txt` as a prefix to each other line of output
to tell us which file those lines came from.

If we want to give `grep` both words as a single argument,
we must wrap them in quotation marks:

```shell
$ grep "is not" haiku.txt
```

```text
Today it is not working
```

> **Quoting**
>
> Quotation marks aren't specific to `grep`:
> the shell interprets them before running the command,
> just as it expands wildcards to create actual filenames
> no matter what we're asking it to do.
> This allows us to do things like `head -n 5 "My Thesis.txt"`
> if we want to edit a file that has a space in its name.
> It is also why many programmers write `"$variable"` instead of just `$variable`
> when creating loops or shell scripts:
> if there's any chance at all that the variable's value will contain spaces,
> it's safest to quote it.

`grep` has many options—so many,
in fact,
that almost every letter of the alphabet means something to it:

```shell
$ man grep
```

```text
GREP(1)                   BSD General Commands Manual                  GREP(1)

NAME
     grep, egrep, fgrep, zgrep, zegrep, zfgrep -- file pattern searcher

SYNOPSIS
     grep [-abcdDEFGHhIiJLlmnOopqRSsUVvwxZ] [-A num] [-B num] [-C[num]]
          [-e pattern] [-f file] [--binary-files=value] [--color[=when]]
          [--colour[=when]] [--context[=num]] [--label] [--line-buffered]
          [--null] [pattern] [file ...]
...more...
```

One of the most useful options is `-n`,
which numbers the lines that match:

```shell
$ grep -n it haiku.txt
```

```text
5:With searching comes loss
9:Yesterday it worked
10:Today it is not working
```

Another is `-i`,
which does case-insensitive matching:

We can combine options (i.e. flags) as we do with other Unix commands.
For example, let's find the lines that contain the word "the". We can combine
the option `-w` to find the lines that contain the word "the" and `-n` to number the lines that match:

```shell
$ grep -i to haiku.txt
```

```text
You bring fresh toner.
Today it is not working
```

We can combine options as with other commands:

```shell
$ grep -i -n haiku.txt
```

```text
3:You bring fresh toner.
10:Today it is not working
```

We can also invert the match—i.e., print lines that *don't* match the pattern—using `-v`:

```shell
$ grep -i -n -v to haiku.txt
```

```text
1:The Tao that is seen
2:Is not the true Tao, until
4:
5:With searching comes loss
6:and the presence of absence:
7:"My Thesis" not found.
8:
9:Yesterday it worked
11:Software is like that.
```

If we want to search several files at once,
all we have to do is give `grep` all of their names.
We will frequently use wildcards to do this,
so if we want to count how many records in our climate data come from the year 2001,
we can do this:

```shell
$ grep 2001 cleaned/*.csv | wc -l
```

```text
    2920
```

Finally,
the `-r` option (for "recursive") tells `grep` to search all of the files
in or below a directory:

```shell
$ grep -r . FIXME | wc -l
```

```text
      28
```

`grep`'s real power comes from the fact that its patterns can include
a powerful kind of wildcards called [regular expressions][regular-expression].
For example,
this command finds lines that start with the letter 'T':

```shell
$ grep -E "^T" haiku.txt
```

```text
The Tao that is seen
Today it is not working
```

The `-E` option tells `grep` to interpret the pattern as a regular expression
rather than taking it literally.
The quotes prevent the shell from treating any special characters in the pattern as wildcards,
and the `^` in front of the `T` means, "Only match at the start of the line."

Many tools support regular expressions:
we can use them in programming languages,
database queries,
online search engines,
and most text editors (though not Nano—its creators wanted to keep it as small as possible).
Chapter \@ref(FIXME) has pointers to a few tutorials if you would like to explore them further.

## How can I find files? {#rse-bash-find}

While `grep` finds things in files,
the `find` command finds files themselves.
It also has a lot of options,
but unlike most Unix commands these are written as full words rather than abbreviations.
To show how it works,
we will explore the `docs` directory within `climate-data`:

```shell
$ cd docs
$ tree .
```

```text
.
├── bibliography.bib
├── chapter-1
│   └── index.html
├── chapter-2
│   └── index.html
├── chapter-3
│   └── index.html
├── figures
│   ├── figure-1.png
│   ├── figure-2.png
│   ├── figure-3.png
│   └── figure-4.png
└── index.html

4 directories, 9 files
```

This directory contains `index.html`, `bibliography.bib`, and four sub-directories:
three for chapters and one for figures.
For our first command,
let's run `find .` to find and list everything in this directory.
(As always,
`.` on its own means the current working directory,
which is where we want our search to start.)

```shell
$ find .
```

```text
.
./index.html
./chapter-1
./chapter-1/index.html
./figures
./figures/figure-4.png
./figures/figure-1.png
./figures/figure-3.png
./figures/figure-2.png
./chapter-3
./chapter-3/index.html
./chapter-2
./chapter-2/index.html
./bibliography.bib
```

If we only want to find directories,
we can tell `find` to show us things of type `d`:

```shell
$ find . -type d
```

```text
.
./chapter-1
./figures
./chapter-3
./chapter-2
```

If we change `-type d` to `-type f`
we get a listing of all the files instead:

```shell
$ find . -type f
```

```text
./index.html
./chapter-1/index.html
./figures/figure-4.png
./figures/figure-1.png
./figures/figure-3.png
./figures/figure-2.png
./chapter-3/index.html
./chapter-2/index.html
./bibliography.bib
```

Now let's try matching by name:

```shell
$ find . -name "*.png"
```

```text
./figures/figure-4.png
./figures/figure-1.png
./figures/figure-3.png
./figures/figure-2.png
```

As we said earlier,
the command line's power lies in combining tools.
We have seen how to do that with pipes;
let's use another technique to see how large our HTML files are:

```shell
$ wc -l $(find . -name "*.html")
```

```text
      16 ./index.html
      14 ./chapter-1/index.html
      15 ./chapter-3/index.html
      11 ./chapter-2/index.html
      56 total
```

When the shell executes this command,
it runs whatever is inside the `$()`
and then replaces `$()` with that command's output.
Since the output of `find` is the paths of four HTML files,
the shell constructs the command:

```shell
$ wc -l ./index.html ./chapter-1/index.html ./chapter-2/index.html ./chapter-3/index.html
```

which is what we wanted.
It is exactly like expanding the wildcard in `*.html`,
but more flexible.

We will often use `find` and `grep` together.
The first finds files whose names match a pattern,
while the second looks for lines inside those files that match another pattern.
For example,
we can look for the titles of all our HTML files:

```shell
$ grep "<title>" $(find . -name "*.html")
```

```text
./index.html:    <title>Climate Change in the Australian Subcontinent</title>
./chapter-1/index.html:    <title>Climate Change in the Australian Subcontinent: Introduction</title>
./chapter-3/index.html:    <title>Climate Change in the Australian Subcontinent: Methods</title>
./chapter-2/index.html:    <title>Climate Change in the Australian Subcontinent: Background</title>
```

We can also use `$()` expansion to create a list of filenames to use in a loop:

```shell
$ for page in $(find . -name "*.html")
> do
> cp $page $page.bak
> done
$ find . -name "*.bak"
```

```text
./chapter-1/index.html.bak
./index.html.bak
./chapter-3/index.html.bak
./chapter-2/index.html.bak
```

## Summary {#rse-bash-summary}

FIXME: concept map

The original Unix shell was created in 1971,
and will soon celebrate its fiftieth anniversary.
Its syntax may be cryptic and inconsistent,
but few programs have lasted as long,
and fewer still remain in daily use.

The secret to its success was and is its generality:
any program that reads text from standard input
and prints text to standard output
can work with any other.
Chapter \@ref(FIXME) will look at how to write tools of our own that do this.

## Exercises {#rse-bash-exercises}

### Exploring more `ls` flags {#rse-bash-ex-more-ls}

You can also use two options at the same time. What does the command `ls` do when used
with the `-l` option? What about if you use both the `-l` and the `-h` option?

Some of its output is about properties that we do not cover in this lesson (such
as file permissions and ownership), but the rest should be useful
nevertheless.

### Listing recursively and by time {#rse-bash-ex-ls-rt}

The command `ls -R` lists the contents of directories recursively, i.e., lists
their sub-directories, sub-sub-directories, and so on at each level. The command
`ls -t` lists things by time of last change, with most recently changed files or
directories first.

In what order does `ls -R -t` display things? Hint: `ls -l` uses a long listing
format to view timestamps.

### Absolute and relative paths {#rse-bash-ex-paths}

Starting from `/Users/amanda/data`,
which of the following commands could Amanda use to navigate to her home directory,
which is `/Users/amanda`?

1. `cd .`
2. `cd /`
3. `cd /home/amanda`
4. `cd ../..`
5. `cd ~`
6. `cd home`
7. `cd ~/data/..`
8. `cd`
9. `cd ..`

### Relative path resolution {#rse-bash-ex-resolve-rel-path}

Using the filesystem diagram below, if `pwd` displays `/Users/thing`,
what will `ls -F ../backup` display?

1.  `../backup: No such file or directory`
2.  `2012-12-01 2013-01-08 2013-01-27`
3.  `2012-12-01/ 2013-01-08/ 2013-01-27/`
4.  `original/ pnas_final/ pnas_sub/`

FIXME: figure (filesystem for exercises)

### `ls` reading comprehension {#rse-bash-ex-reading-ls}

Using the filesystem diagram below,
if `pwd` displays `/Users/backup`,
and `-r` tells `ls` to display things in reverse order,
what command(s) will result in the following output:

```shell
pnas_sub/ pnas_final/ original/
```

FIXME: figure (filesystem for exercises)

1.  `ls pwd`
2.  `ls -r -F`
3.  `ls -r -F /Users/backup`

### Creating files a different way {#rse-bash-ex-touch}

We have seen how to create text files using the `nano` editor.
Now, try the following command:

```shell
$ touch my_file.txt
```

1.  What did the `touch` command do?
    When you look at your current directory using the GUI file explorer,
    does the file show up?

2.  Use `ls -l` to inspect the files.  How large is `my_file.txt`?

3.  When might you want to create a file this way?

### Moving to the current folder {#rse-bash-ex-move-dot}

After running the following commands,
Jamie realizes that she put the files `sucrose.dat` and `maltose.dat` into the wrong folder:

```shell
$ ls -F
  analyzed/ raw/
$ ls -F analyzed
  fructose.dat glucose.dat maltose.dat sucrose.dat
$ cd raw/
```

Fill in the blanks to move these files to the current folder
(i.e., the one she is currently in):

```shell
$ mv ___/sucrose.dat  ___/maltose.dat ___
```

### Renaming files {#rse-bash-ex-renaming-files}

Suppose that you created a plain-text file in your current directory to contain a list of the
statistical tests you will need to do to analyze your data, and named it: `statstics.txt`

After creating and saving this file you realize you misspelled the filename! You want to
correct the mistake, which of the following commands could you use to do so?

1. `cp statstics.txt statistics.txt`
2. `mv statstics.txt statistics.txt`
3. `mv statstics.txt .`
4. `cp statstics.txt .`

### Moving and copying {#rse-bash-ex-last-ls}

What is the output of the closing `ls` command in the sequence shown below?

```shell
$ pwd
```
 
```text
/Users/jamie/data
```
 
```shell
$ ls
```
 
```text
proteins.dat
```
 
```shell
$ mkdir recombine
$ mv proteins.dat recombine/
$ cp recombine/proteins.dat ../proteins-saved.dat
$ ls
```

1.   `proteins-saved.dat recombine`
2.   `recombine`
3.   `proteins.dat recombine`
4.   `proteins-saved.dat`

### Using `rm` safely {#rse-bash-ex-safe-rm}

What happens when we execute `rm -i thesis_backup/quotations.txt`?
Why would we want this protection when using `rm`?

### Copy with multiple filenames {#rse-bash-ex-copy-multi}

For this exercise, you can test the commands in the `data-shell/data` directory.

In the example below, what does `cp` do when given several filenames and a directory name?

```shell
$ mkdir backup
$ cp amino-acids.txt animals.txt backup/
```

In the example below, what does `cp` do when given three or more file names?

```shell
$ ls -F
```

```text
amino-acids.txt  animals.txt  backup/  elements/  morse.txt  pdb/  planets.txt  salmon.txt  sunspot.txt
```

```shell
$ cp amino-acids.txt animals.txt morse.txt 
```

### List filenames matching a pattern {#rse-bash-ex-ls-match}

When run in the `molecules` directory, which `ls` command(s) will
produce this output?

`ethane.pdb   methane.pdb`

1. `ls *t*ane.pdb`
2. `ls *t?ne.*`
3. `ls *t??ne.pdb`
4. `ls ethane.*`

### More on wildcards {#rse-bash-ex-more-wildcards}

Sam has a directory containing calibration data, datasets, and descriptions of
the datasets:

```text
.
├── 2015-10-23-calibration.txt
├── 2015-10-23-dataset1.txt
├── 2015-10-23-dataset2.txt
├── 2015-10-23-dataset_overview.txt
├── 2015-10-26-calibration.txt
├── 2015-10-26-dataset1.txt
├── 2015-10-26-dataset2.txt
├── 2015-10-26-dataset_overview.txt
├── 2015-11-23-calibration.txt
├── 2015-11-23-dataset1.txt
├── 2015-11-23-dataset2.txt
├── 2015-11-23-dataset_overview.txt
├── backup
│   ├── calibration
│   └── datasets
└── send_to_bob
    ├── all_datasets_created_on_a_23rd
    └── all_november_files
```

Before heading off to another field trip, she wants to back up her data and
send some datasets to her colleague Bob. Sam uses the following commands
to get the job done:

```shell
$ cp *dataset* backup/datasets
$ cp ____calibration____ backup/calibration
$ cp 2015-____-____ send_to_bob/all_november_files/
$ cp ____ send_to_bob/all_datasets_created_on_a_23rd/
```

Help Sam by filling in the blanks.

The resulting directory structure should look like this
```text
.
├── 2015-10-23-calibration.txt
├── 2015-10-23-dataset1.txt
├── 2015-10-23-dataset2.txt
├── 2015-10-23-dataset_overview.txt
├── 2015-10-26-calibration.txt
├── 2015-10-26-dataset1.txt
├── 2015-10-26-dataset2.txt
├── 2015-10-26-dataset_overview.txt
├── 2015-11-23-calibration.txt
├── 2015-11-23-dataset1.txt
├── 2015-11-23-dataset2.txt
├── 2015-11-23-dataset_overview.txt
├── backup
│   ├── calibration
│   │   ├── 2015-10-23-calibration.txt
│   │   ├── 2015-10-26-calibration.txt
│   │   └── 2015-11-23-calibration.txt
│   └── datasets
│       ├── 2015-10-23-dataset1.txt
│       ├── 2015-10-23-dataset2.txt
│       ├── 2015-10-23-dataset_overview.txt
│       ├── 2015-10-26-dataset1.txt
│       ├── 2015-10-26-dataset2.txt
│       ├── 2015-10-26-dataset_overview.txt
│       ├── 2015-11-23-dataset1.txt
│       ├── 2015-11-23-dataset2.txt
│       └── 2015-11-23-dataset_overview.txt
└── send_to_bob
    ├── all_datasets_created_on_a_23rd
    │   ├── 2015-10-23-dataset1.txt
    │   ├── 2015-10-23-dataset2.txt
    │   ├── 2015-10-23-dataset_overview.txt
    │   ├── 2015-11-23-dataset1.txt
    │   ├── 2015-11-23-dataset2.txt
    │   └── 2015-11-23-dataset_overview.txt
    └── all_november_files
        ├── 2015-11-23-calibration.txt
        ├── 2015-11-23-dataset1.txt
        ├── 2015-11-23-dataset2.txt
        └── 2015-11-23-dataset_overview.txt
```

### Organizing directories and files {#rse-bash-ex-organizing}

Jamie is working on a project and she sees that her files aren't very well
organized:

```shell
$ ls -F
```

```text
analyzed/  fructose.dat    raw/   sucrose.dat
```

The `fructose.dat` and `sucrose.dat` files contain output from her data
analysis. What command(s) covered in this lesson does she need to run so that the commands below will
produce the output shown?

```shell
$ ls -F
```

```text
analyzed/   raw/
```
 
```shell
$ ls analyzed
```

```text
fructose.dat    sucrose.dat
```

### Reproduce a directory structure {#rse-bash-ex-reproduce-structure}

You're starting a new experiment, and would like to duplicate the directory
structure from your previous experiment so you can add new data.

Assume that the previous experiment is in a folder called '2016-05-18',
which contains a `data` folder that in turn contains folders named `raw` and
`processed` that contain data files.  The goal is to copy the folder structure
of the `2016-05-18-data` folder into a folder called `2016-05-20`
so that your final directory structure looks like this:

	2016-05-20/
	└── data
	    ├── processed
	    └── raw
 
Which of the following set of commands would achieve this objective?

What would the other commands do?

```shell
$ mkdir 2016-05-20
$ mkdir 2016-05-20/data
$ mkdir 2016-05-20/data/processed
$ mkdir 2016-05-20/data/raw
```

```shell
$ mkdir 2016-05-20
$ cd 2016-05-20
$ mkdir data
$ cd data
$ mkdir raw processed
```

```shell
$ mkdir 2016-05-20/data/raw
$ mkdir 2016-05-20/data/processed
```
 
```shell
$ mkdir 2016-05-20
$ cd 2016-05-20
$ mkdir data
$ mkdir raw processed
```
 
### What does `sort -n` do? {#rse-bash-ex-sort-n}

If we run `sort` on a file containing the following lines:

```text
10
2
19
22
6
```

the output is:

```text
10
19
2
22
6
```

If we run `sort -n` on the same input, we get this instead:

```text
2
6
10
19
22
```

Explain why `-n` has this effect.

### What does `>>` mean? {#rse-bash-ex-redirect-append}

We have seen the use of `>`, but there is a similar operator `>>` which works slightly differently.
We'll learn about the differences between these two operators by printing some strings.
We can use the `echo` command to print strings e.g.

```shell
$ echo The echo command prints text
```

```text
The echo command prints text
```

Now test the commands below to reveal the difference between the two operators:

```shell
$ echo hello > testfile01.txt
```

and:

```shell
$ echo hello >> testfile02.txt
```

Hint: Try executing each command twice in a row and then examining the output files.

### Appending data {#rse-bash-ex-append-data}

We have already met the `head` command, which prints lines from the start of a file.
`tail` is similar, but prints lines from the end of a file instead.

Consider the file `data-shell/data/animals.txt`.
After these commands, select the answer that
corresponds to the file `animals-subset.txt`:

```shell
$ head -n 3 animals.txt > animals-subset.txt
$ tail -n 2 animals.txt >> animals-subset.txt
```

1. The first three lines of `animals.txt`
2. The last two lines of `animals.txt`
3. The first three lines and the last two lines of `animals.txt`
4. The second and third lines of `animals.txt`

### Piping commands {#rse-bash-ex-piping}

In our current directory, we want to find the 3 files which have the least number of
lines. Which command listed below would work?

1. `wc -l * > sort -n > head -n 3`
2. `wc -l * | sort -n | head -n 1-3`
3. `wc -l * | head -n 3 | sort -n`
4. `wc -l * | sort -n | head -n 3`

### Why does `uniq` only remove adjacent duplicates? {#rse-bash-ex-uniq-adjacent}

The command `uniq` removes adjacent duplicated lines from its input.
For example, the file `data-shell/data/salmon.txt` contains:

```text
coho
coho
steelhead
coho
steelhead
steelhead
```

Running the command `uniq salmon.txt` from the `data-shell/data` directory produces:

```text
coho
steelhead
coho
steelhead
```

Why do you think `uniq` only removes *adjacent* duplicated lines?
(Hint: think about very large data sets.) What other command could
you combine with it in a pipe to remove all duplicated lines?

### Pipe reading comprehension {#rse-bash-ex-reading-pipes}

A file called `animals.txt` (in the `data-shell/data` folder) contains the following data:

```text
2012-11-05,deer
2012-11-05,rabbit
2012-11-05,raccoon
2012-11-06,rabbit
2012-11-06,deer
2012-11-06,fox
2012-11-07,rabbit
2012-11-07,bear
```

What text passes through each of the pipes and the final redirect in the pipeline below?

```shell
$ cat animals.txt | head -n 5 | tail -n 3 | sort -r > final.txt
```
 
Hint: build the pipeline up one command at a time to test your understanding

### Pipe construction {#rse-bash-ex-pipe-construction}

For the file `animals.txt` from the previous exercise, consider the following command:

```shell
$ cut -d , -f 2 animals.txt
```

The `cut` command is used to remove or "cut out" certain sections of each line in the file.
The `-d` option is used to define the delimiter.
A **delimiter** is a character that is used to separate each line of text into columns.
The default delimiter is <kbd>Tab</kbd>,
meaning that the `cut` command will automatically assume that
values in different columns will be separated by a tab.
The `-f` option is used to specify the field (column) to cut out.
The command above uses the `-d` option to split each line by comma,
and the `-f` option to print the second field in each line, to give the following output:

```text
deer
rabbit
raccoon
rabbit
deer
fox
rabbit
bear
```

What other command(s) could be added to this in a pipeline to find
out what animals the file contains (without any duplicates in their
names)?

### Which pipe? {#rse-bash-ex-which-pipe}

The file `animals.txt` contains 8 lines of data formatted as follows:

```text
2012-11-05,deer
2012-11-05,rabbit
2012-11-05,raccoon
2012-11-06,rabbit
...
```

The `uniq` command has a `-c` option which gives a count of the
number of times a line occurs in its input.  Assuming your current
directory is `data-shell/data/`, what command would you use to produce
a table that shows the total count of each type of animal in the file?

1.  `sort animals.txt | uniq -c`
2.  `sort -t, -k2,2 animals.txt | uniq -c`
3.  `cut -d, -f 2 animals.txt | uniq -c`
4.  `cut -d, -f 2 animals.txt | sort | uniq -c`
5.  `cut -d, -f 2 animals.txt | sort | uniq -c | wc -l`

### Wildcard expressions {#rse-bash-ex-wildcard-expressions}

Wildcard expressions can be very complex, but you can sometimes write
them in ways that only use simple syntax, at the expense of being a bit
more verbose.
Consider the directory `data-shell/north-pacific-gyre/2012-07-03` :
the wildcard expression `*[AB].txt`
matches all files ending in `A.txt` or `B.txt`. Imagine you forgot about
this.

1.  Can you match the same set of files with basic wildcard expressions
    that do not use the `[]` syntax? *Hint*: You may need more than one
    expression.

2.  The expression that you found and the expression from the lesson match the
    same set of files in this example. What is the small difference between the
    outputs?

3.  Under what circumstances would your new expression produce an error message
    where the original one would not?
    
### Removing unneeded files {#rse-bash-ex-remove-unneeded}

Suppose you want to delete your processed data files, and only keep
your raw files and processing script to save storage.
The raw files end in `.dat` and the processed files end in `.txt`.
Which of the following would remove all the processed data files,
and *only* the processed data files?

1. `rm ?.txt`
2. `rm *.txt`
3. `rm * .txt`
4. `rm *.*`

### Doing a dry run {#rse-bash-ex-loop-dry-run}

A loop is a way to do many things at once --- or to make many mistakes at
once if it does the wrong thing. One way to check what a loop *would* do
is to `echo` the commands it would run instead of actually running them.

Suppose we want to preview the commands the following loop will execute
without actually running those commands:

```shell
$ for file in *.pdb
> do
>   analyze $file > analyzed-$file
> done
```

What is the difference between the two loops below, and which one would we
want to run?

```shell
# Version 1
$ for file in *.pdb
> do
>   echo analyze $file > analyzed-$file
> done
```

```shell
# Version 2
$ for file in *.pdb
> do
>   echo "analyze $file > analyzed-$file"
> done
```

### Nested loops {#rse-bash-ex-nested-loops}

Suppose we want to set up up a directory structure to organize
some experiments measuring reaction rate constants with different compounds
*and* different temperatures.  What would be the
result of the following code:

```shell
$ for species in cubane ethane methane
> do
>     for temperature in 25 30 37 40
>     do
>         mkdir $species-$temperature
>     done
> done
```

### Variables in loops {#rse-bash-ex-loop-variables}

This exercise refers to the `data-shell/molecules` directory.
`ls` gives the following output:

```text
cubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb
```

What is the output of the following code?

```shell
$ for datafile in *.pdb
> do
>    ls *.pdb
> done
```

Now, what is the output of the following code?

```shell
$ for datafile in *.pdb
> do
>	ls $datafile
> done
```

Why do these two loops give different outputs?

### Limiting sets of files {#rse-bash-ex-limiting-file-sets}

What would be the output of running the following loop in the `data-shell/molecules` directory?

```shell
$ for filename in c*
> do
>    ls $filename
> done
```

1.  No files are listed.
2.  All files are listed.
3.  Only `cubane.pdb`, `octane.pdb` and `pentane.pdb` are listed.
4.  Only `cubane.pdb` is listed.

How would the output differ from using this command instead?

```shell
$ for filename in *c*
> do
>    ls $filename
> done
```

1.  The same files would be listed.
2.  All the files are listed this time.
3.  No files are listed this time.
4.  The files `cubane.pdb` and `octane.pdb` will be listed.
5.  Only the file `octane.pdb` will be listed.

### Saving to a file in a loop {#rse-bash-ex-loop-save}

In the `data-shell/molecules` directory, what is the effect of this loop?

```shell
for alkanes in *.pdb
> do
>     echo $alkanes
>     cat $alkanes > alkanes.pdb
> done
```

1.  Prints `cubane.pdb`, `ethane.pdb`, `methane.pdb`, `octane.pdb`, `pentane.pdb` and `propane.pdb`,
    and the text from `propane.pdb` will be saved to a file called `alkanes.pdb`.
2.  Prints `cubane.pdb`, `ethane.pdb`, and `methane.pdb`, and the text from all three files would be
    concatenated and saved to a file called `alkanes.pdb`.
3.  Prints `cubane.pdb`, `ethane.pdb`, `methane.pdb`, `octane.pdb`, and `pentane.pdb`, and the text
    from `propane.pdb` will be saved to a file called `alkanes.pdb`.
4.  None of the above.

Also in the `data-shell/molecules` directory, what would be the output of the following loop?

```shell
for datafile in *.pdb
> do
>     cat $datafile >> all.pdb
> done
```

1.  All of the text from `cubane.pdb`, `ethane.pdb`, `methane.pdb`, `octane.pdb`, and
    `pentane.pdb` would be concatenated and saved to a file called `all.pdb`.
2.  The text from `ethane.pdb` will be saved to a file called `all.pdb`.
3.  All of the text from `cubane.pdb`, `ethane.pdb`, `methane.pdb`, `octane.pdb`, `pentane.pdb`
    and `propane.pdb` would be concatenated and saved to a file called `all.pdb`.
4.  All of the text from `cubane.pdb`, `ethane.pdb`, `methane.pdb`, `octane.pdb`, `pentane.pdb`
    and `propane.pdb` would be printed to the screen and saved to a file called `all.pdb`.

### List unique species {#rse-bash-ex-list-unique}

Leah has several hundred data files, each of which is formatted like this:

```text
2013-11-05,deer,5
2013-11-05,rabbit,22
2013-11-05,raccoon,7
2013-11-06,rabbit,19
2013-11-06,deer,2
2013-11-06,fox,1
2013-11-07,rabbit,18
2013-11-07,bear,1
```

An example of this type of file is given in `data-shell/data/animal-counts/animals.txt`.
 
We can use the command `cut -d , -f 2 animals.txt | sort | uniq`
to produce the unique species in `animals.txt`.
In order to avoid having to type out this series of commands every time,
a scientist may choose to write a shell script instead.

Write a shell script called `species.sh` that takes any number of
filenames as command-line arguments,
and uses a variation of the above command to print a list
of the unique species appearing in each of those files separately.

### Why does `history` record commands before running them? {#rse-bash-ex-history-order}

If you run the command:

```shell
$ history | tail -n 5 > recent.sh
```

the last command in the file is the `history` command itself, i.e.,
the shell has added `history` to the command log before actually
running it. In fact, the shell *always* adds commands to the log
before running them. Why do you think it does this?

### Variables in shell scripts {#rse-bash-ex-script-variables}

In the `molecules` directory, imagine you have a shell script called `script.sh` containing the
following commands:

```shell
head -n $2 $1
tail -n $3 $1
```

While you are in the `molecules` directory, you type the following command:

```shell
bash script.sh '*.pdb' 1 1
```

Which of the following outputs would you expect to see?

1. All of the lines between the first and the last lines of each file ending in `.pdb`
    in the `molecules` directory
2. The first and the last line of each file ending in `.pdb` in the `molecules` directory
3. The first and the last line of each file in the `molecules` directory
4. An error because of the quotes around `*.pdb`

### Find the longest file with a given extension {#rse-bash-ex-longest-with-extension}

Write a shell script called `longest.sh` that takes the name of a
directory and a filename extension as its arguments, and prints
out the name of the file with the most lines in that directory
with that extension. For example:

```shell
$ bash longest.sh /tmp/data pdb
```

would print the name of the `.pdb` file in `/tmp/data` that has
the most lines.

### Script reading comprehension {#rse-bash-ex-reading-scripts}

For this question, consider the `data-shell/molecules` directory once again.
This contains a number of `.pdb` files in addition to any other files you
may have created.
Explain what each of the following three scripts would do when run as
`bash script1.sh *.pdb`, `bash script2.sh *.pdb`, and `bash script3.sh *.pdb` respectively.

```shell
# Script 1
echo *.*
```

```shell
# Script 2
for filename in $1 $2 $3
> do
>     cat $filename
> done
```

```shell
# Script 3
echo $@.pdb
```

### Using `grep` {#rse-bash-ex-using-grep}

Which command would result in the following output:

```text
and the presence of absence:
```

1. `grep "of" haiku.txt`
2. `grep -E "of" haiku.txt`
3. `grep -w "of" haiku.txt`
4. `grep -i "of" haiku.txt`

### Tracking a species {#rse-bash-ex-tracking-species}
 
Leah has several hundred 
data files saved in one directory, each of which is formatted like this:
 
```text
2013-11-05,deer,5
2013-11-05,rabbit,22
2013-11-05,raccoon,7
2013-11-06,rabbit,19
2013-11-06,deer,2
```

She wants to write a shell script that takes a species as the first command-line argument 
and a directory as the second argument. The script should return one file called `species.txt` 
containing a list of dates and the number of that species seen on each date.
For example using the data shown above, `rabbit.txt` would contain:
 
```text
2013-11-05,22
2013-11-06,19
```

Put these commands and pipes in the right order to achieve this:
 
```shell
cut -d : -f 2  
>  
|  
grep -w $1 -r $2  
|  
$1.txt  
cut -d , -f 1,3  
```

Hint: use `man grep` to look for how to grep text recursively in a directory
and `man cut` to select more than one field in a line.

An example of such a file is provided in `data-shell/data/animal-counts/animals.txt`

### Counting names {#rse-bash-ex-little-women}

You and your friend, having just finished reading *Little Women* by
Louisa May Alcott, are in an argument.  Of the four sisters in the
book, Jo, Meg, Beth, and Amy, your friend thinks that Jo was the
most mentioned.  You, however, are certain it was Amy.  Luckily, you
have a file `LittleWomen.txt` containing the full text of the novel
(`data-shell/writing/data/LittleWomen.txt`).
Using a `for` loop, how would you tabulate the number of times each
of the four sisters is mentioned?

Hint: one solution might employ
the commands `grep` and `wc` and a `|`, while another might utilize
`grep` options.
There is often more than one way to solve a programming task, so a
particular solution is usually chosen based on a combination of
yielding the correct result, elegance, readability, and speed.

### Matching and subtracting {#rse-bash-ex-match-subtract}

The `-v` option to `grep` inverts pattern matching, so that only lines
which do *not* match the pattern are printed. Given that, which of
the following commands will find all files in `/data` whose names
end in `s.txt` (e.g., `animals.txt` or `planets.txt`), but do
*not* contain the word `net`?
Once you have thought about your answer, you can test the commands in the `data-shell`
directory.

1.  `find data -name '*s.txt' | grep -v net`
2.  `find data -name *s.txt | grep -v net`
3.  `grep -v "temp" $(find data -name '*s.txt')`
4.  None of the above.

### `find` pipeline reading comprehension {#rse-bash-ex-reading-find}

Write a short explanatory comment for the following shell script:

```shell
wc -l $(find . -name '*.dat') | sort -n
```

### Finding files with different properties {#rse-bash-ex-find-properties}
 
The `find` command can be given several other criteria known as "tests"
to locate files with specific attributes, such as creation time, size,
permissions, or ownership.  Use `man find` to explore these, and then
write a single command to find all files in or below the current directory
that are owned by the user `ahmed` and were modified in the last 24 hours.

Hint 1: you will need to use three tests: `-type`, `-mtime`, and `-user`.

Hint 2: The value for `-mtime` will need to be negative---why?

### Combining options {#rse-bash-ex-combining-options}

FIXME: Note that in most command line tools, multiple options can be combined
with a single `-` and no spaces between the options: `ls -F -a` is equivalent to
`ls -Fa`.

### Other wildcards {#rse-bash-ex-other-wildcards}

FIXME: introduce ? and other wildcards.

## Key Points {#rse-bash-keypoints}

```{r, child="keypoints/bash.md"}
```

```{r, child="./links.md"}
```
