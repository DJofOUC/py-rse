# Correctness {#rse-correctness}

When you write software, 
you likely think your code is doing exactly what you wrote it to do.
But how do you *know* for sure, in a quantifiable and objective way?
How do you know if your code correctly outputs or computes what you think it does?
Well, like in science, to find out if something matches your expectation, 
you test that expectation against reality.
In this case, you write tests for your code to validate (or invalidate) your expectation.
Over the course of this chapter we will explain and show how to make use of
["unit tests"](glossary.html#rse-correctness-test) in both Python and R packages.

## Manually testing your code {#rse-correctness-manual-testing}

Without a formal framework for testing code, 
how do you normally see if your code works?
Like many of us before learning about unit testing, 
you might test code interactively by running it yourself on different problems.

For instance, let's say you wrote a function like this:

```{r manual-test-r}
# in R
numSign <- function(x) {
  if (x > 0) {
    1
  } else {
    -1
  }
}

numSign(1)
```

```{python manual-test-py}
# In Python
def numSign(x):
    if x > 0:
        out = 1
    else:
        out = -1
    return(out)

numSign(1)
```

A simple way to test the code is to run commands like:

```{r manual-test-r-2}
numSign(2) == 1
numSign(0) == -1
numSign(-4) == -1
```

You could go a step further and write a script with some simple tests:

```{r stop-not-r}
# In R
stopifnot(numSign(1) == 1)
stopifnot(numSign(-Inf) == -1)
```

`stopifnot()` runs the code and if it resolves to `true` than it continues.
What if you want to test something that you know should fail?

```{r show-stop-not-error, error=TRUE}
stopifnot(numSign(NULL) == -1)
```

Which gives an error as expected.

TODO: Need Python version of this.

TODO: Finish this off...

### Exercise

- Write manually in the console/terminal a test to check if the function has an error or gives you what you expect.
TODO: Need to complete this.

### Why this quickly becomes a problem

Writing simple tests on the fly and interactively (or informally), like above, 
works for a while but quickly becomes unsustainable. 
The above are not sufficient because:

1. If you run the simple tests in a script that causes an error,
the execution will halt at the first failed test.
Any subsequent test won't then run and we may not know if the error was expected or not.

2. The tests are not independent, given that if one test fails, 
all subsequent tests won't be run.
Good tests are independent of each other,
so that all tests run and give an output on their success or failure.

3. Each test prints the output to the screen (true/false... or no output at all),
but there is no overall summary and no easy way to see what test produced what result.
For only three tests like above, this isn't a problem.
But once more tests are included and given no meaningful output is provided,
this way of creating tests becomes unusable.

Instead, we should use a formal framework for writing and running tests.

## Framework for automatic testing {#rse-correctness-framework}

To optimize your time writing code and developing tests, 
a framework for testing software should:

- make it easy for people to write tests (because otherwise they won't do it);
- run a set of tests;
- report which ones have failed; and,
- give some idea of where or why they failed (to help debugging).

Any single test can have one of three results:

- [success](glossary.html#test-success), meaning that the test passed correctly;
- [failure](glossary.html#test-failure), meaning that the software being tested didn't do what it was supposed to; or
- [error](glossary.html#test-error), meaning that the test itself failed (in which case, we don't know anything about the software being tested).

A formal [test framework](glossary.html#test-framework) is a package that supports writing and running tests.
In Python, the tools to run the tests are called a [test runner](glossary.html#test-runner).
The package will find the tests, execute them, and report the results of the tests.
If any test fails, a summary of the failure will be given.

Tests in this framework are usually called "unit tests",
where individual components ("units") of your code are tested.
A unit test is a function that runs your code (with associated tests) and produces one of the three results.
Unit tests take an input (called a [fixture](glossary.html#fixture) in Python and simply "input" in R) 
that are then sent to the functions or code we want to test.
For each function/code we have a set of test "expectations",
meaning that we compare our expected output of the function/code with what the code actually outputs.

## Getting tests set up {#rse-correctness-getting-setup}

There are several test frameworks in both Python and R. 
In Python, the most recommended and most common package is [pytest][pytest].
In R, it is the testthat package.

We'll start with **Python**.
Tests in pytest are structured according to three simple rules:

1. All tests are put in files whose names begin with `test_`.
2. Each test is a function whose name begins with `test_`.
3. Test functions use `assert` to check that results are as expected.

It's good practice to put all test files into the `tests/` folder. 
Running tests is done by typing in the shell:

```shell
pytest
```

pytest searches for all files named `test_*.py` or `*_test.py` in the current directory and its sub-directories (e.g. `tests/`).
If we want to run single tests, we run the command `pytest tests/test_filename.py` to run the tests in file `tests/test_filename.py`.
We'll get into the specific anatomy of unit tests in the next section.

**For R**, setting things up are a bit more straight-forward and automated. 
To start, open the `.Rproj` in RStudio. In the console, type out:

```r
usethis::use_testthat()
```

which will create a new `tests/testthat/` directory, 
add testthat to `Suggests` in the `DESCRIPTION` file,
and creates a `tests/testthat.R` script that instructs R on how to run the tests.
Creating a new test file is done with:

```r
usethis::use_test("name-of-file")
```

which will create a file called `tests/testthat/test-name-of-file.R`. 
Generally, the name of the test file should reflect what you want to test,
usually the contents of a specific R file inside `R/`.
The basic template for writing the test is provided inside the new file.

Running tests in R is done by either typing out:

```r
devtools::test()
```

or by using the keybinding "Ctrl/Cmd-Shift-T" in RStudio.
For further learning on using unit tests in R packages, check out the
[`testthat`][r-testthat] website and the [Testing Chapter][r-pkg-book-testing] of
the [R Packages][r-pkg-book] book.

### Exercise

**In Python**:

- Create a file called `test_add_two.py` in `tests/` by running in the terminal:

    ```shell
    touch tests/test_add_two.py
    ```
    
    Open that file in the Python editor and write (or copy and paste) the below code into the file:
    
    TODO: Fix editor name here.
    
    ```python
    def add_two(x, y):
        val = x + y
        return(val)
    
    # This will succeed
    def test_add_two():
        assert add_two(1, 2) == 3
    
    # This will fail    
    def test_add_two_fail():
        assert add_two(1, 2) == 4
    ```
    
    TODO: Make sure this is correct.
    
    Now, run `pytest` in the terminal. What does the test results say?

**In R**:

- Making sure you are inside your R package in RStudio, 
type out `usethis::use_testthat()` to set up testing, 
if you haven't done so already.
Next run the command:

    ```r
    usethis::use_test("mean")
    ```
    
    The new test file should open up in RStudio. 
    Write out (or copy and paste) the below code into the new file:
    
    ```r
    context("Computing the mean")
    
    test_that("the mean is calculated correctly", {
        # This will succeed
        expect_identical(mean(1:10), 5.5)
        
        # This won't
        expect_identical(mean(1:10), 4.5)
    })
    ```
    
    Run the tests using "Ctrl-Shift-T". What does the test results say?
    
## Anatomy of a unit test {#rse-correctness-anatomy-test}

Unit tests all have a similar and basic structure:

- Loading the packages and setting up any necessary inputs/fixtures.
- Setting up the test function ("expectation") with the code and its expected out.

So template of how it looks like would be (for **Python**):

```python
from package import function

def test_name():
    assert function(fixture) == expectation

# and so on...
```

Load the function from the package, write the test name, set the function name
with an input/fixture, and test against the expectation.

In **R**, it would be:

```r
context("Short description of what is being tested")
library(package)

test_that("short description of what is expected", {
  expect_identical(function_name(input), expectation)
})
```

The `context()` is the short explanation, or documentation, 
on what the is reason for doing the test.
We'll cover writing these more in later sections.
When writing `test_that()` unit tests, 
the description part should be read as, e.g. "test that... the mean is correct".
Then all tests inside the `test_that()` function should be related to the description.
The testthat package has a ton of `expect_*` functions,
which can be easily browsed by typing `testthat::expect_` and then hitting TAB for autocompletion and listing.

### Exercise

Using the tests created from the previous exercise, 
fix the code so that all tests will succeed.
Then re-run the tests (`pytest` or `devtools::test()`) and confirm that the tests passed.

### Exercise

- **In Python**: Create a new test file called `test_simple_arithmetic.py`. 
Write out (or copy and paste) the code below into the newly created test file.
Then fill in the blanks.

    ```python
    
    ```

TODO: Finish this.


- **In R**: Create a new test by running `usethis::use_test("simple-arithmetic")`. 
Write out (or copy and paste) the code below into the newly created test file. 
Fill in the blanks.

    ```r
    context("Check simple calculations")
    
    test_that("plus and minus give correct outputs", {
      expect_identical(1 + 2, 3)
      ___(1 - 2, -1)
      expect_identical(1 + NA, __)
      expect_identical(___ - ___, NA)
    })
    ```

## Describing and developing unit tests {#rse-correctness-develop-tests}

TODO: What is the Python equivalent of context() and the message in test_that()?

Like writing code, your tests should generally be written to be self-explanatory,
and to provide enough documentation to understand what is going on.
Each test file should also try to encompass a general topic
or focus on a specific and similar set of functionality from your package.

In **R**, testthat test's generally are created for one R script in the `R/` folder.
testthat also has several other mechanisms to help clarify what is being tested:

- Multiple forms of `expect_*` functions for different purposes.
For instance, `expect_identical()` compares that actual and expected results are *identical* 
while `expect_equal()` compares that the outputs are approximately equal. 
This last one is used when numerical precision may change depending on operating system 
or other characteristics of the computer influence precision. 
These two are the most commonly used expectation functions. 
Others include `expect_length()`, 
which is used to check how many elements the output should have.

- Documentation of the tests are also incorporated into the results output through
`context("description")` and `test_that("message", ...)`. 
`context()` describes the entire test file, 
while `test_that("message", ...)` is specific to the individual unit test.

TODO: Add python equivalent.

```python

```

### Exercise

- **In Python**:
TODO: Is there an equivalent?

- **In R**: Look at the test below. 
Based on the contents, write appropriate `context()` 
and `test_that()` messages by filling in the blanks. 
There are no right answers, only more descriptive or less descriptive messages.

    ```r
    context("___")
    
    test_that("___", {
      expect_identical(sum(c(1, 2)), 3)
      expect_identical(sum(c(NA, 2)), NA)
    })
    
    test_that("___", {
      expect_identical(mean(c(1, 2)), 1.5)
      expect_identical(mean(c(NA, 2)), NA)
    })
    ```

### Exercise

- Create a new test file. 
Then, write a unit test for `count_words()` to check the following tests.
In R, use a single `test_that()`. 
(**Hint**: For R, use TAB auto-completion to search for useful `expect_*` functions.) 
Test that `count_words()`:
    - Outputs zero when given an empty string.
    - Outputs NA when given NA.
    - Outputs 2 when given two words, separated by a space.
    - Outputs 2 when given two words, but not separated by a space.
    
TODO: Have coherence with functions they previously created? Or stand-alone?

## Testing both code success *and* failure {#rse-correctness-test-failure}

It might not seem immediately necessary, 
but testing how your code *fails* is just as important as when it *succeeds*.
Knowing how and when you code fails (outputs an error) is good practice,
as it forces you to think about how a user might interact with your software
and if your code actually fails when you write that it should.
So our tests should actually include checks on expected errors too.

In **Python**, we can test for an error by using:

```python
import pytest

def test_text_not_empty():
    with pytest.raises(ValueError):
        count_words('')
```

This test includes the `pytest.raises` function to handle tests that checks for errors.
We use `pytest.raises()` with Python's `with` keyword to indicate an expectation
of an error in the code and to have the test fail if an error isn't raised.

In **R**, there are specific testthat functions to test for errors (`expect_error()`)
or warnings (`expect_warning()`):

```r
test_that("count words throws an error", {
    expect_error(count_words(""))
})
```

### Exercise

- Using the same unit as the previous exercise, 
create a new test to check when the code should throw an error. 
Create the test for the `count_words()` function.

## Checking what code is tested {#rse-correctness-coverage}

Great, we've now gone over how to create unit tests. 
But as you write more and more code, 
you may start missing some code and forgetting to write unit tests for them.
Or, you may have complex code with some conditionals (`if ... else ...`) 
that don't always get executed. 
For instance, take a moment to examine the code below.

In **Python**:

```{python}
def first(left, right):
    if left < right:
        left, right = right, left
    while left > right:
        value = second(left, right)
        left, right = right, int(right/2)
    return value

def second(check, balance):
    if check > 0:
        return balance
    else:
        return 0

def main():
    final = first(3, 5)
    print(3, 5, final)

if __name__ == '__main__':
    main()
```

In **R**:

```{r}
first <- function(left, right) {
  if (left < right) {
    tmp <- left
    left <- right
    right <- tmp
  }
  
  while (left > right) {
    value <- second(left, right)
    left <- right
    right <- as.integer(right / 2)
  }
  
  value
}

second <- function(check, balance) {
    if (check > 0)
        balance
    else
        0
}

c(3, 5, first(3, 5))
```

You probably have a hard time figuring out which lines of code were executed and which weren't.
There's actually an easy way to find that out automatically.
There is a tool called code [coverage](glossary.html#coverage) 
to help see how much of your code gets executed by your tests,
which is especially useful with tracking conditional activation.
Code coverage measures which parts of your program are and are not executed.
Essentially, a code coverage tool tracks the progress of execution through the code
and identifies which exact lines gets executed.
After the program finishes, 
code coverage tells you what lines were not executed,
and what percentage was.

Use this tool to make sure a large portion of your code gets tested.
But note, there is no "magic percentage" for how much [test coverage](glossary.html#test-coverage) to have.
It's good to try getting as high a coverage as you can, 
since anything that *isn't* tested may likely be wrong,
while balancing writing tests and writing code.
It usually isn't necessary to get 100% code coverage.

For **Python**, if the above code were in a file, called `demo_coverage.py`,
we can run the coverage with the package `coverage`:

```shell
coverage run demo_coverage.py
coverage report
```

```text
Name               Stmts   Miss  Cover
--------------------------------------
demo_coverage.py      16      1    94%
```

While **R** has similar functionality, it works best at a package level,
rather than a file level.
The covr package has several functions for checking coverage, 
but for testing packages there is:

```r
covr::package_coverage()
```

(covr also has an [RStudio Addin][rstudio-addin] that gives quick access to this command.)

This function will output a list of all R files in the package,
with their corresponding percentage coverage. 
This can give a good overview of where you might need to improve your testing.

## What to test and when {#rse-correctness-what-when}

So far we've discussed *how* to test and *why* to test, but we haven't really 
covered *what* should we test and *when* do we start testing. 
Here are two things to think about when writing tests:

1. No amount of testing can prove the code is completely correct. 
For a function with only a few arguments, 
there may be dozens of possible combinations of inputs.
You can't test all these possible inputs. 
Nor can we write tests correctly all the time (we are only human, who make mistakes).
Writing tests should focus on what is *most likely* going to be input 
and what the software was intended for.

2. Many test cases can be grouped into "classes" of tests. 
For instance, functions with numbers as input could theoretically only need to test a few cases:
infinity (`Inf`), missing values, and one or two tests on intended use.
We may miss unique use cases for our functions when we test this way,
but it makes it easier as the developer 
and as the reader of the code if tests are grouped according to classes.

One topic that often comes up when looking into unit testing is the practice of 
[test-driven development](glossary.html#tdd) (TDD).
Rather than writing code and then writing tests,
TDD advocates suggest to write tests first so we know what the code is supposed to do,
and then write just enough code to make those tests pass.
Once the code works, clean it up and commit it, then move on to the next task.

TDD's advocates claim that this approach emphasizes a focus on what the code should do
so that the developer isn't subject to confirmation bias when viewing the test results.
They also claim that TDD ensures that code actually *is* testable,
and that tests are actually written.
However, the evidence backing these claims is contradictory.
Try TDD out and try to find out what works best for you and your specific problems.

## What is the difference between testing in software engineering and in data analysis? {#rse-correctness-diff}

Testing data analysis pipelines is often harder than testing mainstream software applications.
The reason is that data analysts often don't know what the right answer is,
which makes it hard to check correctness.
The key distinction is the difference between validation and verification.
[Validation](glossary.html#validation) asks, "Is specification correct?"
while [verification](glossary.html#verification) asks,
It's the difference between building the right thing and building something right;
the former question is often much harder for data scientists to answer.

Instead of unit testing,
a better analogy is often physical experiments.
When high school students are measuring acceleration due to gravity,
they should get a figure close to $$10 m/sec^2$$.
Undergraduates might get $$9.8 m/sec^2$$ depending on the equipment used,
but if either group gets $$9.806 m/sec^2$$ with a stopwatch, a marble, and an ramp,
they're either incredibly lucky or cheating.
Similarly,
when testing data analysis pipelines,
we often have to specify tolerances.
Does the answer have to be exactly the same as a hand-calculated value or a previously-saved value?
If not, how close is good enough?

We also need to distinguish between development and production.
During development,
our main concern is whether our answers are (close enough to) what we expect.
We do this by analyzing small datasets
and convincing ourselves that we're getting the right answer in some ad hoc way.

In production,
on the other hand,
our goal is to detect cases where behavior deviates significantly from what we previously decided what right.
We want this to be automated
so that our pipeline will ring an alarm bell to tell us something is wrong
even if we're busy working on something else.
We also have to decide on tolerances once again,
since the real data will never have exactly the same characteristics as the data we used during development.
We also need these checks because the pipeline's environment can change:
for example,
someone could upgrade a library that one of our libraries depends on,
which could lead to us getting slightly different answers than we expected.

## Why should I be cautious when using floating-point numbers? {#rse-correctness-float}

Every tutorial on testing numerical software has to include a discussion of the perils of floating point,
so we might as well get ours out of the way.
The explanation that follows is simplified to keep it manageable;
if you want to know more,
please take half an hour to read @Gold1991.

Finding a good representation for floating point numbers is hard:
we cannot represent an infinite number of real values with a finite set of bit patterns,
and unlike integers,
no matter what values we *do* represent,
there will be an infinite number of values between each of them that we can't.
These days,
floating point numbers are usually represented using [sign](glossary.html#sign),
[magnitude](glossary.html#magnitude) (or [mantissa](glossary.html#mantissa)),
and an [exponent](glossary.html#exponent).
In a 32-bit word,
the IEEE 754 standard calls for 1 bit of sign,
23 bits for the mantissa,
and 8 bits for the exponent.
To illustrate the problems with floating point,
we will use a much simpler 5-bit representation
with 3 bits for the magnitude and 2 for the exponent.
We won't worry about fractions or negative numbers,
since our simple representation will show off the main problems.

The table below shows the possible values (in decimal) that we can represent with 5 bits.
Real floating point representations don't have all the redundancy that you see in this table,
but it illustrates the point.
Using subscripts to show the bases of numbers,
$$110_2 \times 2^{11_2}$$
$$6 \times 2^3$$ or 48.

<table class="table table-striped">
  <tr>
    <th></th>
    <th colspan="4">Exponent</th>
  </tr>
  <tr>
    <th>Mantissa</th> <th>00</th> <th>01</th> <th>10</th> <th>11</th>
  </tr>
  <tr>
    <th>000</th> <td>0</td> <td>0</td> <td>0</td> <td>0</td>
  </tr>
  <tr>
    <th>001</th> <td>1</td> <td>2</td> <td>4</td> <td>8</td>
  </tr>
  <tr>
    <th>010</th> <td>2</td> <td>4</td> <td>8</td> <td>16</td>
  </tr>
  <tr>
    <th>011</th> <td>3</td> <td>6</td> <td>12</td> <td>24</td>
  </tr>
  <tr>
    <th>100</th> <td>4</td> <td>8</td> <td>16</td> <td>32</td>
  </tr>
  <tr>
    <th>101</th> <td>5</td> <td>10</td> <td>20</td> <td>40</td>
  </tr>
  <tr>
    <th>110</th> <td>6</td> <td>12</td> <td>24</td> <td>48</td>
  </tr>
  <tr>
    <th>111</th> <td>7</td> <td>14</td> <td>28</td> <td>56</td>
  </tr>
</table>

Figure \@ref(fig:verify-spacing) is a clearer view of some of the values our scheme can represent:

```{r verify-spacing, echo=FALSE, fig.cap="Number Spacing"}
knitr::include_graphics("figures/rse-correctness/number-spacing.png")
```

A lot of values are missing from this diagram:
for example,
it includes 8 and 10 but not 9.
This is exactly like the problem writing out 1/3 in decimal:
we have to round that to 0.3333 or 0.3334.

But if this scheme has no representation for 9,
then 8+1 must be stored as either 8 or 10.
This raises an interesting question:
if 8+1 is 8, what is 8+1+1?
If we add from the left, 8+1 is 8, plus another 1 is 8 again.
If we add from the right, 1+1 is 2, and 2+8 is 10,
so changing the order of operations can make the difference between right and wrong.

In this case,
if we sort the values and then add from smallest to largest,
it gives us the best chance of getting the best answer.
In other situations,
like inverting a matrix,
the rules are more complicated.
Just as electrical engineers trust oscilloscope makers,
almost all data scientists should trust the authors of core libraries to get this right.

To make this more concrete,
consider the short Python program below.
This program loop runs over the integers from 1 to 9 inclusive
and puts the numbers 0.9, 0.09, 0.009, and so on in `vals`.
The sums should be 0.9, 0.99, 0.999, and so on, but are they?
To find out,
we can calculate the same values by subtracting .1 from 1,
then subtracting .01 from 1, and so on.
This should create exactly the same sequence of numbers, but it doesn't.

<!-- src="verify/fp_table.py" -->
```{python}
vals = []
for i in range(1, 10):
    number = 9.0 * 10.0 ** -i
    vals.append(number)
    total = sum(vals)
    expected = 1.0 - (10.0 ** -i)
    diff = total - expected
    print('{:2d} {:22.21f} {:22.21f}'.format(i, total, total-expected))
```

As the output shows,
the very first value contributing to our sum is already slightly off.
Even with 23 bits for a mantissa,
we cannot exactly represent 0.9 in base 2,
any more than we can exactly represent 1/3 in base 10.
Doubling the size of the mantissa would reduce the error,
but we can't ever eliminate it.

The good news is,
$$9 {\times} 10^{-1}$$ and $$1 - 0.1$$ are exactly the same:
the value might not be precisely right,
but at least they are consistent.
But some later values differ,
and sometimes accumulated error makes the result *more* accurate.

It's very important to note that *this has nothing to do with randomness*.
The same calculation will produce exactly the same results no matter how many times it is run,
because the process is completely deterministic, just hard to predict.
If you see someone run the same code on the same data with the same parameters many times and average the results,
you should ask if they know what they're doing.
(That said,
doing this *can* be defensible if there is parallelism,
which can change evaluation order,
or if you're changing platform,
e.g., moving computation to a GPU.)

## How can I express how close one number is to another? {#rse-correctness-error}

The absolute spacing in the diagram above between the values we can represent is uneven.
However,
the relative spacing between each set of values stays the same:
the first group is separated by 1,
then the separation becomes 2,
then 4,
and then 8.
This happens because we're multiplying the same fixed set of mantissas by ever-larger exponents,
and it leads to some useful definitions.
The [absolute error](glossary.html#absolute-error) in an approximation is the absolute value of
the difference between the approximation and the actual value.
The [relative error](glossary.html#relative-error) is the ratio of the absolute error to the value we're approximating.
For example,
it we are off by 1 in approximating 8+1 and 56+1,
we have the same absolute error,
but the relative error is larger in the first case than in the second.
Relative error is almost always more important than absolute error when we are testing software
because it makes little sense to say that we're off by a hundredth
when the value in question is a billionth.

## How should I write tests that involved floating-point values? {#rse-correctness-numeric}

[Accuracy](glossary.html#accuracy) is how close your answer is to right,
and [precision](glossary.html#precision) is how close repeated measurements are to each other.
You can be precise without being accurate (systematic bias),
or accurate without being precise (near the right answer, but without many significant digits).
Accuracy is usually more important than precision for human decision making,
and a relative error of $$10^{-3}$$ (three decimal places) is more than good enough for most data science
because the decision a human being would make won't change if the number changes by 0.1%.

We now come to the crux of this lesson:
if the function you're testing uses floating point numbers,
what do you compare its result to?
If we compared the sum of the first few numbers in `vals` to what it's supposed to be,
the answer could be `False` even though we're doing nothing wrong.
If we compared it to a previously calculated result that we had stored somehow,
the match would be exact.

No one has a good generic answer to this problem
because its root cause is that we're using approximations,
and each approximation has to be judged in context.
So what can you do to test your programs?
If you are comparing to a saved result,
and the result was saved at full precision,
you could use exact equality,
because there is no reason for the new number to differ.
However,
any change to your code,
however small,
could trigger a report of a difference.
Experience shows that these spurious warnings quickly lead developers to stop paying attention to their tests.

A much better approach is to write a test that checks whether numbers are the same within some [tolerance](glossary.html#tolerance),
which is best expressed as a relative error.
In Python,
you can do this with `pytest.approx`,
which works on lists, sets, arrays, and other collections,
and can be given either relative or absolute error bounds.
To show how it works,
here's an example with an unrealistically tight absolute bound:

<!-- src="verify/approx.py" -->
```python
from pytest import approx

for bound in (1e-15, 1e-16):
    vals = []
    for i in range(1, 10):
        number = 9.0 * 10.0 ** -i
        vals.append(number)
        total = sum(vals)
        expected = 1.0 - (10.0 ** -i)
        if total != approx(expected, abs=bound):
            print('{:22.21f} {:2d} {:22.21f} {:22.21f}'.format(bound, i, total, expected))
```
```text
9.999999999999999790978e-17  6 0.999999000000000082267 0.999998999999999971244
9.999999999999999790978e-17  8 0.999999990000000060775 0.999999989999999949752
```

This tells us that two tests pass with an absolute error of $$10^{-15}$$
but fail when the bound is $$10^{-16}$$,
both of which are unreasonably tight.
(Again, think of physical experiments:
an absolute error of $$10^{-15}$$ is one part in a trillion,
which only a handful of high-precision experiments have ever achieved.)

## How can I test plots and other graphical results? {#rse-correctness-plots}

Testing visualizations is hard:
any change to the dimension of the plot,
however small,
can change many pixels in a [raster image](glossary.html#raster-image),
and cosmetic changes such as moving the legend up a couple of pixels
will similarly generate false positives.

The simplest solution is therefore *not* to test the generated image,
but to test the data used to produce it.
Unless you suspect that the plotting library contains bugs,
feeding it the correct data should produce the correct plot.

If you *do* need to test the generated image,
the only practical approach is to compare it to a saved image that you have visually verified.
[pytest-mpl][pytest-mpl] does this by calculating the root mean square (RMS) difference between images,
which must be below a threshold for the comparison to pass.
It also allows you to turn off comparison of text,
because font differences can throw up spurious failures.
As with choosing tolerances for floating-point tests,
your rule for picking thresholds should be,
"If images are close enough that a human being would make the same decision about meaning,
the test should pass"

FIXME: example

Another approach is to save the plot in a [vector format](glossary.html#vector-image) like [SVG](glossary.html#svg)
that stores the coordinates of lines and other elements as text
in a structure similar to that of HTML.
You can then check that the right elements are there with the right properties,
although this is less rewarding than you might think:
again,
small changes to the library or to plotting parameters can make all of the tests fail
by moving elements by a pixel or two.
Vector-based tests therefore still need to have thresholds on floating-point values.

## How can I test the steps in a data analysis pipeline during development? {#rse-correctness-simple}

We can't tell you how to test your math,
since we don't know what math you're using,
but we *can* tell you where to get data to test it with.
The first method is [subsampling](glossary.html#subsampling):
choose random subsets of your data,
analyze it,
and see how close the output is to what you get with the full dataset.
If output doesn't converge as sample size grows,
something is probably unstable---which is not necessarily the same as wrong.
Instability is often a problem with the algorithm,
rather than with the implementation.

If you do this,
it's important that you select a random sample from your data
rather than (for example) the first N records or every N'th record.
If there is any ordering or grouping in your data,
those techniques can produce samples that are biased,
which may in turn invalidate some of your tests.

FIXME: add an exercise that subsamples the Zipf data.

The other approach is to test with [synthetic data](glossary.html#synthetic-data).
With just a few lines of code,
you can generate uniform data (i.e., data having the same values for all observations),
strongly bimodal data (which is handy for testing clustering algorithms),
or just sample a known distribution.
If you do this,
you should also try giving your pipeline data that *doesn't* fit your expected distribution
and make sure that something, somewhere, complains.
Doing this is the data science equivalent of testing the fire alarm every once in a while.

For example,
we can write a short program to generate data that conforms to Zips' Law and use it to test our analysis.
Real data will be integers (since words only occur or not),
and distributions will be fractional.
We will use 5% relative error as our threshold,
which we pick by experimentation:
1% excludes a valid correct value.
The test function is called `is_zipf`:

<!-- src="verify/test_zipf.py" -->
```python
from pytest import approx


RELATIVE_ERROR = 0.05

    
def is_zipf(hist):
    scaled = [h/hist[0] for h in hist]
    print('scaled', scaled)
    perfect = [1/(1 + i) for i in range(len(hist))]
    print('perfect', perfect)
    return scaled == approx(perfect, rel=RELATIVE_ERROR)
```

<!-- == noindent -->
Here are three tests that use this function
with names that suggest their purpose:

<!-- src="verify/test_zipf.py" -->
```python
def test_fit_correct():
    actual = [round(100 / (1 + i)) for i in range(10)]
    print('actual', actual)
    assert is_zipf(actual)


def test_fit_first_too_small():
    actual = [round(100 / (1 + i)) for i in range(10)]
    actual[0] /= 2
    assert not is_zipf(actual)


def test_fit_last_too_large():
    actual = [round(100 / (1 + i)) for i in range(10)]
    actual[-1] = actual[1]
    assert not is_zipf(actual)
```

## How can I check the steps in a data analysis pipeline in production? {#rse-correctness-operational}

An [operational test](glossary.html#operational-test) is one that is kept in place during production
to tell users if everything is still working as it should.
A common pattern for such tests is to have every tool append information to a log (Chapter \@ref(logging))
and then have another tool check that log file after the run is over.
Logging and then checking makes it easy to compare values between pipeline stages,
and ensures that there's a record of why a problem was reported.
Some common operational tests include:

-   Does this pipeline stage produce the same number of output records as input records?
-   Or fewer if the stage is aggregating?
-   If two or more tables are being [joined](glossary.html#join),
    is the number of output records equal to the product of the number of input records?
-   Is the standard deviation be smaller than the range of the data?
-   Are there any NaNs or NULLs where there aren't supposed to be any?

To illustrate these ideas,
here's a script that reads a document and prints one line per word:

<!-- src="verify/text_to_words.py" -->
```python
import sys

num_lines = num_words = 0
for line in sys.stdin:
    num_lines += 1
    words = [strip_punctuation(w) for w in line.strip().split()]
    num_words += len(words)
    for w in words:
        print(w)
with open('logfile.csv', 'a') as logger:
    logger.write('text_to_words.py,num_lines,{}\n'.format(num_lines))
    logger.write('text_to_words.py,num_words,{}\n'.format(num_words))
```

<!-- == noindent -->
Here's a complementary script that counts how often words appear in its input:

<!-- src="verify/word_count.py" -->
```python
import sys

num_words = 0
count = {}
for word in sys.stdin:
    num_words += 1
    count[word] = count.get(word, 0) + 1
for word in count:
    print('{} {}', word, count[word])
with open('logfile.csv', 'a') as logger:
    logger.write('word_count.py,num_words,{}\n'.format(num_words))
    logger.write('word_count.py,num_distinct,{}\n'.format(len(count)))
```

Both of these scripts write records to `logfile.csv`.
When we look at that file after a typical run,
we see records like this:

```text
text_to_words.py,num_lines,431
text_to_words.py,num_words,2554
word_count.py,num_words,2554
word_count.py,num_distinct,1167
```

We can then write a small program to check that everything went as planned:

```python
# read CSV file into the variable data
check(data['text_to_words.py']['num_lines'] <= data['word_count.py']['num_words'])
check(data['text_to_words.py']['num_words'] == data['word_count.py']['num_words'])
check(data['word_count.py']['num_words'] >= data['word_count.py']['num_distinct'])
```

## How can I infer and check properties of my data? {#rse-correctness-infer}

Writing tests for the properties of data can be tedious,
but some of the work can be automated.
In particular,
the [TDDA library][tdda-site] can infer test rules from data,
such as `age <= 100`, `Date` should be sorted ascending, or `StartDate <= EndDate`.
The library comes with a command-line tool called `tdda`,
so that the command:

```shell
$ tdda discover elements92.csv elements.tdda
```
<!-- used="verify/elements92.csv" -->

<!-- == noindent -->
infers rules from data,
while the command:

```shell
tdda verify elements92.csv elements.tdda
```

<!-- == noindent -->
verifies data against those rules.
The inferred rules are stored as JSON,
which is (sort of) readable with a bit of practice.
Reading the generated rules is a good way to get to know your data,
and modifying values
(e.g., changing the maximum allowed value for `Grade` from the observed 94.5 to the actual 100.0)
is an easy way to make constraints explicit:

```json
"fields": {
    "Name": {
        "type": "string",
        "min_length": 3,
        "max_length": 12,
        "max_nulls": 0,
        "no_duplicates": true
    },
    "Symbol": {
        "type": "string",
        "min_length": 1,
        "max_length": 2,
        "max_nulls": 0,
        "no_duplicates": true
    },
    "ChemicalSeries": {
        "type": "string",
        "min_length": 7,
        "max_length": 20,
        "max_nulls": 0,
        "allowed_values": [
            "Actinoid",
            "Alkali metal",
            "Alkaline earth metal",
            "Halogen",
            "Lanthanoid",
            "Metalloid",
            "Noble gas",
            "Nonmetal",
            "Poor metal",
            "Transition metal"
        ]
    },
    "AtomicWeight": {
        "type": "real",
        "min": 1.007947,
        "max": 238.028913,
        "sign": "positive",
        "max_nulls": 0
    },
    ...
}
```

We can apply these inferred rules to all elements
using the `-7` flag to get pure ASCII output
and the `-f` flag to show only fields with failures:

<!-- used="verify/elements118.csv" -->

```shell
$ tdda verify -7 -f elements118.csv elements92.tdda
```
```text
FIELDS:

Name: 1 failure  4 passes  type OK  min_length OK  max_length X  max_nulls OK  no_duplicates OK

Symbol: 1 failure  4 passes  type OK  min_length OK  max_length X  max_nulls OK  no_duplicates OK

AtomicWeight: 2 failures  3 passes  type OK  min OK  max X  sign OK  max_nulls X

...others...

SUMMARY:

Constraints passing: 57
Constraints failing: 15
```

Another way to use TDDA is to generate constraints for two datasets and then look at differences
in order to see how similar the datasets are to each other.
This is especially useful if the constraint file is put under version control.

## Summary {#rse-correctness-summary}

```{r rse-correctness-concept, echo=FALSE, fig.cap="Correctness Concept Map"}
if (knitr::is_latex_output()) {
  knitr::include_graphics("figures/rse-correctness/concept.pdf")
} else {
  knitr::include_graphics("figures/rse-correctness/concept.svg")
}
```

## Final exercise {#rse-correctness-final-exercise}

1. Write unit tests for all functions you created from previous chapters on Zipf's Law.
Try to think how a user may use your functions and write tests reflecting that.
2. Run code coverage on your package. What is the percentage coverage? 
Is there any way you could write more comprehensive unit tests to increase coverage?
Try to write more to get the coverage higher.

## Key Points {#rse-correctness-keypoints}

```{r, child="keypoints/rse-correctness.md"}
```
