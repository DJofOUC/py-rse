# Project Structure {#rse-project}

```{r rse-project-setup, include=FALSE}
source("_common.R")
```

Project organization is like a diet:
everyone has one,
it's just a question of whether it's healthy or not.
In the case of a project,
"healthy" means that people can find what they need and do what they want without becoming frustrated.
This depends on two things:
how well organized the project is,
and how familiar people are with that style of organization.

As with coding style,
small pieces in predictable places with readable names are easier to find and use
than large chunks that vary from project to project
and have names like "stuff".
While we can be messy while we are working and then tidy up later,
experience teaches that we will be more productive if we make tidiness a habit
and put things in the right place right from the start.
This lesson therefore describes a widely-used template
for organizing small and medium-sized data analysis projects @Nobl2009.

> **Version First**
>
> This chapter assumes that a project's history and development is managed with Git,
> and that each project lives in a single repository.
> If you are organizing something messy,
> please start using version control before you make any changes
> so that you don't accidentally lose valuable work.

## What is a project? {#rse-project-thinking}

The first decision we have to make is what exactly constitutes a "project" @Wils2017.
Some examples are:

-   A dataset that is being used by several research projects.
    The project includes the raw data,
    the programs used to tidy that data,
    the tidied data,
    the extra files needed to make the dataset an R package (Chapter \@ref(rse-package-r)),
    and a few text files describing the data's authors, license, and [provenance][provenance].

-   A set of annual reports written for an [NGO][ngo].
    The project includes several Jupyter notebooks,
    some supporting Python libraries used by those notebooks,
    copies of the HTML and PDF versions of the reports,
    a text file containing links to the datasets used in the report
    (which can't be stored on GitHub since they contain personal identifying information),
    and a text file explaining details of the analysis that the authors didn't include in the reports themselves.

-   A software library that provides an interactive glossary of data science terms in both Python and R.
    The project contains the files needed to create a package in both languages,
    a Markdown file full of terms and definitions,
    and a Makefile with targets to check cross-references, compile packages, and so on.

More generally,
some common criteria for creating projects are one per publication,
one per deliverable piece of software,
or one per team.
The first tends to be too small:
a good dataset will result in several reports,
and the goal of some projects is to produce a steady stream of reports (such as monthly forecasts).
The second is a good fit for software engineering projects
whose primary aim is to produce tools rather than results,
but can be an awkward fit for data analysis work.
The third tends to be too large:
a team of half a dozen people may work on many different things at once,
and a repository that holds them all quickly looks like someone's basement.

The best rule of thumb for deciding what is and isn't a project
is to ask what people have meetings about.
If the same set of people need to get together on a regular basis to talk about something,
that "something" probably deserves its own repository.
And if the list of people changes slowly over time but the meetings continue,
that's an even stronger sign.

## What files should every project contain? {#rse-project-boilerplate}

Most projects' repositories contain four files.
Three of these are so widely used in open source software projects that GitHub provides support for them,
while the fourth is common in research work.
All of these files may be plain text or Markdown,
and may have a `.txt` or `.md` suffix (or no suffix at all),
but should use the principal names given in upper case
since a growing number of tools expect them.

-   `README` includes the project's title and a one-paragraph description of its purpose or content.
    GitHub displays the content of this file on the project's home page.

-   `LICENSE` is the project's license (discussed in Chapter \@ref(rse-teams-software-license)).

-   `CONDUCT` is its code of conduct (also discussed in Chapter \@ref(rse-teams-conduct)).

-   `CITATION` explains how the work should be cited.
    This file should contains a plain text citation that can be copied and pasted into email,
    and may also include entries formatted for various bibliographic systems like [BibTeX][bibtex].

Other information may be included as sections in these files or put into files of their own:

-   `CONTRIBUTORS`
    lists everyone who has contributed to the project.
    Software projects often put this information in `README`,
    while research projects make it a section in `CITATION`.

-   `CONTRIBUTING`
    explains how to contribute,
    i.e.,
    what naming conventions to use for functions,
    what tags to put on issues (Section \@ref(rse-git-advanced-tag)),
    or how to install and configure the software needed to start work on the project.
    These instructions can also be included as a section in `README`;
    wherever they go,
    remember that the easier it is for people to get set up and contribute,
    the more likely they are to do so @Stei2014.

-   `GOVERNANCE`
    explains how the project is run.
    It is still uncommon for this to be in a file of its own—it is more often included
    in `README` or `CONTRIBUTING`—but the open source and open science communities have learned the hard way
    that *not* being explicit about who has a voice in which decisions
    and how contributors can tell when a decision has been made
    causes trouble sooner or later.

Even the four core files may seem like a lot,
but two of these (`LICENSE` and `CONDUCT`) are usually chosen rather than written (Chapter \@ref(rse-teams-conduct))
and the others (`README` and `CITATION`) can be quite short to start with.
Having these files helps new contributors orient themselves,
and also signals that the project is well run.

## How should I structure the rest of my project? {#rse-project-organize}

@Nobl2009 described a way to organize small bioinformatics projects
that is equally useful for other kinds of research computing.
Each project is put in a separate Git repository,
and the directories in the root of this repository are organized according to purpose.
It specifies five top-level directories:

-   The `./src/` directory (short for "source") holds source code
    for programs written in languages like C or C++ that need to be compiled.
    Many projects don't have this directory
    because all of their code is written in languages that don't need compilation.

-   Runnable programs go in `./bin/` (an old Unix abbreviation for "binary", meaning "not text").
    This includes the compiled and runnable versions of C and C++ programs,
    and also shell scripts,
    Python or R programs,
    and everything else that can be executed.

-   Raw data goes in in `./data/` and is never modified after being stored.

-   Results are put in `./results/`.
    This includes cleaned-up data,
    figures,
    and everything else that can be rebuilt using what's in `./bin/` and `./data/`.
    If intermediate results can be re-created quickly and easily,
    they might not be stored in version control,
    but anything that is included in a manuscript should be here.

-   Finally,
    documentation and manuscripts go in `./doc/`.

```{r project-noble, echo=FALSE, fig.cap="Project Layout"}
if (knitr::is_latex_output()) {
  knitr::include_graphics("figures/rse-project/noble.pdf")
} else {
  knitr::include_graphics("figures/rse-project/noble.svg")
}
```

Figure \@ref(fig:project-noble) below shows this layout for a project called `g-trans`.
A few things to notice are:

-   The documentation for the `regulate` script appears in the root of `./doc/`,
    while the paper for JCMB is stored in a sub-directory,
    since it contains several files.

-   The `./src/` directory contains a Makefile to re-build the `regulate` program (Chapter \@ref(rse-automate)).
    Some projects put the Makefile in the root directory,
    reasoning that since it affects both `./src/` and `./bin/`,
    it belongs above them both rather than in either one.

-   There are several sub-directories underneath `./data/` and `./results/`.
    Each of the sub-directories in `./results/` has a Makefile
    to re-create the contents of that directory.

While the directories in the top level of each project are organized by purpose,
the directories within `./data/` and `./results/` are organized chronologically
to make it easy to see when data was gathered and when results were generated.
These directories all have names in [ISO date format][iso-date-format] like `YYYY-MM-DD`
to make it easy to sort them chronologically.
This naming is particularly helpful when data and results are used in several reports.

At all levels,
filenames are chosen so that they will be easy to match with simple shell wildcards.
For example,
a project might use <code><em>species</em>_<em>organ</em>_<em>treatment</em>.csv</code>
as a file-naming convention,
giving filenames like `gorilla_kidney_cm200.csv`.
This allows `gorilla_*_cm200.csv` to match all gorilla organs
or `*_kidney_*.csv` to match all kidney data.
It does produce long filenames,
but [tab completion][tab-completion] means that we only have to type the full name once.
Long filenames are just as easy to match in programs:
Python's `glob` and R's `Sys.glob` will both take a pattern and return a list of matching filenames.

@Marw2018 describes a simple layout:

```
├── DESCRIPTION
├── README.md
├── LICENSE
├── data
│   └── my_data.csv
└── analysis
    └── my_report.Rmd
```

The key differences are:

-   The `DESCRIPTION` file in the root directory contains
    the information needed to make the project an R package (Chapter \@ref(rse-package-r)).

-   Instead of having a separate directory for tidied tata,
    this layout assumes that all work (including data tidying) is done with [computational notebooks][computational-notebook],
    so Noble's `bin`, `src`, and `results` directories are combined.

Going in the other direction,
[Cookiecutter Data Science][cookiecutter] describes a more elaborate layout
with separate sub-directories for models, features, visualizations, and much more.
All of these share a few key advantages:

-   People can find things easily,
    and can easily tell where to put new work.

-   Programs within the project can find things just as easily
    (e.g., by using simple filename patterns).

-   External tools such as GitHub can [aggregate][aggregate] information
    so that the project itself is easier to find.

-   We don't have to document the structure ourselves.

## How should I manage a mix of compiled programs and scripts? {#rse-project-scripts}

Programming languages come in two flavors: compiled and interpreted.
In order to run a program in a [compiled language][compiled-language] such as C++ or Java,
we give the source files to a [compiler][compiler]
that translates them into instructions a computer can actually execute
and saves those instructions in files (Figure \@ref(fig:rse-project-languages)a).
Those files full of instructions can then be re-used as often as we want.
If we are using an [interpreted language][interpreted-language] like R or Python,
on the other hand,
we give our source files to an [interpreter][interpeter].
It also translates the code into instructions,
but puts those instructions in memory and executes them immediately (Figure \@ref(fig:rse-project-languages)b).

```{r rse-project-languages, echo=FALSE, fig.cap="Language Types"}
knitr::include_graphics("figures/FIXME.png")
```

Saving instructions in files versus executing them immediately may seem like a small difference,
but historically it led to very different styles of programming.
Compiled languages usually ran faster than interpreted languages,
but compilation took time,
so interpreted languages were better for [exploratory programming][exploratory-programmming].
The differences are much smaller these days than they were twenty years ago,
but we do still tend to use compiled languages for anything that has to interact directly with hardware
and then [wrap][wrap-code] those libraries for use in interpreted languages.

> **Why `bin`?**
>
> The name `bin` is short for "binary",
> and comes from the fact that the source files of compiled programs are human-readable text,
> but files containing the compiler's output are not.
> Programmers often call files that aren't text "binaries",
> even though text is itself stored in binary as well.

All of this is preamble to deciding where to put things if a project contains compiled programs.
Most software engineers put source code in version control
and recompile it as needed to produce executables:

1.  It saves disk space.

2.  Version control tools can't diff or merge a compiled program.

3.  Compiled programs are much more sensitive to small differences
    between operating system versions and external dependencies
    than interpreted programs,
    so something compiled on one computer might not work on another anyway.

The authors of this book handle all of this in different ways:

1.  Put the source code for compiled languages in `./src/`,
    the runnable programs produced from this code in `./bin/`,
    and programs for interpreted languages in `./scripts/`.
    This makes version control easy—ignore what's in `./bin/` and save everything else—but
    means users have to look in two places to run things.

2.  Put the source for compiled programs in `./src/`
    and both the compiler's output and interpreted programs in `./bin/`.
    This makes version control a little more complicated,
    since some of what's in `./bin/` needs to be saved and some doesn't,
    but means there's only one place to look for runnable programs.
    (It's also easy to delete the hand-written scripts in `./bin/` by accident
    when we only meant to delete the compiled programs,
    but since we have everything under version control,
    that's not a problem, right?)

3.  Put all source files in `./src/` and the compiler's output in `./bin/`.
    This is slightly simpler than the first option,
    but still means users have to look in two places for things they can run.

As always,
the approach matters less than being consistent
and including a note in `CONTRIBUTING` or elsewhere to document the decision.

## How should I document the software in a project? {#rse-project-software-docs}

An old proverb says, "Trust, but verify."
The equivalent in programming is, "Be clear, but document."
No matter how well software is written,
it always embodies decisions that aren't explicit in the final code
or accommodates complications that aren't going to be obvious to the next reader.
Putting it another way,
the best function names in the world aren't going to answer the questions
"Why does the software do this?"
and
"Why doesn't it do this in a simpler way?"
This lesson will explore who we should write documentation for,
what we should write for them,
and where it should go.

Noble's layout places documentation and manuscripts in `./docs/`.
We recommend separating these into `./docs/` and `./reports/`:

1.  Most projects generate the documentation for their software directly from the source code
    (Chapters \@ref(rse-package-r) and \@ref(rse-package-py)).
    Putting these files in the same directory as handwritten files
    has the same problems as putting a compiler's output in the same directory as handwritten scripts.
    In fact,
    it's often worse,
    since [documentation generators][documentation-generator] often create many sub-directories and support files.

2.  We often create several reports for a single project,
    which complicates file management even further.

There are three kinds of people in any domain:
[novices][novice],
[competent practitioners][competent-practitioner],
and [experts][expert] @Wils2018.
A novice doesn't yet have a [mental model][mental-model] of the domain;
they don't know what the key terms are,
how they relate,
what the causes of their problems are,
or how to tell whether a solution to their problem is appropriate or not.

Competent practitioners know enough to accomplish routine tasks with routine effort:
they may need to check [Stack Overflow][stack-overflow] every few minutes,
but they know what to search for and what "done" looks like.
Finally,
experts have such a deep and broad understanding of the domain
that they can solve routine problems at a glance
and are able to handle the one-in-a-thousand cases
that would baffle the merely competent.

Each of these three groups needs a different kind of documentation.
A novice needs a tutorial that introduces her to key ideas one by one
and shows how they fit together.
A competent practitioner needs reference guides, cookbooks, and Q&A sites;
these give her solutions close enough to what she needs
that she can tweak them the rest of the way.
Experts need this material as well—nobody's memory is perfect—but
they may also paradoxically want tutorials.
The difference between them and novices is that experts want tutorials on how things work
and why they were designed that way.

The first thing to decide when writing documentation
is therefore to decide which of these needs you are trying to meet.
Tutorials like this one should be long-form prose that contain code samples and diagrams.
They should use [authentic tasks][authentic-task] to motivate ideas,
i.e.,
show people things they actually want to do rather than printing the numbers from 1 to 10,
and should include regular check-ins
so that learners and instructors alike can tell if they're making progress.

Tutorials help novices build a mental model,
but competent practitioners and experts will be frustrated by their slow pace and low information density.
They will want single-point solutions to specific problems like
how to find cells in a spreadsheet that contain a certain string
or how to configure the web server to load an access control module.
They can make use of an alphabetical list of the functions in a library,
but are much happier if they can search by keyword to find what they need;
one of the signs that someone is no longer a novice is that
they're able to compose useful queries and tell if the results are on the right track or not.

That observation brings us to the notion of a [false beginner][false-beginner],
which is someone who appears not to know anything,
but who has enough prior experience in other domains
to be able to piece things together much more quickly than a genuine novice.
Someone who is proficient with MATLAB, for example,
will speed through a tutorial on Python's numerical libraries
much more quickly than someone who has never programmed before.

In an ideal world,
we would satisfy these needs with a [chorus of explanations][caulfield-chorus],
some long and detailed,
others short and to the point.
In our world, though,
time and resources are limited,
so all but the most popular packages must make do with single explanations.
The rest of this section will therefore look at
how to create reference guides and FAQs.

## What should I document? {#rse-project-infer}

The answer to the question in this section's title depends on what stage of development you are in.
If you are doing [exploratory programming][exploratory-programming],
a short docstring to remind yourself of each function's purpose is good enough.
(In fact, it's probably better than what most people do.)
That one- or two-liner should begin with an active verb and describe either
how inputs are turned into outputs,
or what side effects the function has;
as we discuss below,
if you need to describe both,
you should probably rewrite your function.

An active verb is something like "extract", "normalize", or "find".
For example,
these are all good one-line docstrings:

-   "Create a list of current ages from a list of birth dates."
-   "Clip signals to lie in [0...1]."
-   "Reduce the red component of each pixel."

You can tell your one-liners are useful if you can read them aloud in the order the functions are called
in place of the function's name and parameters.

Once you start writing code for other people---including yourself three months from now---your
docstrings should describe:

1.  The name and purpose of every public class, function, and constant in your code.
2.  The name, purpose, and default value (if any) of every parameter to every function.
3.  Any side effects the function has.
4.  The type of value returned by every function.
5.  What exceptions those functions can raise and when.

The word "public" in the first rule is important.
You don't have to write full documentation for helper functions
that are only used inside your package and aren't meant to be called by users,
but these should still have at least a comment explaining their purpose.
You also don't have to document unit testing functions:
as discussed in Chapter \@ref(rse-correct),
these should have long names that describe what they're checking
so that failure reports are easy to scan.

## How can I create a useful FAQ? {#rse-project-faq}

An [FAQ][faq] is a list of frequently-asked questions and corresponding answers.
A good FAQ uses the terms and concepts that people bring to the software
rather than the vocabulary of its authors;
putting it another way,
the questions should be things that people might search for online,
and the answers should give them enough information to solve their problem.

Creating and maintaining a FAQ is a lot of work,
and unless the community is large and active,
a lot of that effort may turn out to be wasted,
because it's hard for the authors or maintainers of a piece of software
to anticipate what newcomers will be mystified by.
A better approach is to leverage sites like [Stack Overflow][stack-overflow],
which is where most programmers are going to look for answers anyway:

1.  Post every question that someone actually asks you,
    whether it's online, by email, or in person.
    Be sure to include the name of the software package in the question
    so that it's findable.
2.  Answer the question,
    making sure to mention which version of the software you're talking about
    (so that people can easily spot and discard stale answers in the future).

<!-- == noindent -->
With a bit of work,
the [Stack Exchange Data Explorer][stack-exchange-data-explorer]
can be used to download questions and answers about your software
if you want to put them all in an offline guide.
You can also use [Stack Printer][stack-printer] for this;
for example, the URL
<http://www.stackprinter.com/topvoted?service=stackoverflow&tagged=rstudio>
will bring up a paged view of top-voted questions about RStudio.

[Stack Overflow][stack-overflow]'s guide to [asking a good question][stack-overflow-good-question]
has been refined over many years,
and is a good guide for any project:

Write the most specific title you can.
:   "Why does division sometimes give a different result in Python 2.7 and Python 3.5?"
    is much better than, "Help! Math in Python!!"

Give context before giving sample code.
:   A few sentences to explain what you're trying to do and why
    will help people determine if their question is a close match to yours or not.

Provide a minimal reprex.
:   Section \@ref(rse-teams-bugs) explains the value of a [reproducible example][reprex] (reprex),
    and why reprexes should be as short as possible.
    Readers will have a much easier time figuring out if this question and its answers are for them
    if they can see *and understand* a few lines of code.

Tag, tag, tag.
:   Keywords make everything more findable,
    from scientific papers and left-handed musical instruments
    to solutions for programming problems.

Use "I" and question words (how/what/when/where/why).
:   The section headings in these lessons follow this rule for the same reason that questions in a FAQ should:
    writing this way forces you to think more clearly about
    what someone might actually be thinking when they need help.

Keep each item short.
:   The "minimal manual" approach to instructional design @Carr2014
    breaks everything down into single-page steps,
    with half of that page devoted to troubleshooting.
    This may feel like baby steps to the person doing the writing,
    but is often as much as a person searching and reading can handle.
    It also helps writers realize just how much implicit knowledge they are assuming.

Allow for a chorus of explanations.
:   As discussed earlier,
    users are all different from one another,
    and are therefore best served by a chorus of explanations.
    Do not be afraid of providing multiple explanations to a single question
    that suggest different approaches
    or are written for different prior levels of understanding.

## How should I document the data in a project? {#rse-project-data-docs}

## How should I document the data in a project? {#rse-project-data-docs}

## Selecting the right fit

Discussions about what should go into a readme file or other pieces of documentation will often begin the conversation with the end game, and ask authors to report absolutely *everything* that has happened to the data.  However,w the better place to start would be with the honest request of absolutely *anything*.  

Anything is better than nothing but also provides a useful starting point.  Improving from something is an easier task to start with than a goal of writing absolutely everything.  

This chapter has given a set of minimum of information that should be created for every project, and luckily much of that can be boilerplate information.  Software and data licenses will have statements that can be reused or lightly modified, suggested citations may require team discussion but are short, and project titles/goals may be gathered from funding applications.  Taking stock of infromation already produced about a project may yield a good amount of documentation already crafted.

The key to project organization and documentation is sustainability.  Designing a system that can still collect the information needed even during crunch times or late nights depends on the project and the people. As the complexity of a project grows, so should the complexity of the documentation.

The rules to live by when deciding what should go in are:

Documentation should have enough information, 
about the project, methods, and materials
such that the information is maintainable over time, in an accessible format, and valuable for those who need it.

Let's break this apart.

* "enough information" means that there will always be some information safely omitted.  Your goal is not to write everything down, but to write enough down. For example, there may be methods or computations that are best described in the published papers on them.  Unless the paper is locked behind a paywall, it is often sufficient enough to provide this link.
* "about the project, methods, and materials" are the common elements to most projects that will need to be spoken of.  Just like a common personal introduction should have your name, pronouns, and position, you should introduce you data by explaining what the project was for, which methods you used for in, and with which materials.  
* "maintainable" is key as well, because the project or code may grow beyond the documentation.  This may make documentation no longer useful or incorrect.  Documentation should be collected in ways that ensure all members will be able to comply with the system without it becoming burdensome. 
* "accessible format" holds two meanings here. Holding documentation only in proprietary software creates barriers for access and limits their usefulness over time.  Formats change over time and not everyone has access to proprietary reader software.  You may want to have such a format be the primary access method because embedding images and other formatting is better, but always have a plain text copy available. The other meaning is accessible to all people.  This means using proper markup so screen readers can properly interact with the structure of the text, having transcripts for all screencasts or other videos, creating image description alt text, and ensuring that all text is actually rendered so that it can be selected and interacted with.  This is not an exhaustive list of methods to ensure accessibility.  (TODO: is there a better link or paper on this?)
* "for those who need it" means that you should take your audience into account when determining what shold be included and at what level of detail. As discussed within section 10.5, creating material with different styles will help a variety of users. Keeping a broad audience in mind can be a good direction, but this does not mean that you need to write documentation for people who know nothing about the domain.  For example, expecting that you documentation users have graducate level experience in your specific biological research domain may be too narrow, but attempting to write the documentation for a high school biology class too broad of an ask.

The key is balance and sustainability.  More details are great to have in documentation, but asking for too much runs the risk of burnout and creating no documentation.

## How people question data

Good documentation and open science are often promoted for the good of data reusers, or other people who were not part of creating the datasets. Prioritizing these other people can make the converstaion difficult. How can you justify spending this kind of time for other people when you have current projects than need to be addressed for the good of the current people on the project.

Instead of thinking about this group of people as unknown and unrelated, think about the newcomers to your team and your own time to get them on boarded or yourself if you have to come back to this project 5 years after finishing it.  Documentation that serves these two groups well will be either sufficient or really close to what you would need for external reusers. 

## Minimum viable documentation

Think about the questions you would have playing a text adventure game where you wake up in a room you don't recognize.  You would ask things like: Where am I? What's around me? What can I do here?  Where can I go? You need to know that things exist around you before you can go investigate them.  The same thing goes for making your way around data. 

There's a reason that we start with a README file and a short abstract of the project.  People will start in the most general spot to start building their own mental model of expectations around the data.  

This gives a great place to start with, especially with discussions within a team.  What are the absolutely essential pieces of information that someone would need to know about your project, data, and repository folder structure to answer basic questions about it?  

At a minimum, you need to answer questions about content and time. 

Documentation about content should start very general, focusing on describing what is around and where things have gone. People will generally drill down from:

* Project: what did you do?
* Dataset: what did you make?
* Data files (if there are multiple): what's in this file or column here?
* Datum: where did this value come from?

Documenting data is different from code in this way because there's rarely ever an element of time within the documentation.  You are describing contents, not process or a workflow.

Documentation about time includes details about process, transformation, change, and selection.  Similar models for documenting code can apply here. Using short action statements to describe the actions taken to the data are often all that is needed.  

Depending on the complexity, the content description and the process information may be contained in the same statement.  For example, "Contents of gorilla_genome_2020-010-12.bam after run through process.py."  This statement presumes that there is documentation on where these bam files come from and what process.py does is described somewhere.  
 
However, a file that has recieved extensive manual editing and curation of values should have much more detailed description of the editing process.  Some tools, like Open Refine, will include an option to export out a list of all changes and transformations made on a file. This is extremely detailed process information, much like looking at a lenthy series of diffs in version control history.  What these lists of changes cannot include is why these edits were made. Some of this may be within the committ messages, but there may be many changes to speak of within a single commit.

For example, documenting that you changed all values like "[1980?]", "1980?", and "[[1980]" to "1980" is not enough.  The question of "What happened here?" as been answered, but now *why*.  Both pieces must be present for full understanding.

## How to get started

Writing anything can be a difficult process, and documentation should be a collection of living documents. Sustainability remains key, and should be a driver behind decision making.  Setting up places for informaiton to be formally and informally housed is a first step and creating a checklist should be the second.

### Creating "parking lot" notes files

The data used within an active project is often rapidly changed, added, or removed.  Maintaining and collecting relevant documentation within this phase of constant change can be difficult. There are two strategies for keeping up with this pace:

#### Strategy 1: Keep infomal documentation

Creating notes or other "parking lot" files for each dataset provides a place to jot down notes when things happen. These documents should be the home for quick notes, important links, and any other piece of information you aren't sure what to do with.  This takes the pressure off trying to write formally and well, and curbs the decision fatigue of trying to figure out exactly where everything should go.

Screenshots can also be a quick way to grab a bunch of metadata at once.  Whenever you download data from a repository, take a screenshot of the landing page you downloaded it from and save the URL (or suggested citation) to the notes file.  Any website where you have to enter in query fields, ranges, or other information to get a results file is a perfect candidate for this screenshot. 

#### Create a TODO list and assign responsibilities

Like any other element of a research project, establishing a clear scope of work and responsibilities is vital.  "Just write documentation" is a large ask of anyone, without scope for boundaries or end.

Below are topics for documents that cantain tightly scoped questions that can be answered in any order and assigned to the relevant people to complete. Some suggested details are provided, but you should plan to add or change any to best match your particular project.

As mentioned, the complexity of the project will determine the complexity of the documentation created.

##### Inventory the data products

This document will collect important informaion for the team during the active collection and analysis process.  

First, start with a bulletted outline of your data collection points.  This will look different for every domain, but what are the phases or points within the study that data will be collected or produced?  Think very high level here.  For example, a longitudinal study may collect data from subjects during intake and every three weeks following.

Using that list, fill in the data products being collected within that interaction. Think less about the data points being collected, and more about the named instruments, inventories, samples, etc. being collected during those phases.  For example, "participant demographic sheet version April 2019" might be perfectly specific for your team.  

Some of this information may already be available within the detailed descriptions of collection in project proposal documents or human subjects review documentation.  Make a bulleted list of these data products with any caveats.  These names can be short but should be meaningful to the team.  This is a starting point to collect information retrospecively for anything already done, and provide a placeholder for work yet to be completed.


The following questions should be answered for each data product, but can be answered in any order:

* Where are any physical representations of this data stored? This is relevant for anything with surveys, samples, specimines, interview sheets, etc.
* Where are any digital representations of this data stored, and what are their file names?  
* Are there any caveats to data collection for this piece of data?  Note the details and timeframe for any changes to data collection methods, formatting differences, or any other detail that may make this part of the different or surprising in comparison to the others.

##### Develop a template of documentation sections

The next task is to think about the possible sections of documentation, and create a list of relevant sections needed.  

One large problem with guides for creating data documentation is the volume of possible sections needed across all domains.  Net every section is relevant or valuable for each project.

Each team or lab should make it a goal of creating a broadly useful template for data documentation.  This would combine the elements commonly used within their domain and required by their institutions and funders. Even if each project needs a custom version and every project changes, the documentation will all be similar because it all starts from the same template.  

This template should be a list of sections, with relevant boilerplate text for when that makes sense.  For example, the lab may have a standard license, contributing, and code of conduct. 	This template should also be stored under version control so changes and proposals can be discussed, and those changes tracked over time.

## Exercises

### Understanding a project

You will be exploring the following dataset and trying to understand the relationship between the study and the included files.

Meili, Stephen. Do Human Rights Treaties Help Asylum-Seekers: Findings from the U.K.. Ann Arbor, MI: Inter-university Consortium for Political and Social Research [distributor], 2015-05-21. http://doi.org/10.3886/E17507V2

Go to the dataset's page (http://doi.org/10.3886/E17507V2) and download the files.  You'll need to make an ICPSER account and agree to their data agreement before you can download.

Review the dataset's main page to get a sense of the study.  Review the spreadsheet file and the coded response file.  Answer the following questions:

* Who are the participants of this study?
* What types of data was collected and used for analysis?
* Can you find information on the demographics of the interviewees?
* This dataset is clearly in support of an article. What information can you find about it, and can you find a link to it?

### Creating documentation template

The first step to creting documentation is to think about the audience over time.  Not every potential user of the documentation will need it in the short term, so starting with the most immediate users provides some strategic planning for time.  

#### Developing your list of audiences

Below is a grid of potential audiences and timeframes.  These groups are generic, and should be adjusted as needed for your project's domain.  For example, some geology and climate data are meant to be preserved for decades, while other projects only have a useful lifespan of a few years.

Step through each potential audience group and place an X or other mark in each box to indicate if that audience group would be in need of the data and in which timeframes. You can repeat marks in each column and row as needed. You may also want to use several types of marks to indicate certainty or uncertainty.

| Currently | Within the next 2 years | 2-5 years from now | 5+ years | Audience                        |
|-----------|-------------------------|--------------------|----------|---------------------------------|
|           |                         |                    |          | Just me                         |
|           |                         |                    |          | My project advisor or PI        |
|           |                         |                    |          | Others in my project team       |
|           |                         |                    |          | My lab or department members    |
|           |                         |                    |          | Researchers in my field         |
|           |                         |                    |          | Researchers outside of my field |
|           |                         |                    |          | Publication reviewers           |
|           |                         |                    |          | Federal or government agency    |
|           |                         |                    |          |                                 |
|           |                         |                    |          |                                 |

#### Developing your list of sections

Below is a list of sections and subsections that are generally useful across many domains. Start with this list and edit to remove, add, or edit sections that would be relevant to your team or project. You may also wish to print this out and 	pass this around to all your group members to fill out independently, and a facilitator gathers and synthesizes the renults.  A blank section is at the end to encourage you to add your own informaiton.

Additional notes and other metadata could be added to each section to assign a specific person to write that section, collect up the informaiton for it, etc.

* Administrative and personnel details:
	* Authors, principle investigators, contributors, etc., with associated including institutions, contact information, and other identifiers when available
	* Description of project team
	* Associated papers, code, talks, datasets, etc.
	* Funders and grant numbers
* Data licensing information
	* Suggested citation
	* Who to contact if there are any questions
* Project information:
	* Brief description of the dataset and/or abstract, including relevant collection and processing dates
	* Collection methods, including dates of collection, data processing, etc.
	* Names, model numbers, and calibration information for any instruments used during data collection
	* Description of scripts (e.g. R, Python, MATLAB, etc.) and their purpose
	* Data processing workflow and stages
	* Data cleaning process
	 De-identification or other data scrubbing steps that occurred
* Data file information (repeat for each file as needed):
	* List of files to be included, grouped in meaningful units
	* Number of rows, fields, columns, etc.
	* Description of folder contents and/or of large groups of similar files
	* Explanation of formats and required software to read them
	* Languages represented within your data
	* Description of the values, units, etc. for each column or field (codebook)
	* Description of columns and fields in the data files (data dictionary)
	* Other domain specific descriptive information
* Other:
	*  
	*  
	*  
	*  
	*   




#### Solutions

* Who are the participants of this study?
	* 51 soliciters were interviwed as the participants
* What types of data was collected and used for analysis?
	* Interview data and a data from a database on court decisions
* Can you find information on the demographics of the interviewees?
	* This information is not available within the documentation.  Information on their jobs and opinions are, but the participant demographics are only described within the associated article. The difficulty is that the article is not linked within the documentation or the metadata.
* This dataset is clearly in support of an article. What information can you find about it, and can you find a link to it?
	* You can search the dataset name and authorname trying to find this. But if you search for "National Science Foundation (1228602)", which is the grant information, you will find the grant page.  https://www.nsf.gov/awardsearch/showAward?AWD_ID=1228602. There are two articles linked there.  Unfortunately both the DOI links are broken.  You can search with the citation for each paper to find them. The Forced Migration article can be found here: https://www.fmreview.org/fragilestates/meili but uses a different subset of interviews and does not mention demographics nor links to the deposited dataset.  The Boston College Law Review article can be found here: https://lawdigitalcommons.bc.edu/cgi/viewcontent.cgi?article=3318&context=bclr which has the same two problems of different data and no dataset citation. Searching more broadly through Meili's work, we can find this article: 

	Meili, Stephen, Do Human Rights Treaties Help Asylum-Seekers?: Lessons from the United Kingdom (October 1, 2015). Minnesota Legal Studies Research Paper No. 15-41. Available at SSRN: https://ssrn.com/abstract=2668259 or http://dx.doi.org/10.2139/ssrn.2668259. This does list the dataset as a footnote and reports the 51 interviews with demographic data on reported gender of the interviewees.  This paper lists data collection as 2010-2014, while the other two say 2010-2013. We might come to a conclusion that this extra year is where the extra 9 interviews come in, but that difference is not explained anywhere.


### References

Wickes, E. & Stein, A. (2016). Data Documentation Materials. Accessed from IDEALS: http://hdl.handle.net/2142/91611

## How should I manage data that can't be stored in version control? {#rse-project-external}

Small datasets that don't contain sensitive information should be stored in version control:
as a rule of thumb,
anything you would send as an email attachment is probably small enough to be put into Git,
while anything that might reveal someone's identity should not be.
If data is large or sensitive,
there should still be something in `./data/` to show its existence,
and that "something" should be easy for programs to read.
One option is a CSV file whose columns are:

-   the name of the dataset,
-   its URL or other unique identifier,
-   the date it was last checked, and
-   its size (so that users will have some idea of how much work is involved in processing it).

Another option is to have one file per dataset,
so that instead of reading `gorilla_genome.bam`,
the program reads `gorilla_genome.yml` and then uses the `url` key in that file to find the data it actually wants.
Whatever you do,
you should always include a `README.md` file in `./data/`
that documents the [provenance][provenance] and organization of the data
(Section \@ref(rse-publish-data)).

## Summary {#rse-project-summary}

FIXME: create concept map for project structure

## Exercises {#rse-project-exercises}

FIXME: mimic Elizabeth's data exercise from https://www.ideals.illinois.edu/handle/2142/91611 to look at a couple of projects on GitHub and complete some set tasks.
Get learners to reflect on what makes it easy to navigate an unfamiliar project.

FIXME: Think about a project you have and all the files it it.  What would it look like in one of these structures?

## Key Points {#rse-project-keypoints}

```{r, child="keypoints/rse-project.md"}
```
