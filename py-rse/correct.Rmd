# Correctness {#py-rse-correct}

TODO: Add Git commits for exercises throughout

```{r py-rse-correct-setup, include=FALSE}
source(here::here("_common.R"))
```

Now that we've written our software for counting and analyzing the words in classic texts,
how can we be sure that it’s producing reliable results?
The short is answer is that we can't be completely certain,
but that as in science,
we can test our expectations against reality to help us decide if we are sure enough.
In this chapter we will explore the approaches available for testing code:
assertions, unit tests, regresson tests and integration tests.

Our Zipf's Law project files are structured as they were at the end of the previous chapter:

```text
├── CONDUCT.md
├── LICENSE.md
├── Makefile
├── README.md
├── bin
│   ├── book_summary.sh
│   ├── collate.py
│   ├── countwords.py
│   ├── mymodule.py
│   ├── plotcounts.py
│   └── rcparams.yml
├── data
│   ├── README.md
│   ├── dracula.txt
│   ├── risk.txt
│   └── ...
└── results
    ├── dracula.csv
    ├── dracula.png
    └── ...
```

## Assertions {#py-rse-assertions}

The first step toward getting the right answers from our programs
is to assume that mistakes will happen and to guard against them.
This is called defensive programming,
and the most common way to do it is to add assertions to our code
so that it checks itself as it runs.
An assertion is simply a statement that something must be true at a certain point in a program.
When Python sees one, it evaluates the assertion’s condition.
If it’s true, Python does nothing, but if it’s false,
Python halts the program immediately and prints the error message if one is provided.

To demonstrate an assertion in action,
consider this piece of code that halts as soon as the loop encounters
a rainfall observation that isn’t positive:

```python
rainfall_observations = [1.5, 2.3, 0.7, -0.2, 4.4]
total = 0.0
for ob in rainfall_observations:
    assert ob >= 0.0, 'Rainfall observations should only contain positive values'
    total += ob
print('total rainfall is:', total)
```

```text
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-19-33d87ea29ae4> in <module>()
      2 total = 0.0
      3 for ob in rainfall_obs:
----> 4     assert ob > 0.0, 'Rainfall observations should only contain positive values'
      5     total += ob
      6 print('total rainfall is:', total)

AssertionError: Rainfall observations should only contain positive values
```

Programs like the Firefox browser are full of assertions:
10-20% of the code they contain are there to check that the other 80-90% are working correctly.


## Unit testing {#py-rse-unit-testing}

As the name suggests,
unit tests relate to small "units" or pieces of our code.
This typically means functions that we've defined.
What is considered to be the smallest code unit is subjective --
the body of a function can be long or short,
and shorter functions are arguably more unit-like than long ones --
but a good guideline is that if the code cannot be made any simpler
logically (you cannot split apart the addition operator) or 
practically (a function is self-contained and well defined),
then it is a unit.

In the case of our Zipf's Law software,
the `count_words` function in the `wordcounts.py` script is a good example of a code unit:

```
def count_words(reader):
    """Count the occurrence of each word in a string."""
    text = reader.read()
    findwords = re.compile(r"\w+", re.IGNORECASE)
    word_list = re.findall(findwords, text)
    word_counts = Counter(word_list)
    return word_counts
```

A single unit test will typically have:

-   a [fixture][fixture],
    which is the thing being tested (e.g., an array of numbers);
-   an [actual result][actual-result],
    which is what the code produces when given the fixture; and
-   an [expected result][expected-result]
    that the actual result is compared to.

In defining/creating an appropriate test fixture,
a common approach is to use a subset or smaller version of the data
that the function will typically process.
For instance, in order to write a unit test for the `count_words` function,
an appropriate fixture would be a small body of text
for which we know the frequency of each word.
The poem Risk by Anais Nin is a good choice,

```shell
$ cat ../data/risk.txt
```

```text
And then the day came,
when the risk
to remain tight
in a bud
was more painful
than the risk
it took
to blossom.
```

because it's so short that we can count the words by hand in order to create
an expected result variable:

```python
from collections import Counter

risk_poem_counts = {'the': 3, 'risk': 2, 'to': 2, 'And': 1, 'then': 1,
                    'day': 1, 'came': 1, 'when': 1, 'remain': 1, 'tight': 1,
                    'in': 1, 'a': 1, 'bud': 1, 'was': 1, 'more': 1,
                    'painful': 1, 'than': 1, 'it': 1, 'took': 1, 'blossom': 1}
expected_result = Counter(risk_poem_counts)
```

We can generate the actual result by calling the `word_counts` function,
and then use an assertion to check that the actual and expected results
are the same:

```python
import countwords

with open('../data/risk.txt', 'r') as reader:
    actual_result = countwords.count_words(reader)
assert actual_result == expected_result
```

There's no output,
which means the test has passed
(recall that assertions only do something if the condition evaluates to false).

### Testing frameworks

Writing a single unit test for our Zipf's Law software is a good start,
but to be sure about the reliability of our code we'll probably want to write more unit tests.
To coordinate the running of many tests,
we can use a [test framework][test-framework] (also called a [test runner][test-runner]).
The most widely used test framework for Python is [`pytest`][pytest],
for which tests obey three rules:

1.  All tests are put in files whose names begin with `test_`.
2.  Each test is a function whose name also begins with `test_`.
3.  These functions use `assert` to check results.

Following those rules,
we can create a `test_zipfs.py` script that contains the test we just developed:

```python
from collections import Counter
import countwords

def test_word_count():
    """Test the counting of words.
    
    The example poem is Risk, by Anais Nin.
    """
    risk_poem_counts = {'the': 3, 'risk': 2, 'to': 2, 'And': 1, 'then': 1,
                        'day': 1, 'came': 1, 'when': 1, 'remain': 1, 'tight': 1,
                        'in': 1, 'a': 1, 'bud': 1, 'was': 1, 'more': 1,
                        'painful': 1, 'than': 1, 'it': 1, 'took': 1, 'blossom': 1}
    expected_result = Counter(risk_poem_counts)
    with open('risk.txt', 'r') as reader:
        actual_result = countwords.count_words(reader)
    assert actual_result == expected_result
```

Tests can be saved anywhere in the project directory,
but usually they are placed in a `tests/` subdirectory.

The `pytest` library comes with a command-line tool that is also called `pytest`.
When we run it with no options,
it searches for all files in the working directory and subdirectories named `test_*.py`.
It runs the tests in these files and then summarizes their results.
(If you only want to run tests in a particular file,
use the command `pytest path/to/test_file.py`.)

```shell
$ pytest
```
```text
============================= test session starts ==============================
platform darwin -- Python 3.7.6, pytest-5.4.1, py-1.8.1, pluggy-0.12.0
rootdir: /Users/amira/bin
collected 1 item                                                               

test_zipfs.py .                                                          [100%]

============================== 1 passed in 0.02s ===============================
```

In order to add to our suite of unit tests,
we can now simply add more `test_` functions to `test_zipfs.py`.
For instance,
besides the counting of words (which we've already tested),
the other critical part of our code is the calculation of the \(\alpha\) parameter.
Earlier we defined a power law relating \(\alpha\)
to the word frequency \(f\), word rank \(r\) 
and a constant \(c\) of proportionality,\[
r = cf^{\frac{-1}{\alpha}}
\]
and noted that Zipf's Law holds exactly when \(\alpha\) is equal to one.
Setting \(\alpha\) to one and re-arranging the power law gives the expression,\[
c = f/r
\], which we can use to generate synthetic word counts data 
(i.e. our test fixture) with a constant of proportionality
set to a hypothetical maximum word frequency of 600
(and thus \(r\) ranges from 1 to 600):

```python
import numpy as np

max_freq = 600
word_counts = np.floor(max_freq / np.arange(1, max_freq + 1)) 
```

(The `np.floor` function rounds down to the nearest whole number,
because you can't have fractional word counts.)

Passing this test fixture to the `get_power_law_params` function from our
`plotcounts.py` script,

```python
def get_power_law_params(word_counts):
    """
    Get the power law parameters.
    References
    ----------
    Moreno-Sanchez et al (2016) define alpha (Eq. 1),
      beta (Eq. 2) and the maximum likelihood estimation (mle)
      of beta (Eq. 6).
    Moreno-Sanchez I, Font-Clos F, Corral A (2016)
      Large-Scale Analysis of Zipf’s Law in English Texts.
      PLoS ONE 11(1): e0147073.
      https://doi.org/10.1371/journal.pone.0147073
    """
    mle = minimize_scalar(nlog_likelihood, bracket=(1 + 1e-10, 4),
                          args=(word_counts), method='brent')
    beta = mle.x
    alpha = 1 / (beta - 1)
    return alpha
```
should return an expected value of 1.0.

To test this, we can add a second test to `test_zipfs.py`,

```python
import numpy as np
from collections import Counter

import plotcounts
import countwords

def test_alpha():
    """Test the calculation of the alpha parameter.
    
    The test word counts satisfy the relationship,
      r = cf**(-1/alpha), where
      r is the rank,
      f the word count, and
      c is a constant of proportionality.

    To generate test word counts for an expected alpha value of 1.0,
      a maximum word frequency of 600 is used
      (i.e. c = 600 and r ranges from 1 to 600)
    """    
    max_freq = 600
    word_counts = np.floor(max_freq / np.arange(1, max_freq + 1))
    actual_alpha = plotcounts.get_power_law_params(word_counts)
    expected_alpha = 1.0
    assert actual_alpha == expected_alpha

def test_word_count():
    """Test the counting of words.
    
    The example poem is Risk, by Anais Nin.
    """
    risk_poem_counts = {'the': 3, 'risk': 2, 'to': 2, 'And': 1, 'then': 1,
                        'day': 1, 'came': 1, 'when': 1, 'remain': 1, 'tight': 1,
                        'in': 1, 'a': 1, 'bud': 1, 'was': 1, 'more': 1,
                        'painful': 1, 'than': 1, 'it': 1, 'took': 1, 'blossom': 1}
    expected_result = Counter(risk_poem_counts)
    with open('../data/risk.txt', 'r') as reader:
        actual_result = countwords.count_words(reader)
    assert actual_result == expected_result
```

and then re-run pytest:

```shell
$ pytest
```

```text
============================= test session starts ==============================
platform darwin -- Python 3.7.6, pytest-5.4.1, py-1.8.1, pluggy-0.12.0
rootdir: /Users/amira/bin
collected 2 items                                                              

test_zipfs.py F.                                                         [100%]

=================================== FAILURES ===================================
__________________________________ test_alpha __________________________________

    def test_alpha():
        """Test the calculation of the alpha parameter.
    
        The test word counts satisfy the relationship,
          r = cf**(-1/alpha), where
          r is the rank,
          f the word count, and
          c is a constant of proportionality.
    
        To generate test word counts for an expected alpha value of 1.0,
          a maximum word frequency of 600 is used
          (i.e. c = 600 and r ranges from 1 to 600)
        """
        max_freq = 600
        word_counts = np.floor(max_freq / np.arange(1, max_freq + 1))
        actual_alpha = plotcounts.get_power_law_params(word_counts)
        expected_alpha = 1.0
>       assert actual_alpha == expected_alpha
E       assert 0.9951524579316625 == 1.0

test_zipfs.py:24: AssertionError
=========================== short test summary info ============================
FAILED test_zipfs.py::test_alpha - assert 0.9951524579316625 == 1.0
========================= 1 failed, 1 passed in 0.85s ==========================
```

The output tells us that one test failed but the other test passed.
This is a nice feature of test runners like pytest -- they continue on and complete all the tests
rather than stopping at the first assertion failure like a regular Python script would.
Looking more closely at the output, we can see that although `test_alpha` failed,
the `actual_alpha` value of 0.9951524579316625 was awfully close to the expected value of 1.0.
In fact,
we don't expect the estimation routine used by `get_power_law_params` to be perfect.
In order to edit `test_alpha` so that "close enough is good enough"
we can borrow a `pytest` function used for equating floating point values...  

### Testing and floating-point values? {#py-rse-correct-numeric}

Testing floating-point values is tricky because of the way computers represent these values.
Different operating systems, hardware,
and software have subtle differences in how these values are stored as numbers.
So, if we are testing a function that uses floating point numbers,
what do we compare its result to if computers represent the number differently?
If we try to test against a value with many decimal places (highly precise),
the test may fail because floating point values are represented as approximations.
No one has a good generic answer to this problem
because its root cause is that we're using approximations,
and each approximation has to be judged in its own context.

So what can you do to test your programs?
A good approach is to write a test that checks whether numbers are the same within some [tolerance][tolerance],
which is best expressed as a relative or absolute error.
The [absolute error][absolute-error] is the absolute value of
the difference between the approximation and the actual value.
The [relative error][relative-error] is the ratio of the absolute error to the value we're approximating.
For example,
if we are off by 1 in approximating 8+1 and 56+1,
we have the same absolute error,
but the relative error is larger in the first case than it is in the second.

For our `test_alpha` function,
we might decide that an absolute error of 0.01 is acceptable.
Using the pytest library,
we can define this tolerance level using `pytest.approx`:

```python
import pytest
import numpy as np
from collections import Counter

import plotcounts
import countwords

def test_alpha():
    """Test the calculation of the alpha parameter.
    
    The test word counts satisfy the relationship,
      r = cf**(-1/alpha), where
      r is the rank,
      f the word count, and
      c is a constant of proportionality.

    To generate test word counts for an expected alpha value of 1.0,
      a maximum word frequency of 600 is used
      (i.e. c = 600 and r ranges from 1 to 600)
    """    
    max_freq = 600
    word_counts = np.floor(max_freq / np.arange(1, max_freq + 1))
    actual_alpha = plotcounts.get_power_law_params(word_counts)
    expected_alpha = pytest.approx(1.0, abs=0.01)
    assert actual_alpha == expected_alpha

def test_word_count():
    """Test the counting of words.
    
    The example poem is Risk, by Anais Nin.
    """
    risk_poem_counts = {'the': 3, 'risk': 2, 'to': 2, 'And': 1, 'then': 1,
                        'day': 1, 'came': 1, 'when': 1, 'remain': 1, 'tight': 1,
                        'in': 1, 'a': 1, 'bud': 1, 'was': 1, 'more': 1,
                        'painful': 1, 'than': 1, 'it': 1, 'took': 1, 'blossom': 1}
    expected_result = Counter(risk_poem_counts)
    with open('../data/risk.txt', 'r') as reader:
        actual_result = countwords.count_words(reader)
    assert actual_result == expected_result
```

Re-running pytest, all tests now pass:

```shell
$ pytest
```

```text
============================= test session starts ==============================
platform darwin -- Python 3.7.6, pytest-5.4.1, py-1.8.1, pluggy-0.12.0
rootdir: /Users/amira/bin
collected 2 items                                                              

test_zipfs.py ..                                                         [100%]

============================== 2 passed in 0.69s ===============================
```

## Integration testing {#py-rse-correct-integration}

We've seen in previous chapters that our Zipf's Law analysis is a two-step process.
The first involves counting the words in a text
and the second involves estimating the \(\alpha\) parameter from the word count.
Our unit tests `test_word_count` and `test_alpha` have checked that those two
individual components work in isolation, but do they work correctly together?
Checking that multiple units work correctly together is called *integration testing*.

The structure of integration tests is very similar to that of unit tests.
There is an expected result, which is compared against the observed value.
However, the work involved in creating the expected result or setting up the code to run
can be considerably more complicated and more involved. 
For example,
in the case of our Zipf's Law software an appropriate integration test fixture
might be a text file full of words that have a known alpha value. 
In order to create this text fixture,
we'll need a way of generating random words.
Fortunately, a Python library exits to do just that.
We can install it (and the `pypandoc` library it depends on) at the command line
using the Python Package Installer (pip): 

```shell
$ pip install pypandoc
$ pip install randomwordgenerator
```

Borrowing from the word count distribution we created for `test_alpha`,
we can then create a text file full of random words that correspond to an alpha value
of approximately 1.0.

```python
import numpy as np
from randomwordgenerator import randomwordgenerator

max_freq = 600
word_counts = np.floor(max_freq / np.arange(1, max_freq + 1))
random_words = randomwordgenerator.generate_random_words(n=max_freq)
fout = open('../data/random_words.txt', 'w')
for index in range(max_freq):
    word_sequence = f"{random_words[index]} " * int(word_counts[index])
    fout.write(word_sequence + '\n')
fout.close()
```

The following integration test can then be added to `test_zipfs.py`,

```python
def test_integration():
    """Test the full word count to alpha parameter workflow."""    

    with open('../data/random_words.txt', 'r') as reader:
        word_counts_dict = countwords.count_words(reader)
    word_counts_array = np.array(list(word_counts_dict.values()))
    actual_alpha = plotcounts.get_power_law_params(word_counts_array)
    expected_alpha = pytest.approx(1.0, abs=0.01)
    assert actual_alpha == expected_alpha
```

and we can re-run `pytest` to see whether the integration test passes.

```shell
$ pytest
```

```text
============================= test session starts ==============================
platform darwin -- Python 3.7.6, pytest-5.4.1, py-1.8.1, pluggy-0.12.0
rootdir: /Users/amira/bin
collected 3 items                                                                                         

test_zipfs.py ...                                                        [100%]

=============================== 3 passed in 0.48s ==============================
```


## Regression testing {#py-rse-correct-regression}

So far we've tested two simplified texts --
a short poem and a collection of random words -- but not a "real"  text.
The problem with testing a large text is that we don't know the expected result 
(e.g. it's not practical to sit and count the words in Dracula by hand).
For this kind of situation we can consider *regression testing*.
Rather than assuming that the test author knows what the expected result should be,
regression tests look to the past for the expected behavior.
The expected result is taken as what was previously computed for the same inputs.
In this way regression tests are great for letting developers know when and how a code base has changed,
but not great for letting anyone know why the change occurred.
The change between what a code produces now and what it computed before is called a regression.

In Chapter X we calculated an \(\alpha\) value of 1.1620041050803658 for Dracula.
We can use this expected result to add a regression test to `test_zipfs.py`:

```python
def test_regression():
    """Regression test for Dracula."""    

    with open('../data/dracula.txt', 'r') as reader:
        word_counts_dict = countwords.count_words(reader)
    word_counts_array = np.array(list(word_counts_dict.values()))
    actual_alpha = plotcounts.get_power_law_params(word_counts_array)
    expected_alpha = pytest.approx(1.162, abs=0.001)
    assert actual_alpha == expected_alpha
```

```shell
$ pytest
```

```text
============================= test session starts ==============================
platform darwin -- Python 3.7.6, pytest-5.4.1, py-1.8.1, pluggy-0.12.0
rootdir: /Users/amira/bin
collected 4 items                                                                                

test_zipfs.py ....                                                       [100%]

============================== 4 passed in 0.56s ===============================
```

## Test coverage {#py-rse-correct-coverage}

In defining tests for counting words and calculating the alpha parameter,
it's likely that we have now tested (or "covered") all critical lines of our code.
To be sure,
we can use a tool to check the coverage of our tests.
Most Python programmers use the `coverage` library:

```shell
$ pip install coverage
```

Using the command-line utility that comes with the install,
we can then run `pytest` under coverage:

```shell
$ coverage run -m pytest
```

```text
====================================== test session starts =======================================
platform darwin -- Python 3.7.6, pytest-5.4.1, py-1.8.1, pluggy-0.12.0
rootdir: /Users/amira/bin
collected 4 items                                                                                

test_zipfs.py ....                                                                         [100%]

======================================= 4 passed in 0.72s ========================================
```

This coverage command hasn't displayed any information of its own;
instead,
it puts coverage data in a file called `.coverage` (with a leading `.`) in the current directory.
To display that data,
we run:

```shell
$ coverage report -m
```

```text
Name            Stmts   Miss  Cover   Missing
---------------------------------------------
countwords.py      20      8    60%   19-21, 25-31
mymodule.py         7      4    43%   24-27
plotcounts.py      46     27    41%   41-47, 67-69, 74-90, 94-104
test_zipfs.py      32      0   100%
---------------------------------------------
TOTAL             105     39    63%
```

From this coverage summary,
we can see that not every single line of `countwords.py` or `plotcounts.py`
was executed when we ran the tests (only 60% and 41% of the lines we run, respectively).
This would seem to make sense, because much of the code in those scripts
is devoted to handling command line arguments or file input/output,
rather than the word counting and parameter estimation functionality 
that our unit, integration and regression tests focused on.
To make sure that's the case,
we can get a more complete report by running `coverage html` at the command line
and opening `htmlcov/index.html`.
Clicking on the name of our `countwords.py` script, for instance,
produces the colorized line-by-line display shown in Figure \@ref(fig:python-coverage).

```{r python-coverage, echo=FALSE, fig.cap="Coverage report"}
knitr::include_graphics("figures/rse-correct/python-coverage.png")
```

This output confirms that all lines relating to word counting were tested,
but not any of the lines related to the handling of command line arguments or
file input and output. 

### How much test coverage is enough? {#py-rse-correct-enough}

How much we test our software depends on the purposes it will be used for.
If we are writing software for a safety-critical application such as a medical device,
we should aim for 100% code coverage,
i.e.,
every single line in the application should be tested.
In fact,
we should probably go further and aim for 100% [path coverage][path-coverage]
and ensure that every possible path through the code has been checked.

But most of us don't write software that people's lives depend on,
so requiring 100% code coverage is like asking for ten decimal places of accuracy
when checking the voltage of a household electrical outlet.
We always need to balance the effort required to create tests
against the likelihood that those tests will uncover useful information.

It's also important to understand that no amount of testing
can prove a piece of software is completely correct.
A function with only two numeric arguments has 2^128^ possible inputs.
Even if we could write the tests,
how could we be sure we were checking the result of each one correctly?

Testing data analysis pipelines is often harder than testing mainstream software applications,
since data analysts often don't know what the right answer is.
(If we did,
we would have submitted our report and moved on to the next problem already.)
The key distinction is the difference between [validation][validation],
which asks whether the specification is correct,
and [verification][verification],
which asks whether we have met that specification.
The difference between them is the difference between
building the right thing and building something right,
and the first is often hard for data scientists to address.

Luckily,
we can group the test cases for most functions into classes.
For example,
it might be possible to test a function that takes numbers as inputs
as well as needing to use only a few cases:
zero, a positive number, a negative one, and infinity.
If we want to go further,
we could check that it fails the right way when given a string or a list.
Similarly,
when testing a function that summarizes a table full of data,
we probably should check that it handles tables with:

-   no rows
-   only one row
-   many identical rows
-   rows having keys that are supposed to be unique, but aren't
-   rows that contain nothing but missing values

Some projects develop [checklists][checklist] to remind programmers what they ought to test.
These checklists can be a bit daunting for newcomers,
but they are a great way to pass on hard-earned experience.


## Other considerations when testing {#py-rse-correct-other}

TODO:
- When should I write my tests?
- Plots and graphical results
- What data should I use? (subsampling, synthetic)
- Checking that things fail correctly (i.e. with the correct error message)




## Exercises {#py-rse-correct-exercises}

TODO


## Key Points {#py-rse-correct-keypoints}

```{r, child="keypoints/rse-correct.md"}
```

```{r, child="./links.md"}
```
