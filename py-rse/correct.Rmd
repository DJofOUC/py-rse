# Correctness {#py-rse-correct}

```{r py-rse-correct-setup, include=FALSE}
source(here::here("_common.R"))
```

We all hope the software we write does what we wrote it to do.
But how can we be sure?
The short is answer is that we can't be completely certain,
but that as in science,
we can test our expectations against reality to help us decide if we are sure enough.
This chapter explores ways of doing that.

## How would I manually test a function? {#py-rse-correct-manual}

Testing is about comparing what you "expect" a function outputs with the "actual" output.
Let's do a simple example to demonstrate by writing out a function that gets the sign of a number.

```{python manual-test}
def num_sign(x):
    if x > 0:
        out = 1
    else:
        out = -1
    return out
```

The simplest way to test the function would be to interactively check it produces the correct values.
Type out these commands to see that we get what we expect.

```{python manual-test-2}
num_sign(2) == 1
num_sign(0) == 0
num_sign(-4) == -1
```

These simple tests show that the function handles (some) positive and negatives value correctly,
but gives the wrong value for zero, which should be 0.

Manually testing functions is a great place to start,
but quickly becomes unmanageable as the function or tests get longer or more complicated.
And if we want to easily re-run multiple tests in one command? 
Then we need to start considering automated methods of testing.
The Python `assert` statement can achieve some of what we want.

Let's try it out.
Type out this new function...

```{python stop-not}
import math

def test_numSign():
    assert numSign(0.1) == 1
    assert numSign(0) == 0
    assert numSign(math.inf) == -1
```

Then execute it:

```{python stop-not-eval, error=TRUE}
test_numSign()
```

Do exercise \@ref(py-rse-correct-ex-numsign) on fixing `numSign()` so the tests pass.

## What does a testing framework have? {#py-rse-correct-features}

The individual tests in `test_numSign()` are called [unit tests][unit-test]
because they test small "units" or pieces of our code.
On it's own, this function is fine, but it misses two things:

1. If one test fails or throws an error, the subsequent tests don't get executed,
as we saw from running it.
In this case, we don't know if `numSign` handles `-Inf` correctly or not because
testing stops when `numSign(0)` gives the wrong answer.
2. If a test fails, it doesn't tell us *how many tests failed* and *what tests did fail*. 
In this case, the failure only tells us that *something failed* but doesn't say anything else. 
Another requirement for a good set of tests is [isolation][test-isolation].
For example, there are two functions `first` and `second` that are tested in this way:

```
first_result = first()
check(first_result)
second_result = second(first_result)
check(second_result)
```

If `first()` produces a wrong answer,
we won't know if `second()` is working properly 
because the `first()` test fails.
Using the output of one test as the input of another
increases the risk of [false positives][false-positive] (tests passing when they should fail)
and [false negatives][false-negative] (tests failing when they should pass).

Put together, a single unit test should have:

-   a [fixture][fixture],
    which is the thing being tested (e.g., the number 0 or a list of images);
-   an [actual result][actual-result],
    which is what the code produces when given the fixture; and
-   an [expected result][expected-result]
    that the actual result is compared to.

Each test can have one of three results:

-   [success][test-success]: the code passed the test.
-   [failure][test-failure]: the code didn't pass the test.
-   [error][test-error]: something went wrong with the test itself,
    so we don't know anything for certain about the code being tested.

A [test framework][test-framework] (also called a [test runner][test-runner]) should:

-   find and run tests;
-   summarize results;
-   pinpoint the locations of failures so that users know where to start debugging;
-   encourage people to isolate tests; and
-   make it easy to write and update tests (because otherwise people won't do it).

The next two sections will introduce the most popular framework for Python.
We will then explore general issues
such as how we can tell which parts of our code have and haven't been tested
and what kinds of tests we should write.

TODO: Exercise idea is identify the different parts of a unit test (fixture, etc)

## When should I write my tests? {#py-rse-correct-tdd}

Many programmers are passionate advocates of a practice called
[test-driven development][tdd] (TDD).
Rather than writing code and then writing tests,
they write the tests first and then write just enough code to make those tests pass.
Once the code is working,
they clean it up (Chapter \@ref(refactor)) and then move on to the next task.

TDD's advocates claim that working this way leads to better code in less time because:

1.  Writing tests clarifies what the code is actually supposed to do.

2.  It eliminates [confirmation bias][confirmation-bias].
    If someone has just written a function,
    they are predisposed to want it to be right,
    so they will bias their tests towards proving that it is correct
    instead of trying to uncover errors.

3.  Writing tests first ensures that they actually get written.

These arguments are plausible.
However,
studies such as @Fucc2016 don't support them:
in practice,
writing tests first or last doesn't appear to affect productivity.
What *does* have an impact is working in small, interleaved increments,
i.e.,
writing just a few lines of code and testing it before moving on
rather than writing several pages of code and then spending hours on testing.

So how do most data scientists figure out if their software is doing the right thing?
The answer is spot checks:
each time they produce an intermediate or final result,
they scan a table, create a chart, or inspect some summary statistics
to see if everything looks OK.
Their heuristics are usually easy to state,
like "there shouldn't be NAs at this point" or "the age range should be reasonable",
but applying those heuristics to a particular analysis always depends on
their evolving insight into the data in question.

By analogy with test-driven development,
we could call this process [checking-driven development][cdd] (CDD).
Each time we add a step to our pipeline and look at its output,
we can also add a check of some kind to the pipeline to ensure that
what we are checking for remains true as the pipeline evolves or is run on other data.
This helps reusability—it's amazing how often a one-off analysis
winds up being used many times—but the real goal is comprehensibility.
If someone can get our code and data,
then runs the code on the data,
and gets the same result that we did,
then our computation is reproducible,
but that doesn't mean they can understand it.
Comments help
(either in the code or as blocks of prose in a [computational notebook][computational-notebook]),
but they won't check that assumptions and invariants hold.
And unlike comments,
runnable assertions can't fall out of step with what the code is actually doing.

We also need to distinguish between testing during development
and testing in production.
During development,
our main concern is whether our answers are (close enough to) what we expect.
We do this by analyzing small datasets
and convincing ourselves that we're getting the right answer in some ad hoc way.

In production,
on the other hand,
our goal is to detect cases where behavior deviates significantly from what we previously decided was right.
We want this to be automated
so that our pipeline will inform us that something is wrong,
even if we're busy working on something else.
This can happen because real data will never have exactly the same characteristics as the data we used during development.
We also need these checks because the pipeline's environment can change:
for example,
a library we depend on could get upgraded,
which could lead to getting slightly different answers than we expected.

## How do I create and run unit tests? {#py-rse-correct-create-python}

[`pytest`][pytest] is the most widely used test framework for Python.
Tests obey three rules:

1.  All tests are put in files whose names begin with `test_`.
2.  Each test is a function whose name also begins with `test_`.
3.  These functions use `assert` to check results.

The `pytest` library comes with a command-line tool that is also called `pytest`.
When we run it with no options:

```shell
$ pytest
```

it searches for all files in the working directory and subdirectories named `test_*.py`.
It runs the tests in these files and then summarizes their results.
If you only want to run tests in a particular file,
use the command `pytest path/to/test_file.py`.

Tests can be saved anywhere in the project directory,
but usually they are placed in a `tests/` subdirectory.
If the project uses a [build tool][build-tool] like Make (Chapter \@ref(py-rse-automate)),
adding a target called `test` makes it easy to re-run tests by using `make test`.
So a project structure would look something like:

```
project
├── ...
├── tests
│   └── test_add.py
└── Makefile
```

Tests all have a similar structure. 
Let's demonstrate it by creating a file called `test_add.py` inside `tests/` 
and inserting the following code:

```python
def add(x, y):
    val = x + y
    return val

def test_add_two_small_integers_2():
    assert add_two(1, 2) == 3

def test_add_integer_and_float():
    left = 1
    right = 2.0
    result = add_two(left, right)
    assert result == 3
    assert type(result) == int
```

Generally we wouldn't include the function being tested in the tests file. 
Normally you'd import these functions into the test file.
For the demonstration, we've put the function `add()` in the test file.
The best practice for writing tests is to use descriptive names for test functions,
even if the function name will be long.
That's because `pytest` will print the names of the functions that fail testing.
There are two styles of testing presented here:

- one where the fixture is the value passed on to the function
and the result is checked immediately,
- and, one where the fixture is constructed step-by-step 
(i.e. the variables `left` and `right`)
and the function's result is set as a variable so multiple checks can be used.

Run these tests by using:

```shell
$ pytest test_add.py
```

The output is:

```text
============================= test session starts ==============================
platform darwin -- Python 3.7.3, pytest-5.0.1, py-1.8.0, pluggy-0.12.0
rootdir: /Users/merely/tests
plugins: openfiles-0.3.2, arraydiff-0.3, doctestplus-0.3.0, remotedata-0.3.1
collected 2 items

test_add.py FF                                                          [100%]

=================================== FAILURES ===================================
________________________ test_add_two_small_integers_2 _________________________

    def test_add_two_small_integers_2():
>       assert add_two(1, 2) == 3
E       NameError: name 'add_two' is not defined

test_add.py:6: NameError
__________________________ test_add_integer_and_float __________________________

    def test_add_integer_and_float():
        left = 1
        right = 2.0
>       result = add_two(left, right)
E       NameError: name 'add_two' is not defined

test_add.py:11: NameError
=========================== 2 failed in 0.10 seconds ===========================
```

The two `F`'s in the top line after `test_add.py` tell us there are two failures.
The `E` in the `FAILURES` section tells us why that test failed,
and the output immediately above this line tells us where the code failed.
In this example, we used the wrong function name. 
There is no function called `add_two()`, 
since the function we created is called `add()`.
Fix the name and re-run the tests. We'll get:

```text
============================= test session starts ==============================
platform darwin -- Python 3.7.3, pytest-5.0.1, py-1.8.0, pluggy-0.12.0
rootdir: /Users/merely/tests
plugins: openfiles-0.3.2, arraydiff-0.3, doctestplus-0.3.0, remotedata-0.3.1
collected 2 items

test_add.py .F                                                           [100%]

=================================== FAILURES ===================================
__________________________ test_add_integer_and_float __________________________

    def test_add_integer_and_float():
        left = 1
        right = 2.0
        result = add_two(left, right)
        assert result == 3
>       assert type(result) == int
E       AssertionError: assert <class 'float'> == int
E        +  where <class 'float'> = type(3.0)

test_add.py:13: AssertionError
====================== 1 failed, 1 passed in 0.05 seconds ======================
```

This time the first line of output tells us that one test passed (the `.`)
and another failed (the `F`).
`pytest` then shows where the problem occurred in the failing test:
we wrote that `result` should be an integer `int`,
but it is actually `float`.

Complete exercises \@ref(py-rse-correct-ex-purpose), 
\@ref(py-rse-correct-ex-create-countwords-test)
and \@ref(py-rse-correct-ex-pytest-makefile).

## How should fixtures be set up? {#py-rse-correct-fixtures}

Test files all have a similar structure:

1.  Load the software to be tested
    (since it normally won't be in the same file as the tests).
2.  Write one short function to test each feature of the software.

Each test function also has a stereotypical structure:

1.  Create the fixture.
2.  Run the test.
3.  Check the result.

Creating a fixture often requires many lines of code,
so we often write helper functions to do this.
Using helper functions to create fixtures like this
reduces the [cognitive load][cognitive-load] on whoever has to maintain the tests,
since they only have to form a [mental model][mental-model] of a few cases
rather than a new one for each test. 
Below is an example structure of how a Python helper function may look like:

TODO: Include example Python code of a helper function for fixtures.

```python
import ... # Load software/functions to test

def test_NAME():
    value = ... # Fixture
    assert function_to_test(value, 2) == expected # Test assertion
```

Note that we do *not* create the fixture once as a global variable
and then re-use it in many tests.
Tests should be isolated,
and having them share data increases the chances that
one of our tests will modify the data in ways that interfere with other tests.

## How should I check the results of tests? {#py-rse-correct-check}

Using the `assert` statement to test functions is useful as
it [raises][raise] an [exception][exception] when a condition is not true.
This normally causes Python to halt the program,
but `pytest` catches these exceptions,
adds them to its summary report,
and then runs the next test function.
Because of this, 
when all of the test functions are run,
they only run until the first failed `assert` (Figure \@ref(fig:py-rse-correct-failed-test)).

```{r py-rse-correct-failed-test, echo=FALSE, fig.cap="Test Failure"}
knitr::include_graphics("figures/FIXME.png")
```

This is one reason to re-use fixtures and keep test functions short.
Look at the code below. 
In this example,
we can't be sure that the second test runs in `combined_tests()`:

```python
def combined_tests():
    fixture = make_fixture()
    result = function(fixture)
    assert len(result) > 2
    assert result[0] != 0  # might not run
```

but we can be sure that both checks run in `split_test_len()` and `split_test_zero()`.

```python
def split_test_len():
    fixture = make_fixture()
    result = function(fixture)
    assert len(result) > 2

def split_test_zero():
    fixture = make_fixture()
    result = function(fixture)
    assert result[0] != 0
```

But in practice,
most programmers would write `combined_tests()` in the first case
because it is simpler and faster to write and read.
The purpose of testing is to draw attention to things that don't work as expected.
If the second check in `combined_tests()` doesn't run because the first one failed,
we may not have all the information that we could,
but we know one thing:
our code is broken.

## Why and how should I test that software fails correctly? {#py-rse-correct-failure}

Testing to check for correctness is great when functions fail "loudly",
but somtimes, there are [silent failures][silent-failure]:
something that goes wrong but doesn't crash,
print a warning,
or otherwise signal that human attention is required.
Testing that our code fails when it should
is therefore just as important as testing that it runs correctly.
The need to test error handling is not just folklore:
in their study of failures in data-intensive applications,
@Yuan2014 found that,
"the majority of catastrophic failures could easily have been prevented
by performing simple testing on error handling code—the last line of defense—even without
an understanding of the software design."

We test for an error by using a `with` statement and the `pytest.raises()` function.
For example,
for a function called `count_words()` is supposed to raise a `ValueError` exception when given an empty string.
We test this behavior like below:

```python
import pytest

def test_text_must_not_be_empty():
    with pytest.raises(ValueError):
        count_words('')
```

Add some more unit tests for `count_words()` by completing exercises \@ref(py-rse-correct-ex-count-words)
and \@ref(py-rse-correct-ex-check-errors).

## How do I write tests that have floating-point values? {#py-rse-correct-numeric}

Testing floating-point values is tricky because of the way computers represent these values. 
Different operating systems, hardware, and software have subtle differences in how these values are stored as numbers.
So, if we are testing a function that uses floating point numbers,
what do we compare its result to if computers represent the number differently?
If we try to test against a value with many decimal places (highly precise),
the test may fail because floating point values are represented as approximations.
No one has a good generic answer to this problem
because its root cause is that we're using approximations,
and each approximation has to be judged in its own context.

So what can you do to test your programs?
A generally good approach is to write a test that checks whether numbers are the same within some [tolerance][tolerance],
which is best expressed as a relative error.
The [relative error][relative-error] is the ratio of the absolute error to the value we're approximating.
The [absolute error][absolute-error] is the absolute value of
the difference between the approximation and the actual value.
So for example,
if we are off by 1 in approximating 8+1 and 56+1,
we have the same absolute error,
but the relative error is larger in the first case than it is in the second.

Relative error is almost always more important than absolute error when we are testing software.
It makes little sense to say that we're off by a hundredth
when the value is a billionth.
[Accuracy][accuracy] is how close our answer is to being right,
and [precision][precision] is how close repeated measurements are to each other.
We can be precise without being accurate (systematic bias),
or accurate without being precise (near the right answer but without many significant digits).
Accuracy is generally more important than precision for human decision making:
a relative error of 10^-2^ (two decimal places) is good enough for most data science
because the decision a human being would make won't change if the number changes by 1%.

In Python,
we can use `pytest.approx`,
which works on lists, sets, arrays, and other collections,
and can be given either relative or absolute error bounds.
To show how it works,
here's an example with an unrealistically tight absolute bound:

```{python}
from pytest import approx

for bound in (1e-15, 1e-16):
    vals = []
    for i in range(1, 10):
        number = 9.0 * 10.0 ** -i
        vals.append(number)
        total = sum(vals)
        expected = 1.0 - (10.0 ** -i)
        if total != approx(expected, abs=bound):
            print('{:22.21f} {:2d} {:22.21f} {:22.21f}'.format(bound, i, total, expected))
```

This tells us that two tests pass with an absolute error of 10^-15^
but fail when the bound is 10^-16^,
both of which are unreasonably tight.
Again,
it helps to think of physical experiments:
an absolute error of 10^-15^ is one part in a trillion,
which only a handful of high-precision experiments have ever achieved.

## How can I test plots and other graphical results? {#py-rse-correct-plots}

TODO: Update this section when I know more about how the plot scripts work.

Testing visualizations is hard:
any change to the dimension of the plot,
however small,
can change many pixels in a [raster image][raster-image],
and cosmetic changes such as moving the legend up a couple of pixels
will similarly cause false positives.

The simplest solution is therefore to test the data used to produce the image
rather than the image itself.
Unless we suspect that the plotting library contains bugs,
the correct data should always produce the correct plot.

If we *do* need to test the generated image,
the only practical approach is to compare it to a saved image that we have visually verified.
For example,
[pytest-mpl][pytest-mpl] does this by calculating the root mean square (RMS) difference between images,
which must be below a threshold for the comparison to pass.
It also allows us to turn off comparison of text,
because font differences can throw up spurious failures.
As with choosing tolerances for floating-point tests,
our rule for picking thresholds should be,
"If images are close enough that a human being would make the same decision about meaning,
the test should pass"

Another approach is to save the plot in a [vector format][vector-image] like [SVG][svg]
that stores the coordinates of lines and other elements as text.
We can then check that the right elements are there with the right properties.
However,
this is usually unrewarding:
again,
small changes to the library or to plotting parameters can make all of the tests fail
by moving elements by a pixel or two.
Vector-based tests therefore still need to have thresholds on floating-point values.

## What data should I use to test my analyses? {#py-rse-correct-data}

There aren't any useful general rules for testing the calculations that our pipelines do,
since those calculations vary from one pipeline to the next,
but there *are* some rules about where to get data to test with.
The first method is random [subsampling][subsampling]:
choose random subsets of your data,
analyze it,
and see how close the output is to what you get with the full dataset.
If the output doesn't converge as the sample size grows,
something is probably unstable in either the algorithm or its implementation.
It's important to randomly subset the data to reduce our risk of potential biases
inherent in the ordering of the data.

The other approach is to test with [synthetic data][synthetic-data].
It's possible to generate uniform data (i.e., data having the same values for all observations),
strongly bimodal data (which is handy for testing clustering algorithms),
or a pseudorandom sample with a known distribution with only a few lines of code.
If we do this,
we should give our pipeline data that *doesn't* fit your expected distribution
and make sure that our tests let us know about this unexpected distribution.
Doing this is the data science equivalent of testing the fire alarm every once in a while.

Let's write a short program to generate data that conforms to Zips' Law and then use it to test our analysis.
Since words either occur or don't,
the data will be integers and the distributions will be fractional.
We'll set an arbitrary threshold of 5% relative error to test for correctness.

```python
from pytest import approx

RELATIVE_ERROR = 0.05

def is_zipf(hist):
    scaled = [h/hist[0] for h in hist]
    print('scaled', scaled)
    perfect = [1/(1 + i) for i in range(len(hist))]
    print('perfect', perfect)
    return scaled == approx(perfect, rel=RELATIVE_ERROR)
```

Let's write up some tests to check if the function works on synthetic data:

```python
def test_fit_correct():
    actual = [round(100 / (1 + i)) for i in range(10)]
    print('actual', actual)
    assert is_zipf(actual)

def test_fit_first_too_small():
    actual = [round(100 / (1 + i)) for i in range(10)]
    actual[0] /= 2
    assert not is_zipf(actual)

def test_fit_last_too_large():
    actual = [round(100 / (1 + i)) for i in range(10)]
    actual[-1] = actual[1]
    assert not is_zipf(actual)
```

Now, try running the tests. Do they pass or fail? 
Complete exercise \@ref(py-rse-correct-ex-test-dataset)

## How can I test a data analysis pipeline in production? {#py-rse-correct-production}

An [operational test][operational-test] is one that is kept in place during production
to tell users if everything is still working as it should.
Some common operational tests include:

-   Does this pipeline stage produce the same number of output records as input records?
-   Or fewer if the stage is aggregating?
-   If two or more tables are being [joined][join],
    is the number of output records equal to the product of the number of input records?
-   Is the standard deviation smaller than the range of the data?
-   Are there any NaNs or NULLs where there aren't supposed to be?

A common pattern for such tests is to have every tool append information to a [log file][log-file]
and then have another tool check that log file after the run is over.
Logging while calculating and checking afterward makes it easy to compare values between pipeline stages,
and ensures that there's a record of why a problem was reported.

To illustrate these ideas,
let's write a script that reads a document and prints one line per word:

```python
import sys

num_lines = num_words = 0
for line in sys.stdin:
    num_lines += 1
    words = [strip_punctuation(w) for w in line.strip().split()]
    num_words += len(words)
    for w in words:
        print(w)

with open('logfile.csv', 'a') as logger:
    logger.write('text_to_words.py,num_lines,{}\n'.format(num_lines))
    logger.write('text_to_words.py,num_words,{}\n'.format(num_words))
```

Then, let's write another script script that counts how often words appear in its input:

```python
import sys

num_words = 0
count = {}
for word in sys.stdin:
    num_words += 1
    count[word] = count.get(word, 0) + 1

for word in count:
    print('{} {}', word, count[word])

with open('logfile.csv', 'a') as logger:
    logger.write('word_count.py,num_words,{}\n'.format(num_words))
    logger.write('word_count.py,num_distinct,{}\n'.format(len(count)))
```

Both of these scripts write records to `logfile.csv`.
Open up the file and look at the contents.
You should see something like this:

```text
text_to_words.py,num_lines,431
text_to_words.py,num_words,2554
word_count.py,num_words,2554
word_count.py,num_distinct,1167
```

Now, let's write a small program to check that everything went as planned:

```python
import sys
import csv

data = {}
for row in csv.reader(sys.stdin):
    data[(row[0], row[1])] = int(row[2])

assert data[('text_to_words.py', 'num_lines')] <= data['word_count.py', 'num_words')]
assert data[('text_to_words.py', 'num_words')] == data['word_count.py', 'num_words')]
assert data[('word_count.py', 'num_words')] >= data['word_count.py', 'num_distinct')]
```

Tests like these are simple to write
and catch a surprising number of errors,
particularly when the person using the pipeline isn't its original author.

## How can I infer and check properties of my data? {#py-rse-correct-infer}

Writing tests for the properties of data can be tedious,
but some of the work can be automated.
In particular,
the Python [TDDA library][tdda-site] for can infer test rules from data,
such as "`age` should be less than 100",
"`Date` should be sorted in ascending order",
or "`StartDate` should be less than or equal to `EndDate`".
The library comes with a command-line tool called `tdda`,
so that the command:

```shell
$ tdda discover training-data.csv properties.tdda
```

infers rules from data,
while the command:

```shell
$ tdda verify actual-data.csv properties.tdda
```

verifies data against those rules.
The inferred rules are stored as [JSON][json],
which is (sort of) readable with a bit of practice.
Reading the generated rules is a good way to get to know your data,
and modifying values
(e.g., changing the maximum allowed value for `Grade` from the observed 94.5 to the actual 100.0)
is an easy way to make constraints explicit.
For example,
if we run:

```shell
$ tdda discover elements92.csv elements.tdda
```

(which contains information about the first 92 elements)
the output includes:

```json
"fields": {
    "Name": {
        "type": "string",
        "min_length": 3,
        "max_length": 12,
        "max_nulls": 0,
        "no_duplicates": true
    },
    "Symbol": {
        "type": "string",
        "min_length": 1,
        "max_length": 2,
        "max_nulls": 0,
        "no_duplicates": true
    },
    "ChemicalSeries": {
        "type": "string",
        "min_length": 7,
        "max_length": 20,
        "max_nulls": 0,
        "allowed_values": [
            "Actinoid",
            "Alkali metal",
            "Alkaline earth metal",
            "Halogen",
            "Lanthanoid",
            "Metalloid",
            "Noble gas",
            "Nonmetal",
            "Poor metal",
            "Transition metal"
        ]
    },
    "AtomicWeight": {
        "type": "real",
        "min": 1.007947,
        "max": 238.028913,
        "sign": "positive",
        "max_nulls": 0
    },
    ...more...
}
```

We can apply these inferred rules to all elements up to number 118
using the `-7` option to get pure ASCII output
and the `-f` option to show only fields with failures:

```shell
$ tdda verify -f elements118.csv elements.tdda
```

```text
FIELDS:

Z: 1 failure  5 passes  type ✓  min ✓  max ✗  sign ✓  max_nulls ✓  no_duplicates ✓

Name: 1 failure  4 passes  type ✓  min_length ✓  max_length ✗  max_nulls ✓  no_duplicates ✓

Symbol: 1 failure  4 passes  type ✓  min_length ✓  max_length ✗  max_nulls ✓  no_duplicates ✓

AtomicWeight: 2 failures  3 passes  type ✓  min ✓  max ✗  sign ✓  max_nulls ✗

...other reports...

SUMMARY:

Constraints passing: 57
Constraints failing: 15
```

Another way to use TDDA is to generate constraints for two datasets and then look at differences
in order to see how similar the datasets are to each other.
This is especially useful if the constraint file is put under version control.

## How can I tell what code has been tested or not? {#py-rse-correct-coverage}

As we add to or change our code,
we may lose track of what has and hasn't been tested. 
Even in short programs,
loops and conditionals can make it difficult to figure out
what does or doesn't execute.
Take a few moments to read this function in Python:

```python
def first(left, right):
    if left < right:
        left, right = right, left
    while left > right:
        value = second(left, right)
        left, right = right, int(right/2)
    return value

def second(check, balance):
    if check > 0:
        return balance
    else:
        return 0

print(first(3, 5))
```

Can you easily tell what and how many lines gets executed?
While this is a simple example, 
even manually checking the code is difficult.
The good news is that there is a tool to do this automatically,
and it's called [code coverage][code-coverage].
Essentially,
a code coverage tool records whether a line of code gets evaluated or not
(Figure \@ref(fig:py-rse-correct-how-coverage-works)).
When the code coverage program finishes,
the tool tells us which lines were evaluated
and reports some summary statistics like the number or percentage of lines that weren't used.

```{r py-rse-correct-how-coverage-works, echo=FALSE, fig.cap="How Coverage Works"}
knitr::include_graphics("figures/FIXME.png")
```

Most Python programmers use the `coverage` module as a coverage checking tool.
When we install it we get a command-line utility that is also called `coverage`.
If the example program above is in a file called `demo.py`,
we can check coverage by running:

```shell
$ coverage run demo.py
```

This command doesn't display anything of its own;
instead,
it puts coverage data in a file called `.coverage` (with a leading `.`) in the current directory.
To display that data,
we run:

```shell
$ coverage report
```

```text
Name      Stmts   Miss  Cover
-----------------------------
demo.py      12      1    92%
```

To get a more complete report,
run `coverage html`
and open `htmlcov/index.html`.
Clicking on the name of our file produces the colorized line-by-line display shown in Figure \@ref(fig:python-coverage).

```{r python-coverage, echo=FALSE, fig.cap="Python Coverage"}
knitr::include_graphics("figures/rse-correct/python-coverage.png")
```

Complete exercise \@ref(py-rse-correct-ex-coverage).

### How much test coverage is enough? {#py-rse-correct-enough}

How testing we do of our software depends on the purposes it will be used for.
If we are writing software for a safety-critical application such as a medical device,
we should aim for 100% code coverage,
i.e.,
every single line in the application should be tested.
In fact,
we should probably go further and aim for 100% [path coverage][path-coverage]
and ensure that every possible path through the code has been checked.

But most of us don't write software that people's lives depend on,
so requiring 100% code coverage is like asking for ten decimal places of accuracy
when checking the voltage of a household electrical outlet.
We always need to balance the effort required to create tests
against the likelihood that those tests will uncover useful information.

It's important to understand that no amount of testing can prove a piece of software is completely correct.
A function with only two numeric arguments has 2^128^ possible inputs.
Even if we could write the tests,
how could we be sure we were checking the result of each one correctly?

Testing data analysis pipelines is often harder than testing mainstream software applications,
since data analysts often don't know what the right answer is.
(If we did,
we would have submitted our report and moved on to the next problem already.)
The key distinction is the difference between [validation][validation],
which asks whether the specification is correct,
and [verification][verification],
which asks whether we have met that specification.
The difference between them is the difference between building the right thing and building something right,
and the first is often hard for data scientists to address.

Luckily,
we can group the test cases for most functions into classes.
For example,
it might be possible to test a function that takes numbers as inputs
as well as needing to use only a few cases:
zero, a positive number, a negative one, and infinity.
If we want to go further,
we could check that it fails the right way when given a string or a list.
Similarly,
when testing a function that summarizes a table full of data,
we probably should check that it handles tables with:

-   no rows
-   only one row
-   many identical rows
-   rows having keys that are supposed to be unique, but aren't
-   rows that contain nothing but missing values

Some projects develop [checklists][checklist] to remind programmers what they ought to test.
These checklists can be a bit daunting for newcomers,
but they are a great way to pass on hard-earned experience.

## Summary {#py-rse-correct-summary}

```{r py-rse-correct-concept, echo=FALSE, fig.cap="Correctness Concept Map"}
if (knitr::is_latex_output()) {
  knitr::include_graphics("figures/rse-correct/concept.pdf")
} else {
  knitr::include_graphics("figures/rse-correct/concept.svg")
}
```

## Exercises {#py-rse-correct-exercises}

### Fixing `numSign()` {#py-rse-correct-ex-numsign}

1.  Fix the implementation of `numSign()` so that all tests will succeed.
2.  Re-run the tests to confirm that the function is working.
3.  Extend the function so that it does something sensible when passed a string or an empty list.
    Write functions to check your changes.

### Explain the purpose of tests {#py-rse-correct-ex-purpose}

1. Based on the contents of the tests below,
try to write an informative and descriptive test name.

TODO: Convert to Python code.

```python
def add(x, y):
    val = x + y
    return val

def test____()
    assert add_two(1, 2) == 3
```

### Create a first, simple unit test for `countwords.py` {#py-rse-correct-ex-create-countwords-test}

1. If not create, create a `tests/` directory in the Zipf's Law project.
2. Create a `test_countwords.py` file for the `countwords.py` script in the `tests/` directory.
TODO: This needs to be fixed so it fits the testing framework (i.e. how do you test a script?)
3. Add a simple test in the file to check that it can count at least one word.
4. Run `pytest` in the project directory. Does it work? What is the output?

### Include the Python testing command in the project Makefile {#py-rse-correct-ex-pytest-makefile}

1. Open up the project Makefile and create a new recipe for `pytest` to run the unit tests.
The new recipe should be called `test`.
2. Run `make test` in the project directory and see how it works. 
If it doesn't work, fix it until it gives the testing output.

### Add more tests for `count_words()` {#py-rse-correct-ex-count-words}

TODO: This currently doesn't fit with how countwords.py is structured.. it is a script, how to test it as a function?

1. In the `tests/test_countwords.py` file, add some additional unit tests for `count_words()` that checks the following:
    -   Returns zero when given an empty string.
    -   Returns `NA` when given `NA`.
    -   Returns 2 when given two words separated by a space.
    -   Returns 2 when given two words separated by a dash `-`.
1. Run `make test` to run the new tests. What does the output give you?

### Checking that errors occur for `count_words()` {#py-rse-correct-ex-check-errors}

1. Add new unit tests for the word-counting function
to check that it raises errors when it should.
1. Run `make test` to run these new tests.

### Generate a dataset of text to test on {#py-rse-correct-ex-test-dataset}

1. Create a new test file called `test_textdata.py` in the `tests/` directory.
1. Write code to generate random text and words.
Use the below code as a template:
1. Run `make test` to check if this new test works.

TODO: Add a template that they can base off of? Some with more Python experience deal with this.

```python
def random_words():
    ...
```

### Check coverage of project and add to Makefile {#py-rse-correct-ex-coverage}

1. Run coverage using this command `coverage run countwords.py BOOKTEXT` and 
then run `coverage report`. 
What does the report say?
1. Add the coverage command as a recipe in the Makefile.
Call the recipe `coverage`.
1. Run `make coverage` to check that it works.

## Key Points {#py-rse-correct-keypoints}

```{r, child="keypoints/rse-correct.md"}
```
