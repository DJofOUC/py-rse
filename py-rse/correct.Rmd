# Correctness {#py-rse-correct}

TODO: Add Git commits for exercises throughout

```{r py-rse-correct-setup, include=FALSE}
source(here::here("_common.R"))
```

Now that we've written our software for counting and analyzing the words in classic texts,
how can we be sure that it’s producing reliable results?
The short is answer is that we can't be completely certain,
but that as in science,
we can test our expectations against reality to help us decide if we are sure enough.
In this chapter we will explore the approaches available for testing code:
assertions, unit tests, regresson tests and integration tests.

Our Zipf's Law project files are structured as they were at the end of the previous chapter:

```text
├── CONDUCT.md
├── LICENSE.md
├── Makefile
├── README.md
├── bin
│   ├── book_summary.sh
│   ├── collate.py
│   ├── countwords.py
│   ├── mymodule.py
│   ├── plotcounts.py
│   └── rcparams.yml
├── data
│   ├── README.md
│   ├── dracula.txt
│   ├── risk.txt
│   └── ...
└── results
    ├── dracula.csv
    ├── dracula.png
    └── ...
```

## Assertions {#py-rse-assertions}

The first step toward getting the right answers from our programs
is to assume that mistakes will happen and to guard against them.
This is called defensive programming,
and the most common way to do it is to add assertions to our code
so that it checks itself as it runs.
An assertion is simply a statement that something must be true at a certain point in a program.
When Python sees one, it evaluates the assertion’s condition.
If it’s true, Python does nothing, but if it’s false,
Python halts the program immediately and prints the error message if one is provided.

To demonstrate an assertion in action,
consider this piece of code that halts as soon as the loop encounters
a rainfall observation that isn’t positive:

```python
rainfall_observations = [1.5, 2.3, 0.7, -0.2, 4.4]
total = 0.0
for ob in rainfall_observations:
    assert ob >= 0.0, 'Rainfall observations should only contain positive values'
    total += ob
print('total rainfall is:', total)
```

```text
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-19-33d87ea29ae4> in <module>()
      2 total = 0.0
      3 for ob in rainfall_obs:
----> 4     assert ob > 0.0, 'Rainfall observations should only contain positive values'
      5     total += ob
      6 print('total rainfall is:', total)

AssertionError: Rainfall observations should only contain positive values
```

Programs like the Firefox browser are full of assertions:
10-20% of the code they contain are there to check that the other 80-90% are working correctly.


## Unit testing {#py-rse-unit-testing}

As the name suggests,
unit tests relate to small "units" or pieces of our code.
This typically means functions that we've defined.
What is considered to be the smallest code unit is subjective --
the body of a function can be long or short,
and shorter functions are arguably more unit-like than long ones --
but a good guideline is that if the code cannot be made any simpler
logically (you cannot split apart the addition operator) or 
practically (a function is self-contained and well defined),
then it is a unit.

In the case of our Zipf's Law software,
the `count_words` function in the `wordcounts.py` script is a good example of a code unit:

```
def count_words(reader):
    """Count the occurrence of each word in a string."""
    text = reader.read()
    findwords = re.compile(r"\w+", re.IGNORECASE)
    word_list = re.findall(findwords, text)
    word_counts = Counter(word_list)
    return word_counts
```

A single unit test will typically have:

-   a [fixture][fixture],
    which is the thing being tested (e.g., an array of numbers);
-   an [actual result][actual-result],
    which is what the code produces when given the fixture; and
-   an [expected result][expected-result]
    that the actual result is compared to.

In order to write a unit test for the `count_words` function,
an appropriate fixture would be a body of text for which we know the frequency of each word.
The poem Risk by Anais Nin is a good choice,

```shell
$ cat ../data/risk.txt
```

```text
And then the day came,
when the risk
to remain tight
in a bud
was more painful
than the risk
it took
to blossom.
```

because it's short and we can count the words by hand in order to create
an expected result variable:

```python
from collections import Counter

risk_poem_counts = {'the': 3, 'risk': 2, 'to': 2, 'And': 1, 'then': 1,
                    'day': 1, 'came': 1, 'when': 1, 'remain': 1, 'tight': 1,
                    'in': 1, 'a': 1, 'bud': 1, 'was': 1, 'more': 1,
                    'painful': 1, 'than': 1, 'it': 1, 'took': 1, 'blossom': 1}
expected_result = Counter(risk_poem_counts)
```

We can generate the actual result by calling the `word_counts` function,
and then use an assertion to check that the actual and expect results
are the same:

```python
import countwords

with open('../data/risk.txt', 'r') as reader:
    actual_result = countwords.count_words(reader)
assert actual_result == expected_result
```

There's no output,
which means the test has passed
(recall that assertions only do something if the condition evaluates to false).

### Testing frameworks

Writing a single unit test for our Zipf's Law software is a good start,
but to be sure about the reliability of our code we'll probably want to write more unit tests.
To coordinate the running of many tests,
we can use a [test framework][test-framework] (also called a [test runner][test-runner]).
The most widely used test framework for Python is [`pytest`][pytest],
for which tests obey three rules:

1.  All tests are put in files whose names begin with `test_`.
2.  Each test is a function whose name also begins with `test_`.
3.  These functions use `assert` to check results.

Following those rules,
we can create a `test_zipfs.py` script that contains the test we just developed:

```python
from collections import Counter
import countwords

def test_word_count():
    """Test the counting of words.
    
    The example poem is Risk, by Anais Nin.
    """
    risk_poem_counts = {'the': 3, 'risk': 2, 'to': 2, 'And': 1, 'then': 1,
                        'day': 1, 'came': 1, 'when': 1, 'remain': 1, 'tight': 1,
                        'in': 1, 'a': 1, 'bud': 1, 'was': 1, 'more': 1,
                        'painful': 1, 'than': 1, 'it': 1, 'took': 1, 'blossom': 1}
    expected_result = Counter(risk_poem_counts)
    with open('risk.txt', 'r') as reader:
        actual_result = countwords.count_words(reader)
    assert actual_result == expected_result
```

Tests can be saved anywhere in the project directory,
but usually they are placed in a `tests/` subdirectory.

The `pytest` library comes with a command-line tool that is also called `pytest`.
When we run it with no options,
it searches for all files in the working directory and subdirectories named `test_*.py`.
It runs the tests in these files and then summarizes their results.
(If you only want to run tests in a particular file,
use the command `pytest path/to/test_file.py`.)

```shell
$ pytest
```
```text
============================= test session starts ==============================
platform darwin -- Python 3.7.6, pytest-5.4.1, py-1.8.1, pluggy-0.12.0
rootdir: /Users/z3526123/Documents/volunteer/merely-useful.github.io/zipfs-law/bin
collected 1 item                                                               

test_zipfs.py .                                                          [100%]

============================== 1 passed in 0.02s ===============================
```

In order to add to our suite of unit tests,
we can now simply add more `test_` functions to `test_zipfs.py`.
For instance,
besides the counting of words (which we've already tested),
the other critical part of our code is the calculation of the \(\alpha\) parameter.
Earlier we defined a power law relating \(\alpha\)
to the word frequency \(f\), word rank \(r\) 
and a constant \(c\) of proportionality,\[
r = cf^{\frac{-1}{\alpha}}
\]
and noted that Zipf's Law holds exactly when \(\alpha\) is equal to one.
Setting \(\alpha\) to one and re-arranging the power law gives,\[
c = f/r
\], which we can use to generate synthetic word counts data 
(i.e. our test fixture) with a constant of proportionality
set to a hypothetical maximum word frequency of 600
(and thus \(r\) ranges from 1 to 600):

```python
import numpy as np

max_freq = 600
word_counts = np.floor(max_freq / np.arange(1, max_freq + 1)) 
```

The `np.floor` function rounds down to the nearest whole number,
because you can't have fractional word counts.

Passing this test fixture to the `get_power_law_params` function from our
`plotcounts.py` script,

```python
def get_power_law_params(word_counts):
    """
    Get the power law parameters.
    References
    ----------
    Moreno-Sanchez et al (2016) define alpha (Eq. 1),
      beta (Eq. 2) and the maximum likelihood estimation (mle)
      of beta (Eq. 6).
    Moreno-Sanchez I, Font-Clos F, Corral A (2016)
      Large-Scale Analysis of Zipf’s Law in English Texts.
      PLoS ONE 11(1): e0147073.
      https://doi.org/10.1371/journal.pone.0147073
    """
    mle = minimize_scalar(nlog_likelihood, bracket=(1 + 1e-10, 4),
                          args=(word_counts), method='brent')
    beta = mle.x
    alpha = 1 / (beta - 1)
    return alpha
```
should return an expected value of 1.0.

To test this, we can add a second test to `test_zipfs.py`,

```python
import numpy as np
from collections import Counter

import plotcounts
import countwords

def test_alpha():
    """Test the calculation of the alpha parameter.
    
    The test word counts satisfy the relationship,
      r = cf**(-1/alpha), where
      r is the rank,
      f the word count, and
      c is a constant of proportionality.

    To generate test word counts for an expected alpha value of 1.0,
      a maximum word frequency of 600 is used
      (i.e. c = 600 and r ranges from 1 to 600)
    """    
    max_freq = 600
    word_counts = np.floor(max_freq / np.arange(1, max_freq + 1))
    actual_alpha = plotcounts.get_power_law_params(word_counts)
    expected_alpha = 1.0
    assert actual_alpha == expected_alpha

def test_word_count():
    """Test the counting of words.
    
    The example poem is Risk, by Anais Nin.
    """
    risk_poem_counts = {'the': 3, 'risk': 2, 'to': 2, 'And': 1, 'then': 1,
                        'day': 1, 'came': 1, 'when': 1, 'remain': 1, 'tight': 1,
                        'in': 1, 'a': 1, 'bud': 1, 'was': 1, 'more': 1,
                        'painful': 1, 'than': 1, 'it': 1, 'took': 1, 'blossom': 1}
    expected_result = Counter(risk_poem_counts)
    with open('../data/risk.txt', 'r') as reader:
        actual_result = countwords.count_words(reader)
    assert actual_result == expected_result
```

and then re-run pytest:

```shell
$ pytest
```

```text
============================= test session starts ==============================
platform darwin -- Python 3.7.6, pytest-5.4.1, py-1.8.1, pluggy-0.12.0
rootdir: /Users/z3526123/Documents/volunteer/merely-useful.github.io/zipfs-law/bin
collected 2 items                                                              

test_zipfs.py F.                                                         [100%]

=================================== FAILURES ===================================
__________________________________ test_alpha __________________________________

    def test_alpha():
        """Test the calculation of the alpha parameter.
    
        The test word counts satisfy the relationship,
          r = cf**(-1/alpha), where
          r is the rank,
          f the word count, and
          c is a constant of proportionality.
    
        To generate test word counts for an expected alpha value of 1.0,
          a maximum word frequency of 600 is used
          (i.e. c = 600 and r ranges from 1 to 600)
        """
        max_freq = 600
        word_counts = np.floor(max_freq / np.arange(1, max_freq + 1))
        actual_alpha = plotcounts.get_power_law_params(word_counts)
        expected_alpha = 1.0
>       assert actual_alpha == expected_alpha
E       assert 0.9951524579316625 == 1.0

test_zipfs.py:24: AssertionError
=========================== short test summary info ============================
FAILED test_zipfs.py::test_alpha - assert 0.9951524579316625 == 1.0
========================= 1 failed, 1 passed in 0.85s ==========================
```

The output tells us that one test failed but the other test passed.
This is a nice feature of test runners like pytest -- they continue on and complete all the tests
rather than stopping at the first assertion failure like a regular Python script would.
Looking more closely at the output, we can see that although `test_alpha` failed,
the `actual_alpha` value of 0.9951524579316625 was awfully close to the expected value of 1.0.
In fact,
we don't expect the estimation routine used by `get_power_law_params` to be perfect.
In order to edit `test_alpha` so that "close enough is good enough"
we can borrow a `pytest` function used for equating floating point values...  

### Testing and floating-point values? {#py-rse-correct-numeric}

Testing floating-point values is tricky because of the way computers represent these values.
Different operating systems, hardware,
and software have subtle differences in how these values are stored as numbers.
So, if we are testing a function that uses floating point numbers,
what do we compare its result to if computers represent the number differently?
If we try to test against a value with many decimal places (highly precise),
the test may fail because floating point values are represented as approximations.
No one has a good generic answer to this problem
because its root cause is that we're using approximations,
and each approximation has to be judged in its own context.

So what can you do to test your programs?
A good approach is to write a test that checks whether numbers are the same within some [tolerance][tolerance],
which is best expressed as a relative or absolute error.
The [absolute error][absolute-error] is the absolute value of
the difference between the approximation and the actual value.
The [relative error][relative-error] is the ratio of the absolute error to the value we're approximating.
For example,
if we are off by 1 in approximating 8+1 and 56+1,
we have the same absolute error,
but the relative error is larger in the first case than it is in the second.

For our `test_alpha` function,
we might decide that an absolute error of 0.01 is acceptable.
Using the pytest library,
we can define this tolerance level using `pytest.approx`:

```python
import pytest
import numpy as np
from collections import Counter

import plotcounts
import countwords

def test_alpha():
    """Test the calculation of the alpha parameter.
    
    The test word counts satisfy the relationship,
      r = cf**(-1/alpha), where
      r is the rank,
      f the word count, and
      c is a constant of proportionality.

    To generate test word counts for an expected alpha value of 1.0,
      a maximum word frequency of 600 is used
      (i.e. c = 600 and r ranges from 1 to 600)
    """    
    max_freq = 600
    word_counts = np.floor(max_freq / np.arange(1, max_freq + 1))
    actual_alpha = plotcounts.get_power_law_params(word_counts)
    expected_alpha = pytest.approx(1.0, abs=0.01)
    assert actual_alpha == expected_alpha

def test_word_count():
    """Test the counting of words.
    
    The example poem is Risk, by Anais Nin.
    """
    risk_poem_counts = {'the': 3, 'risk': 2, 'to': 2, 'And': 1, 'then': 1,
                        'day': 1, 'came': 1, 'when': 1, 'remain': 1, 'tight': 1,
                        'in': 1, 'a': 1, 'bud': 1, 'was': 1, 'more': 1,
                        'painful': 1, 'than': 1, 'it': 1, 'took': 1, 'blossom': 1}
    expected_result = Counter(risk_poem_counts)
    with open('../data/risk.txt', 'r') as reader:
        actual_result = countwords.count_words(reader)
    assert actual_result == expected_result
```

Re-running pytest, all tests now pass:

```shell
$ pytest
```

```text
============================= test session starts ==============================
platform darwin -- Python 3.7.6, pytest-5.4.1, py-1.8.1, pluggy-0.12.0
rootdir: /Users/z3526123/Documents/volunteer/merely-useful.github.io/zipfs-law/bin
collected 2 items                                                              

test_zipfs.py ..                                                         [100%]

============================== 2 passed in 0.69s ===============================
```

## Regression testing












## Exercises {#py-rse-correct-exercises}

### Fixing `num_sign()` {#py-rse-correct-ex-numsign}

1.  Fix the implementation of `num_sign()` so that all tests will succeed.
2.  Re-run the tests to confirm that the function is working.
3.  Extend the function so that it does something sensible when passed a string or an empty list.
    Write functions to check your changes.

### Explain the purpose of tests {#py-rse-correct-ex-purpose}

1. Based on the contents of the tests below,
try to write an informative and descriptive test name.

TODO: Convert to Python code.

```python
def add(x, y):
    val = x + y
    return val

def test____()
    assert add_two(1, 2) == 3
```

### Create a first, simple unit test for `countwords.py` {#py-rse-correct-ex-create-countwords-test}

1. If not create, create a `tests/` directory in the Zipf's Law project.
2. Create a `test_countwords.py` file for the `countwords.py` script in the `tests/` directory.
TODO: This needs to be fixed so it fits the testing framework (i.e. how do you test a script?)
3. Add a simple test in the file to check that it can count at least one word.
4. Run `pytest` in the project directory. Does it work? What is the output?

### Include the Python testing command in the project Makefile {#py-rse-correct-ex-pytest-makefile}

1. Open up the project Makefile and create a new recipe for `pytest` to run the unit tests.
The new recipe should be called `test`.
2. Run `make test` in the project directory and see how it works.
If it doesn't work, fix it until it gives the testing output.

### Add more tests for `count_words()` {#py-rse-correct-ex-count-words}

TODO: This currently doesn't fit with how countwords.py is structured.. it is a script, how to test it as a function?

1. In the `tests/test_countwords.py` file, add some additional unit tests for `count_words()` that checks the following:
    -   Returns zero when given an empty string.
    -   Returns `NA` when given `NA`.
    -   Returns 2 when given two words separated by a space.
    -   Returns 2 when given two words separated by a dash `-`.
1. Run `make test` to run the new tests. What does the output give you?

### Checking that errors occur for `count_words()` {#py-rse-correct-ex-check-errors}

1. Add new unit tests for the word-counting function
to check that it raises errors when it should.
1. Run `make test` to run these new tests.

### Generate a dataset of text to test on {#py-rse-correct-ex-test-dataset}

1. Create a new test file called `test_textdata.py` in the `tests/` directory.
1. Write code to generate random text and words.
Use the below code as a template:
1. Run `make test` to check if this new test works.

TODO: Add a template that they can base off of? Some with more Python experience deal with this.

```python
def random_words():
    ...
```

### Check coverage of project and add to Makefile {#py-rse-correct-ex-coverage}

1. Run coverage using this command `coverage run countwords.py BOOKTEXT` and
then run `coverage report`.
What does the report say?
1. Add the coverage command as a recipe in the Makefile.
Call the recipe `coverage`.
1. Run `make coverage` to check that it works.

## Key Points {#py-rse-correct-keypoints}

```{r, child="keypoints/rse-correct.md"}
```

```{r, child="./links.md"}
```
