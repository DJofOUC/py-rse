# Publishing {#publish}

```{r publish-setup, include=FALSE}
source(here::here("_common.R"))
```

This final chapter explores how we could publish a report on our Zipf's Law analysis.
We use the term "report" to include research papers,
summaries for clients,
or anything else that is shorter than a book
and aimed at people other than its creators.

The first and most important point to make
is that modern publishing involves much more than producing a printable PDF.
It also entails providing the data underpinning the report
as well as the code used to do the analysis:

> An article about computational science in a scientific publication
> is *not* the scholarship itself,
> it is merely *advertising* of the scholarship.
> The actual scholarship is the complete software development environment
> and the complete set of instructions which generated the figures.
>
> --- Jonathan Buckheit and David Donoho, paraphrasing Jon Claerbout, in @Buck1995

Depending on the analysis performed,
the "complete set of instructions" relating to software and code
may range from scripts or notebooks written solely for the purpose of the report
to packages that are developed and released for use by a wider audience.
While some reports, datasets, software packages and/or analysis scripts
can't be published without violating personal or commercial confidentiality,
every researcher's default should be to make all these components
of their work as widely available as possible.
Publishing it under an open license (Section \@ref(teams-license)) is the first step;
the sections below describe some of the others.

## Identifying Reports and Authors {#publish-identifiers}

Before publishing anything,
we need to understand how authors and their works are identified.
A [Digital Object Identifier][doi] (DOI)
is a unique identifier for a particular version of a particular digital artifact
such as a report, a dataset, or a piece of software.
DOIs are written as `doi:prefix/suffix`,
but are often also represented as URLs like `http://dx.doi.org/prefix/suffix`.
In order to be allowed to issue a DOI,
an academic journal, data archive, or other organiation
must guarantee a certain level of security, longevity and access.

An [ORCID][orcid] is an Open Researcher and Contributor ID.
Anyone can get an ORCID for free,
and should include it in publications
because people's names and affiliations change over time.

## Where to Publish {#publish-where}

The best option for publishing a report depends on the context.
Numerous open access journals support academic, peer-reviewed research papers,
and many formerly closed-access journals also now offer an open access option
(though they sometimes charge an additional fee).

Another option is to publish with an online pre-print server
such as [arXiv][arxiv] (which started it all) or [bioRxiv][biorxiv].
A preprint is a version of an academic research paper that precedes formal peer review
and publication in a peer-reviewed journal.
Authros can make preprints available before and/or after a paper is published in a journal.

Finally,
there are online platforms such as [Overleaf][overleaf] and [Authorea][authorea].
These allow a report to be openly viewable on the web throughout the entire writing process.
Once work is complete,
these sites can issue a DOI themselves
or the text can be exported for submission to an academic journal.
For everything other than reports,
online platforms such as [Figshare][figshare] and [Zenodo][zenodo]
can issue DOIs as well.
People frequently upload datasets and other items to these platforms
so that they can be easily accessed by others.

Small datasets (i.e., anything under 500 MB) can be stored in version control
using the conventions described in Chapter \@ref(project).
If the data is being used in several projects,
it may make sense to create one repository to hold only the data;
the R community refers to these as [data packages][data-package],
and they are often accompanied by small scripts to clean up and query the data.

For medium-sized datasets (between 500 MB and 5 GB),
it's better to put the data on platforms
like the [Open Science Framework][osf], [Dryad][dryad], and [Figshare][figshare].
Each of these will give the datasets identifiers;
those identifiers should be included in reports
along with scripts to download the data.
Big datasets (i.e., anything more than 5 GB)
may not be ours in the first place,
and probably need the attention of a professional archivist.
Any processed or intermediate data that takes a long time to regenerate
should probably be published as well using these same sizing rules;
all of this data should be given identifiers,
and those identifiers should be included in reports.

> **Data Journals**
>
> While archiving data at a site like Dryad or Figshare (following the FAIR Principles)
> is usually the end of the data publishing process,
> there is the option of publishing a journal paper to describe the dataset in detail.
> Some research disciplines have journals devoted
> to describing particular types of data
> (e.g., the [Geoscience Data Journal][geoscience-data-journal])
> and there are also generic data journals
> (e.g., [Scientific Data][scientific-data]).

## Publishing Data {#publish-data}

The first step in publishing the data associated with a report
is to determine what (if anything) needs to be published.
If the report involved the analysis of a publicly available dataset
that is maintained and documented by a third party,
the report simply needs to document where to access the data
and what version was analyzed:
it's not necessary to publish a duplicate of the dataset.

It's not strictly necessary to publish intermediate data
produced during the analysis of a publicly available dataset either,
so long as readers have access to the original data and the software used to process it.
However,
making intermediate data available can save people time and effort,
particularly if it takes a lot of computing power to reproduce it.
For example,
NASA has published
the [Goddard Institute for Space Studies Surface Temperature Analysis][gistemp],
which estimates the global average surface temperature
based on land and ocean weather observations,
because a simple metric of global warming is expensive to produce
and useful in many research contexts.

If a report involves a new dataset,
such as observations collected during a field experiment,
following a few simple rules can make it much more usable:

1.  Always use [tidy data][tidy-data].
2.  Include keywords describing the data in the project's `README.md`
    so that they appear on its home page and can easily be found by search engines.
3.  Give the data a unique identifier (Section \@ref(publish-identifiers)).
4.  Put the data in an open repository like [Figshare][figshare] or [Zenodo][zenodo].
5.  Use well-known formats like CSV and HDF5.
6.  Include an explicit license in every project and every dataset.
7.  Include units and other metadata.

The last point is often the hardest for people to implement,
since many researchers have never seen a properly-documented dataset.
We draw inspiration from the data catalog included in [the repository][womens-pockets-data] for @Dieh2018
and include a file `./data/README.md` in every project
that looks like this:

```
-   Infants born to women with HIV receiving an HIV test within two months of birth, 2009-2017
    -   `Infant_HIV_Testing_2017.xlsx`
        -   What is this?: Excel spreadsheet with summarized data.
        -   Source(s): UNICEF, <https://data.unicef.org/resources/dataset/hiv-aids-statistical-tables/>
        -   Last Modified: July 2018 (according to website)
        -   Contact: Greg Wilson <greg.wilson@rstudio.com>
        -   Spatial Applicability: global
        -   Temporal Applicability: 2009-2017
    -   `infant_hiv.csv`
        -   What is this?: CSV export from `Infant_HIV_Testing_2017.xlsx`
    -   Notes
        -   Data is not tidy: some rows are descriptive comments,
            others are blank separators between sections, and column headers are inconsistent.
        -   Use `tidy_infant_hiv()` to tidy this data.
-   Maternal health indicators disaggregated by age
    -   `maternal_health_adolescents_indicators_April-2016_250d599.xlsx`
        -   What is this?: Excel spreadsheet with summarized data.
        -   Source(s): UNICEF, <https://data.unicef.org/resources/dataset/maternal-health-data/>
        -   Last Modified: July 2018 (according to website)
        -   Contact: Greg Wilson <greg.wilson@rstudio.com>
        -   Spatial Applicability: global
        -   Temporal Applicability: 2000-2014
    -   `at_health_facilities.csv`
        -   What is this?: percentage of births at health facilities by country, year, and mother's age
        -   Source(s): single sheet from `maternal_health_adolescents_indicators_April-2016_250d599.xlsx`
    -   `c_sections.csv`
        -   What is this?: percentage of Caesarean sections by country, year, and mother's age
        -   Source(s): single sheet from `maternal_health_adolescents_indicators_April-2016_250d599.xlsx`
    -   `skilled_attendant_at_birth.csv`
        -   What is this?: percentage of births with skilled attendant present by country, year, and mother's age
        -   Source(s): single sheet from `maternal_health_adolescents_indicators_April-2016_250d599.xlsx`
    -   Notes
        -   Data is not tidy:
            some rows are descriptive comments,
            others are blank separators between sections,
            and column headers are inconsistent.
        -   Use `tidy_maternal_health_adolescents()` to tidy this data.
```

The catalog above doesn't include column headers or units
because the raw data isn't tidy.
It *does* include the names of the functions used to reformat that data,
and `./results/README.md` then includes the information that users will want.
One section of that file is shown below:

```
-   Infants born to women with HIV receiving an HIV test within two months of birth, 2009-2017
    -   infant_hiv.csv
      -   What is this?: tidied version of CSV export from spreadsheet.
      -   Source(s): UNICEF, <https://data.unicef.org/resources/dataset/hiv-aids-statistical-tables/>
      -   Last Modified: September 2018
      -   Contact: Greg Wilson <greg.wilson@rstudio.com>
      -   Spatial Applicability: global
      -   Temporal Applicability: 2009-2017
      -   Generated By: scripts/tidy-24.R

| Header   | Datatype | NA    | Description                                 |
|----------|----------|-------|---------------------------------------------|
| country  | char     | false | ISO3 country code of country reporting data |
| year     | integer  | false | year CE for which data reported             |
| estimate | double   | true  | estimated percentage of measurement         |
| hi       | double   | true  | high end of range                           |
| lo       | double   | true  | low end of range                            |
```

Note that this catalog includes both units
and whether or not a field can have missing values.

## The FAIR Principles {#publish-fair}

The [FAIR Principles][go-fair] describe what research data should look like.
They are still aspirational for most researchers @Broc2019,
but tell us what to aim for.
The most immediately important elements of the FAIR Principles are outlined below.

### Data should be *findable*.

The first step in using or re-using data is to find it.
We can tell we've done this if:

1.  (Meta)data is assigned a globally unique and persistent identifier
    (Section \@ref(publish-identifiers)).
2.  Data is described with rich metadata (like the catalog shown above).
3.  Metadata clearly and explicitly includes the identifier of the data it describes.
4.  (Meta)data is registered or indexed in a searchable resource,
    such as the data sharing platforms described in Section \@ref(publish-data).

### Data should be *accessible*.

People can't use data if they don't have access to it.
In practice,
this rule means the data should be openly accessible (the preferred solution)
or that authenticating in order to view or download it should be free.
We can tell we've done this if:

1.  (Meta)data is retrievable by its identifier
    using a standard communications protocol like HTTP.
2.  Metadata is accessible even when the data is no longer available.

### Data should be *interoperable*.

Data usually needs to be integrated with other data,
which means that tools need to be able to process it.
We can tell we've done this if:

1.  (Meta)data uses a formal, accessible, shared, and broadly applicable language for knowledge representation.
2.  (Meta)data uses vocabularies that follow FAIR principles.
3.  (Meta)data includes qualified references to other (meta)data.

### Data should be *reusable*.

This is the ultimate purpose of the FAIR Principles and much other work.
We can tell we've done this if:

1.  Meta(data) is described with accurate and relevant attributes.
2.  (Meta)data is released with a clear and accessible data usage license.
3.  (Meta)data has detailed [provenance][provenance].
4.  (Meta)data meets domain-relevant community standards.

## Publishing Software {#publish-software}

In Chapter \@ref(packaging) we packaged and documented our Zipf's Law software.
That's enough for our users,
but if we want our package to be citable by researchers,
we need to give it a DOI.
Luckily,
GitHub [integrates with Zenodo][github-zenodo-tutorial]
for precisely this purpose.

> **Software Journals**
>
> While creating a DOI using a site like Zenodo
> is often the end of the software publishing process,
> there is the option of publishing
> a journal paper to describe the software in detail.
> Some research disciplines have journals devoted
> to describing particular types of software
> (e.g., [Geoscientific Model Development][geoscientific-model-development]),
> and there are also a number of generic software journals such as the
> [Journal of Open Research Software][jors] and
> the [Journal of Open Source Software][theoj].

But what about the software that *isn't* in a package,
such as short scripts written solely to create the figures and tables presented in the report
or a Jupyter notebook that calculates a set of results?
This software typically leverages other software packages,
so three separate items need to be published:

1.  A detailed description of the analysis software used.
2.  A copy of any analysis scripts used to produce the key results
    presented in the report.
3.  A description of the data processing steps taken in producing each key result
    (i.e. a step-by-step account of how the software and scripts were actually used).

Unfortunately,
librarians, publishers, and regulatory bodies are still trying to determine
the best way to document and archive material like this,
so there is not yet anything like the FAIR Principles.
The best advice we can give is presented below.

### Software Description

In order to document the software packages that were used,
the bare minimum requirement is to list the name and version number
of each software package
that played a role in producing the analysis presented in our report.
As Chapter \@ref(packaging) described,
we can get these automatically for Python by running:

```shell
$ pip freeze > requirements.txt
```

For everything else,
we should write a script or create a rule in your project's Makefile (Chapter \@ref(automate)),
since the commands used to get version numbers will vary from tool to tool:

```make
## versions : dump versions of software.
versions :
        @echo '# Python packages'
        @pip freeze
        @echo '# dezply'
        @dezply --version
        @echo '# parajune'
        @parajune --status | head 1
```

Such a list means your software environment is now technically reproducible,
but we have left it up to the reader to figure out how to get all those software packages
and libraries installed and working together.
In some cases it might be easy enough for a reader to install the handful of packages we used,
but in other cases we might want to make life easier.
For example,
a description of a complete `conda` environment can be saved as YAML using:

```shell
$ conda env export > environment.yml
```

That environment can then be recreated on another computer using:

```
$ conda env create -f environment.yml
```

More complex tools like [Docker][docker]
can literally install our entire environment (down to the precise version of the operating system)
on a different computer.
However,
their complexity can be daunting,
and there is a lot of debate
about how well (or whether) they actually make research more reproducible in practice.

### Analysis Scripts

FIXME: should we remove this and tell people to put everything in Git
and then use GitHub distributions?

The next item we need to publish is a copy of the scripts
written to execute those software packages.
Depending on the size or complexity of those scripts and whether we use them in multiple projects,
we may publish script by script
or create a zip file or tar file that includes everything.
For example,
the Makefile fragment below creates a file `~/archive/gothic-2019-02-21.tgz`
to hold the scripts used in analyzing Gothic tropes in classic novels:

```make
ARCHIVE=${HOME}/archive
PROJECT=gothic
TODAY=$(shell date "+%Y-%m-%d")
FILES=./${PROJECT}/Makefile ./${PROJECT}/bin/*.py ./${PROJECT}/bin/*.sh

## archive : create an archive of all the scripts used today.
archive :
        @mkdir -p ${ARCHIVE}
        @tar zcf ${ARCHIVE}/${PROJECT}-${TODAY}.tgz ${FILES}
```

### Data Processing Steps

Finally,
people need to know exactly how we ran our programs
if they are to check and reproduce our results.
The way in which we collect and archive this information
depends on our workflow.
If all of a program's parameters are in a configuration file (Chapter \@ref(config)),
then that file can be archived.
Alternatively,
we can have our program log its configuration parameters
and then use `grep` or a script to extract them from the logfile
(Section \@ref(errors-logging)).

If our workflow involves executing a series of command line programs,
we can keep a log/record of the command line entries
required to produce a given result.
For example,
the [cmdline-provenance][cmdline-provenance] Python package generates such records,
including keeping track of the corresponding version control revision number,
so we know exactly which version of your command line program was executed.
More prosaically,
running:

```shell
$ history | tail -n 20 > generate-figure-5.txt
```

is better than nothing.

## Summary {#publish-summary}

The Internet started a revolution in scientific publishing
that shows no sign of ending.
Where an inter-library loan once took weeks to arrive
and data had to be transcribed from published papers
(if it could be found at all),
we can now download one another's work in minutes:
*if* we can find it and make sense of it.
Organizations like [Our Research][our-research] are building tools to help with both;
by using DOIs and ORCIDs,
publishing on preprint servers,
following the FAIR Principles,
and documenting our workflow,
we help ensure that everyone can pursue their ideas as we did.

## Exercises {#publish-exercises}

### ORCID {#publish-ex-get-orcid}

If you don't already have an [ORCID][orcid],
go to the website and register now.
If you do have an ORCID,
log in and make sure that your details and publication record are up-to-date.

### A FAIR test {#publish-ex-fair-test}

An [online questionnaire][fair-questionnaire]
for measuring the extent to which datasets are FAIR
has been created by the Australian Research Data Commons.
Take the questionnaire for a dataset you have published or that you use often.

### Publishing your code

Think about a project that you're currently working on.
How would you go about publishing the code associated with that project
(i.e., the software description, analysis scripts, and data processing steps)?

## Key Points {#publish-keypoints}

```{r, child="keypoints/shared-rse/publish.md"}
```

```{r, child="./links.md"}
```
