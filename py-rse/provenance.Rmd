# Provenance {#provenance}

```{r publish-setup, include=FALSE}
source(here::here("_common.R"))
```

We are now at the point where we've developed, automated and tested
a workflow for plotting the word count distribution for our collection of classic novels.
In the normal course of events,
outputs from that workflow (e.g. our figures and \(\alpha\) values)
would ultimately be included in a report.
We use the term "report" to include research papers,
summaries for clients,
or anything else that is shorter than a book
and aimed at people other than its creators. 

The first and most important point to make
is that modern publishing involves much more than producing a printable PDF.
It also entails providing the data underpinning the report
as well as the code used to do the analysis:

> An article about computational science in a scientific publication
> is *not* the scholarship itself,
> it is merely *advertising* of the scholarship.
> The actual scholarship is the complete software development environment
> and the complete set of instructions which generated the figures.
>
> --- Jonathan Buckheit and David Donoho, paraphrasing Jon Claerbout, in @Buck1995

While some reports, datasets, software packages and/or analysis scripts
can't be published without violating personal or commercial confidentiality,
every researcher's default should be to make all these components
of their work as widely available as possible.
Publishing it under an open license (Section \@ref(teams-license)) is the first step;
the sections below describe some of the other steps we can take
to capture the provenance of our data analysis.

## Data Provenance {#data-provenance}

The first step in documenting the data associated with a report
is to determine what (if anything) needs to be published.
If the report involved the analysis of a publicly available dataset
that is maintained and documented by a third party,
the report simply needs to document where to access the data
and what version was analyzed:
it's not necessary to publish a duplicate of the dataset.
This is the case for our Zipf's Law analysis,
since the texts we analyze are available at Project Gutenberg.

It's not strictly necessary to publish intermediate data
produced during the analysis of a publicly available dataset either,
so long as readers have access to the original data and the software used to process it.
However,
making intermediate data available can save people time and effort,
particularly if it takes a lot of computing power to reproduce it.
For example,
NASA has published
the [Goddard Institute for Space Studies Surface Temperature Analysis][gistemp],
which estimates the global average surface temperature
based on land and ocean weather observations,
because a simple metric of global warming is expensive to produce
and useful in many research contexts.

If a report involves a new dataset,
such as observations collected during a field experiment,
following a few simple rules can make it much more usable:

1.  Always use [tidy data][tidy-data].
2.  Include keywords describing the data in the project's `README.md`
    so that they appear on its home page and can easily be found by search engines.
3.  Give the data a unique identifier (Section \@ref(publish-identifiers)).
4.  Put the data in an open repository like [Figshare][figshare] or [Zenodo][zenodo].
5.  Use well-known formats like CSV and HDF5.
6.  Include an explicit license in every project and every dataset.
7.  Include units and other metadata.

The last point is often the hardest for people to implement,
since many researchers have never seen a properly-documented dataset.
We draw inspiration from the data catalog included in [the repository][womens-pockets-data] for @Dieh2018
and include a file `./data/README.md` in every project
that looks like this:

```
-   Infants born to women with HIV receiving an HIV test within two months of birth, 2009-2017
    -   `Infant_HIV_Testing_2017.xlsx`
        -   What is this?: Excel spreadsheet with summarized data.
        -   Source(s): UNICEF, <https://data.unicef.org/resources/dataset/hiv-aids-statistical-tables/>
        -   Last Modified: July 2018 (according to website)
        -   Contact: Greg Wilson <greg.wilson@rstudio.com>
        -   Spatial Applicability: global
        -   Temporal Applicability: 2009-2017
    -   `infant_hiv.csv`
        -   What is this?: CSV export from `Infant_HIV_Testing_2017.xlsx`
    -   Notes
        -   Data is not tidy: some rows are descriptive comments,
            others are blank separators between sections, and column headers are inconsistent.
        -   Use `tidy_infant_hiv()` to tidy this data.
-   Maternal health indicators disaggregated by age
    -   `maternal_health_adolescents_indicators_April-2016_250d599.xlsx`
        -   What is this?: Excel spreadsheet with summarized data.
        -   Source(s): UNICEF, <https://data.unicef.org/resources/dataset/maternal-health-data/>
        -   Last Modified: July 2018 (according to website)
        -   Contact: Greg Wilson <greg.wilson@rstudio.com>
        -   Spatial Applicability: global
        -   Temporal Applicability: 2000-2014
    -   `at_health_facilities.csv`
        -   What is this?: percentage of births at health facilities by country, year, and mother's age
        -   Source(s): single sheet from `maternal_health_adolescents_indicators_April-2016_250d599.xlsx`
    -   `c_sections.csv`
        -   What is this?: percentage of Caesarean sections by country, year, and mother's age
        -   Source(s): single sheet from `maternal_health_adolescents_indicators_April-2016_250d599.xlsx`
    -   `skilled_attendant_at_birth.csv`
        -   What is this?: percentage of births with skilled attendant present by country, year, and mother's age
        -   Source(s): single sheet from `maternal_health_adolescents_indicators_April-2016_250d599.xlsx`
    -   Notes
        -   Data is not tidy:
            some rows are descriptive comments,
            others are blank separators between sections,
            and column headers are inconsistent.
        -   Use `tidy_maternal_health_adolescents()` to tidy this data.
```

The catalog above doesn't include column headers or units
because the raw data isn't tidy.
It *does* include the names of the functions used to reformat that data,
and `./results/README.md` then includes the information that users will want.
One section of that file is shown below:

```
-   Infants born to women with HIV receiving an HIV test within two months of birth, 2009-2017
    -   infant_hiv.csv
      -   What is this?: tidied version of CSV export from spreadsheet.
      -   Source(s): UNICEF, <https://data.unicef.org/resources/dataset/hiv-aids-statistical-tables/>
      -   Last Modified: September 2018
      -   Contact: Greg Wilson <greg.wilson@rstudio.com>
      -   Spatial Applicability: global
      -   Temporal Applicability: 2009-2017
      -   Generated By: scripts/tidy-24.R

| Header   | Datatype | NA    | Description                                 |
|----------|----------|-------|---------------------------------------------|
| country  | char     | false | ISO3 country code of country reporting data |
| year     | integer  | false | year CE for which data reported             |
| estimate | double   | true  | estimated percentage of measurement         |
| hi       | double   | true  | high end of range                           |
| lo       | double   | true  | low end of range                            |
```

Note that this catalog includes both units
and whether or not a field can have missing values.

### The FAIR Principles {#publish-fair}

The [FAIR Principles][go-fair] describe what research data should look like.
They are still aspirational for most researchers @Broc2019,
but tell us what to aim for.
The most immediately important elements of the FAIR Principles are outlined below.

#### Data should be *findable*.

The first step in using or re-using data is to find it.
We can tell we've done this if:

1.  (Meta)data is assigned a globally unique and persistent identifier
    (Section \@ref(publish-identifiers)).
2.  Data is described with rich metadata (like the catalog shown above).
3.  Metadata clearly and explicitly includes the identifier of the data it describes.
4.  (Meta)data is registered or indexed in a searchable resource,
    such as the data sharing platforms described in Section \@ref(publish-data).

#### Data should be *accessible*.

People can't use data if they don't have access to it.
In practice,
this rule means the data should be openly accessible (the preferred solution)
or that authenticating in order to view or download it should be free.
We can tell we've done this if:

1.  (Meta)data is retrievable by its identifier
    using a standard communications protocol like HTTP.
2.  Metadata is accessible even when the data is no longer available.

#### Data should be *interoperable*.

Data usually needs to be integrated with other data,
which means that tools need to be able to process it.
We can tell we've done this if:

1.  (Meta)data uses a formal, accessible, shared, and broadly applicable language for knowledge representation.
2.  (Meta)data uses vocabularies that follow FAIR principles.
3.  (Meta)data includes qualified references to other (meta)data.

#### Data should be *reusable*.

This is the ultimate purpose of the FAIR Principles and much other work.
We can tell we've done this if:

1.  Meta(data) is described with accurate and relevant attributes.
2.  (Meta)data is released with a clear and accessible data usage license.
3.  (Meta)data has detailed [provenance][provenance].
4.  (Meta)data meets domain-relevant community standards.

## Code provenance {#code-provenance}

Our Zipf's Law analysis represents a typical data science project,
in that we've written some code that leverages other pre-existing software packages
(e.g. Matplotlib, numpy) in order to conduct our analysis.
In order to document this code and software in an open, transparent and reproducible manner,
three separate items need to be published:

1.  A detailed description of the analysis software used.
2.  A copy of any analysis scripts/notebooks used to produce the key results
    presented in the report.
3.  A description of the data processing steps taken in producing each key result
    (i.e. a step-by-step account of how the software and scripts were actually used).

Unfortunately,
librarians, publishers, and regulatory bodies are still trying to determine
the best way to document and archive material like this,
so there is not yet anything like the FAIR Principles.
The best advice we can give is presented below.

### Software Description

FIXME: This section would make more sense if we were using a conda environment
from the beginning (i.e. in the installation instructions).
We should consider the merits of doing that.

In order to document the software packages that were used/leveaged in our analysis,
the bare minimum requirement is to list the name and version number
of each software package.
We can get version information for the Python packages we are using by running:

```shell
$ pip freeze > requirements.txt
```

Other non-Python command line tools will often have an option like `--version` or `--status`
that you can run to get version information.

Such a list means your software environment is now technically reproducible,
but we have left it up to the reader to figure out how to get all those software packages
and libraries installed and working together.
In some cases it might be easy enough for a reader to install the handful of packages we used,
but in other cases we might want to make life easier.
For example,
a description of a complete `conda` environment can be saved as YAML using:

```shell
$ conda env export > environment.yml
```

That environment can then be recreated on another computer using:

```
$ conda env create -f environment.yml
```

More complex tools like [Docker][docker]
can literally install our entire environment (down to the precise version of the operating system)
on a different computer (ref: https://osf.io/fsd7t/).
However,
their complexity can be daunting,
and there is a lot of debate
about how well (or whether) they actually make research more reproducible in practice.

### Analysis Scripts

The next item we need to publish is a copy of the scripts
written to execute those software packages.
Depending on the size or complexity of those scripts and whether we use them in multiple projects,
we may publish script by script
or create a zip file or tar file that includes everything.
In our case,
the Python scripts we ran
are located in our `bin/` directory,
so we can put them all in a zip folder

```shell
$ zip scripts.zip bin/*.py
```
```text
  adding: bin/collate.py (deflated 52%)
  adding: bin/countwords.py (deflated 54%)
  adding: bin/plotcounts.py (deflated 58%)
  adding: bin/test_zipfs.py (deflated 67%)
  adding: bin/utilities.py (deflated 51%)
```

### Data Processing Steps

Finally,
to make our results truly reproducible,
people need to know exactly how (and in what order) we ran our analysis scripts
in our software environment.
The way in which this information is collected and archived
depends on the characteristics of the workflow.
Our Zipf's Law analysis involved executing a series of command line programs,
so we can generate a record of the command line entries involved in plotting the
the collated word count distribution, for instance,
by running `make` with the `-n` option (for a dry run so the commands are not executed)
and the `-B` option (to run all steps, regardless or whether they need updating or not): 

```
$ make all -n -B
```
```
python bin/countwords.py data/dracula.txt > results/dracula.csv
python bin/countwords.py data/time_machine.txt > results/time_machine.csv
python bin/countwords.py data/sense_and_sensibility.txt > results/sense_and_sensibility.csv
python bin/countwords.py data/sherlock_holmes.txt > results/sherlock_holmes.csv
python bin/countwords.py data/moby_dick.txt > results/moby_dick.csv
python bin/countwords.py data/frankenstein.txt > results/frankenstein.csv
python bin/countwords.py data/jane_eyre.txt > results/jane_eyre.csv
mkdir -p results
python bin/collate.py results/time_machine.csv results/moby_dick.csv results/jane_eyre.csv results/dracula.csv results/sense_and_sensibility.csv results/sherlock_holmes.csv results/frankenstein.csv > results/collated.csv
python bin/plotcounts.py results/collated.csv --outfile results/plotcounts.png
```

If all of a program's parameters are in a configuration file (Chapter \@ref(config)),
then that file can be archived.
Alternatively,
we can have our program log its configuration parameters
and then use `grep` or a script to extract them from the logfile
(Section \@ref(errors-logging)).

### Where to put this information

We have the following files to archive:
- environment.yml
- scripts.zip
- commands.txt
- rcparams.yml


FIXME: Talk about Figshare.

### Inspectability

FIXME: Expand on the ideas in http://ivory.idyll.org/blog/tag/futurepaper.html

"We don't really care about our ability to exactly re-run a decade old computational analysis.
What we do care about is our ability to figure out what was run
and what the important decisions were -- something that Yolanda Gil refers to as inspectability.
But exact repeatability has a short shelf-life (and this is ok).


## Summary {#publish-summary}

The Internet started a revolution in scientific publishing
that shows no sign of ending.
Where an inter-library loan once took weeks to arrive
and data had to be transcribed from published papers
(if it could be found at all),
we can now download one another's work in minutes:
*if* we can find it and make sense of it.
Organizations like [Our Research][our-research] are building tools to help with both;
by using DOIs and ORCIDs,
publishing on preprint servers,
following the FAIR Principles,
and documenting our workflow,
we help ensure that everyone can pursue their ideas as we did.

## Exercises {#publish-exercises}

### ORCID {#publish-ex-get-orcid}

If you don't already have an [ORCID][orcid],
go to the website and register now.
If you do have an ORCID,
log in and make sure that your details and publication record are up-to-date.

### A FAIR test {#publish-ex-fair-test}

An [online questionnaire][fair-questionnaire]
for measuring the extent to which datasets are FAIR
has been created by the Australian Research Data Commons.
Take the questionnaire for a dataset you have published or that you use often.

### Publishing your code {#publish-ex-publish-code}

Think about a project that you're currently working on.
How would you go about publishing the code associated with that project
(i.e., the software description, analysis scripts, and data processing steps)?

## Key Points {#publish-keypoints}

```{r, child="keypoints/shared-rse/publish.md"}
```
