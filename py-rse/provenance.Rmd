# Provenance {#provenance}

We are now at the point where we've developed, automated and tested
a workflow for plotting the word count distribution for our collection of classic novels.
In the normal course of events,
outputs from that workflow (e.g. our figures and \(\alpha\) values)
would ultimately be included in a report.
Here we use the term "report" to include research papers,
summaries for clients,
or anything else that is shorter than a book
and aimed at people other than its creators. 

The first and most important point to make
is that modern publishing involves much more than producing a printable PDF.
It also entails providing the data underpinning the report
as well as the code used to do the analysis:

> An article about computational science in a scientific publication
> is *not* the scholarship itself,
> it is merely *advertising* of the scholarship.
> The actual scholarship is the complete software development environment
> and the complete set of instructions which generated the figures.
>
> --- Jonathan Buckheit and David Donoho, paraphrasing Jon Claerbout, in @Buck1995

While some reports, datasets, software packages and/or analysis scripts
can't be published without violating personal or commercial confidentiality,
every researcher's default should be to make all these components
of their work as widely available as possible.
Publishing it under an open license (Section \@ref(teams-license)) is the first step;
the sections below describe some of the other steps we can take
to capture the provenance of our data analysis.

> **Identifying Reports and Authors**
>
> Before publishing anything,
> we need to understand how authors and their works are identified.
> A [Digital Object Identifier][doi] (DOI)
> is a unique identifier for a particular version of a particular digital artifact
> such as a report, a dataset, or a piece of software.
> DOIs are written as `doi:prefix/suffix`,
> but are often also represented as URLs like `http://dx.doi.org/prefix/suffix`.
> In order to be allowed to issue a DOI,
> an academic journal, data archive, or other organiation
> must guarantee a certain level of security, longevity and access.
>
> An [ORCID][orcid] is an Open Researcher and Contributor ID.
> Anyone can get an ORCID for free,
> and should include it in publications
> because people's names and affiliations change over time.

FIXME: Integrate the following two sections into this chapter

## Data versus Code {#project-data-vs-code}

Documenting data is different from documenting code
because data always comes from somewhere
and has (almost always) had something done to it.
Its documentation must therefore include details about process,
selection,
and transformation.
Often,
all we need is short statements to describe the actions taken.
Depending on the complexity,
the content description and the process information may be contained in the same statement,
such as,
"Contents of `verb-frequency.csv` after run through `select-verbs.py` and `countwords.py`."

> **Explaining Why**
>
> Tools like [OpenRefine][openrefine] allow us to export a list of all the changes we have made to a dataset,
> but these logs do not explain *why* these changes were made:
> they might,
> for example,
> tell us that we changed "[1980?]", "1980?", and "[[1980]" to "1980",
> but that is a description, not a reason.

A file that has recieved extensive manual editing and curation
should have a detailed description of what was done.
To decide what to say,
we can think about the questions we would ask
if we were playing a text adventure game
in which we wake up in a room we don't recognize.
We would ask things like:
Where am I?
What's around me?
What can I do here?
Where can I go?
We need to know that things exist before we can go investigate them,
and the same goes for making our way around data.

Discussions about data documentation often ask authors
to report absolutely everything;
however,
anything is better than nothing and is a useful starting point.
This is why documentation starts with a README file:
it is where most newcomers will start building their expectations around the data.  
Beyond that,
we need to answer questions about what is available and where it came from.
Newcomers will generally drill down in this order:

-   Project: what did we do?
-   Dataset: what did we make?
-   Data files: what's in this file or that column?
-   Datum: where did this value come from?

Multiple styles of material will help different people best (SectionÂ \@ref(project-faq)),
but most projects do not have the resources to do this.
More details are great to have in documentation,
but asking for too much risks burnout.
Instead:

1.  Documentation should have enough information,
2.  about the project, methods, and materials
3.  that is maintainable,
4.  in an accessible format,
5.  and valuable for those who need it.

Breaking this down:

-   "Enough information" means that we can skip things that are available elsewhere.
    For example,
    if methods or computations are described in published papers
    that *aren't* locked behind paywalls,
    we can link to them instead of duplicating their content.

-   "The project, methods, and materials"
    are what we need to describe.
    Just like a personal introduction should have our name, pronouns, and position,
    we should introduce our data by explaining what the project was for,
    what methods we used in it,
    and what materials we applied those methods to.

-   "Maintainable" reflects the fact that active projects outgrow their documentation.
    If things are changing rapidly,
    they may not be worth documenting (yet),
    and if our documentation scheme is complex,
    the effort required to keep it up to date may never pay off.

-   "Accessible" means two things.
    First,
    proprietary formats create barriers to access,
    since some people may not have access to the software needed to read the documents.
    They also shorten those documents' useful lifespan,
    since manufacturers can change those formats over time.
    If we do use such a format because we have to integrate with particular systems
    or because formatting and image embedding are better,
    we should always make a plain-text copy available as well.

    Second,
    we should use proper markup so that screen readers can interact with the structure of the text,
    provide transcripts for all screencasts or other videos,
    add alt-text descriptions to images,
    and ensure that all text is actually rendered so that it can be selected and interacted with.
    The UK Home Office has published a [set of posters][ukho-accessibility] that summarize accessibility guidelines
    and also serve as a great checklist for ensuring access.

-   Finally,
    "for those who need it" means taking our audience into account
    when deciding what to include at what level of detail.
    For example,
    the documentation for a specialized sentiment analysis package
    could reasonably expect readers to have a graduate-level understanding of the topic,
    so documentation aimed at high school students may simply not be worth creating.

> **Your Future Self Will Thank You**
>
> Documentation is often promoted for the good of people reusing data
> who were not part of creating the data in the first place.
> Prioritizing their needs can be difficult:
> how can we justify spending time for other people
> when our current projects need work fo the good of the people working on them right now?
>
> Instead of thinking about people who are unknown and unrelated,
> we can think about newcomers to our team
> and the time we will save ourselves in onboarding them.
> We can also think about the time we will save ourselves
> when we come back to this project five months or five years from now.
> Documentation that serves these two groups well
> will almost certainly serve the needs of strangers as well.

These additional rules are taken from [@Good2014,@Mich2015,@Hart2016,@Zook2017]:

-   **Keep raw data raw.**
    Exactly what constitutes "raw" data isn't always clear:
    raw data may be the data we downloaded from a repository,
    or it may come directly from field observations or hardware.
    Either way,
    retaining an unchanged version is important for reproducibility
    (and in case we accidentally overwrite our working copy).

-   **Give every part of the data a unique identifier.**
    To aid reproducibility (and save a lot of confusion in conversation),
    it should be possible to uniquely identify every part of every dataset.
    This doesn't mean every cell has to have a unique ID;
    instead,
    every datum should have an "address" or location description that can uniquely identify it.
    For example,
    a datum may be "in file `ACX-02.csv`, under column `reading_type`, in row 450".

-   **Have a backup plan.**
    Every hard drive eventually fails.
    The hardware needed to read various storage media eventually becomes unavailable,
    and there is always the risk that bankruptcy, corporate merger, or a court order
    will shut down the site we have been using to store our data.
    We should therefore always have a second copy in a different physical location and legal jurisdiction.
    If the primary store is GitHub,
    we should copy the data to a secondary online storage provider
    or onto a USB drive and take it home.
    If the data contains sensitive information such as personal health records,
    we must ensure that the storage complies with data privacy regulations
    and that the data is properly anonymized before it is uploaded to an off-site storage location.

    Before doing any of this, though,
    we should ask our department or institution what facilities they provide.
    Many academic institutions have some form of institutional repository
    that data can be uploaded to
    or a unit whose job it is to handle long term data storage requests.
    Their staff will also be more familiar with data privacy regulations than most researchers.

-   **Test that the backups work.**
    It's hard to find out how often backups fail---many statistics are inflated
    by companies selling backup tools---but figures of 15--25% are often quoted.
    Restoration failures are particularly common after adding new datasets,
    changing backup software or procedures,
    moving to new hardware or new backup locations,
    or just because it's Tuesday.
    We should therefore periodically check that backups can actually be restored,
    and in particular,
    that they can be restored by someone other than the person who created them
    (so that if a password is required,
    someone else knows what it is).


## Managing External Data {#project-external}

Small datasets that don't contain sensitive information should be stored in version control:
as a rule of thumb,
anything we would send as an email attachment is probably small enough to be put into Git,
while anything that might reveal someone's identity should not be.
If data is large or sensitive,
there should still be something in `data` to show its existence,
and that "something" should be easy for programs to read.
One option is a CSV file whose columns are:

-   the name of the dataset,
-   its URL or other unique identifier,
-   the date it was last checked, and
-   its size (so that users will have some idea of how much work is involved in processing it).

Another option is to have one file per dataset,
so that instead of storing a thousand movies in the `films` directory,
we store a thousand [YAML][yaml] files,
each of which contains a `url` key identifying the film's location.

## Data Provenance {#provenance-data}

The first step in documenting the data associated with a report
is to determine what (if anything) needs to be published.
If the report involved the analysis of a publicly available dataset
that is maintained and documented by a third party,
the report simply needs to document where to access the data
and what version was analyzed:
it's not necessary to publish a duplicate of the dataset.
This is the case for our Zipf's Law analysis,
since the texts we analyze are available at [Project Gutenberg][project-gutenberg].

It's not strictly necessary to publish intermediate data
produced during the analysis of a publicly available dataset either
(e.g. our CSV files produced by `countwords.py`),
so long as readers have access to the original data and the code/software used to process it.
However,
making intermediate data available can save people time and effort,
particularly if it takes a lot of computing power to reproduce it.
For example,
NASA has published
the [Goddard Institute for Space Studies Surface Temperature Analysis][gistemp],
which estimates the global average surface temperature
based on land and ocean weather observations,
because a simple metric of global warming is expensive to produce
and is useful in many research contexts.

If a report involves a new dataset,
such as observations collected during a field experiment,
then they need to be published following the FAIR Principles.

### The FAIR Principles {#provenance-data-fair}

The [FAIR Principles][go-fair] describe what research data should look like.
They are still aspirational for most researchers @Broc2019,
but tell us what to aim for.
The most immediately important elements of the FAIR Principles are outlined below.

#### Data should be *findable*.

The first step in using or re-using data is to find it.
We can tell we've done this if:

1.  (Meta)data is assigned a globally unique and persistent identifier
    (i.e. a [DOI][doi]).
2.  Data is described with rich metadata
3.  Metadata clearly and explicitly includes the identifier of the data it describes.
4.  (Meta)data is registered or indexed in a searchable resource,
    such as the data sharing platforms described in SectionÂ \@ref(provenance-data-where).

#### Data should be *accessible*.

People can't use data if they don't have access to it.
In practice,
this rule means the data should be openly accessible (the preferred solution)
or that authenticating in order to view or download it should be free.
We can tell we've done this if:

1.  (Meta)data is retrievable by its identifier
    using a standard communications protocol like HTTP.
2.  Metadata is accessible even when the data is no longer available.

#### Data should be *interoperable*.

Data usually needs to be integrated with other data,
which means that tools need to be able to process it.
We can tell we've done this if:

1.  (Meta)data uses a formal, accessible, shared, and broadly applicable language for knowledge representation.
2.  (Meta)data uses vocabularies that follow FAIR principles.
3.  (Meta)data includes qualified references to other (meta)data.

#### Data should be *reusable*.

This is the ultimate purpose of the FAIR Principles and much other work.
We can tell we've done this if:

1.  Meta(data) is described with accurate and relevant attributes.
2.  (Meta)data is released with a clear and accessible data usage license.
3.  (Meta)data has detailed [provenance][provenance].
4.  (Meta)data meets domain-relevant community standards.

### Where to Archive Data {#provenance-data-where}

Small datasets (i.e., anything under 500 MB) can be stored in version control
using the conventions described in Chapter \@ref(project).
If the data is being used in several projects,
it may make sense to create one repository to hold only the data;
the R community refers to these as [data packages][data-package],
and they are often accompanied by small scripts to clean up and query the data.

For medium-sized datasets (between 500 MB and 5 GB),
it's better to put the data on platforms
like the [Open Science Framework][osf], [Dryad][dryad], and [Figshare][figshare],
which will give the dataset a DOI.
Big datasets (i.e., anything more than 5 GB)
may not be ours in the first place,
and probably need the attention of a professional archivist.

> **Data Journals**
>
> While archiving data at a site like Dryad or Figshare (following the FAIR Principles)
> is usually the end of the data publishing process,
> there is the option of publishing a journal paper to describe the dataset in detail.
> Some research disciplines have journals devoted
> to describing particular types of data
> (e.g., the [Geoscience Data Journal][geoscience-data-journal])
> and there are also generic data journals
> (e.g., [Scientific Data][scientific-data]).

## Code Provenance {#code-provenance}

Our Zipf's Law analysis represents a typical data science project
in that we've written some code (in the form of a series of Python scripts)
that leverages other pre-existing software packages (matplotlib, numpy, etc)
in order to produce the key results of a report (the word distribution plots).
Documenting the details of a computational workflow like this
in an open, transparent and reproducible manner
typically requires that three key items are archived:

1.  A copy of any **analysis scripts/notebooks** used to produce the key results
    presented in the report.
2.  A detailed description of the **software environment** in which those analysis scripts/
    notebooks were executed.
3.  A description of the **data processing steps** taken in producing each key result
    (i.e. for each key result, a step-by-step account of which scripts were executed
    and in what order).

Unfortunately,
librarians, publishers, and regulatory bodies are still trying to determine
the best way to document and archive material like this,
so there is not yet anything like the FAIR Principles.
The best advice we can give is presented below.
It involves adding information about the software environment
and data processing steps to a GitHub repository that contains
the analysis scripts/notebooks,
before creating a new release of that repository and archiving it (with a DOI)
with [Zenodo][zenodo].

### Software Environment {#code-provenance-environment}

FIXME: This section would make more sense if we were using a conda environment
from the beginning (i.e. in the installation instructions).
We should consider the merits of doing that.

In order to document the software packages that were used in our analysis,
the bare minimum requirement is to archive a list of
the names and version numbers of each software package.
We can get version information for the Python packages we are using by running:

```shell
$ pip freeze
```

```text
FIXME: Show output
```

Other command line tools will often have an option like `--version` or `--status`
to access the version information.

Archiving a list of package names and version numbers would mean that our
software environment is technically reproducible,
but it would be left up to the reader of the report
to figure out how to get all those packages installed and working together.
This might be fine for a small number of packages with very few dependencies,
but in more complex cases we probably want to make life easier for the reader
(and for our future selves looking to re-run the analysis).
One way to make things easier is to export a description of a complete
`conda` environment, which can be saved as YAML using:

```shell
$ conda env export > environment.yml
```

That software environment can be recreated on another computer with one line of code:

```
$ conda env create -f environment.yml
```

We can go ahead and add the `environment.yml` file to our GitHub repository:

```shell
$ git add environment.yml
$ git commit -m "Adding the conda environment file"
$ git push origin master
```

> **Container Images**
> 
> More complex tools like [Docker][docker]
> can literally install our entire environment
> (down to the precise version of the operating system)
> on a different computer @Nust2020.
> However,
> their complexity can be daunting,
> and there is a lot of debate
> about how well (or whether) they actually make research more reproducible in practice.

### Data Processing Steps {#code-provenance-steps}

The second item that needs to be added to our GitHub repository is a description
of the data processing steps involved in each key result.
Assuming the author list on our report is Amira Khan and Sami Virtanen (Section \@ref(intro-personas)),
we could create a new markdown file called `KhanVirtanen2020.md` to describe the steps:

```markdown
The code in this repository was used in generating the results for the following paper:

Khan A and Virtanen S (2020). Zipf's Law in classic english texts.
*Journal of Important Research*, 27, 134-139. 

The code was executed in the software environment described by `environment.yml`.
It can be installed using [conda](https://docs.conda.io/en/latest/):
$ conda env create -f environment.yml

Figure 1 in the paper was created by running the following at the command line:
$ make all
```

This new file can be added to the repository:

```shell
$ git add KhanVirtanen2020.md
$ git commit -m "Adding data processing steps for Khan and Virtanen (2020)"
$ git push origin master
```

### Analysis Scripts {#code-provenance-scripts}

Later in this book we will package and release our Zipf's Law code
so that it can be downloaded and installed by the wider research community,
just like any other Python package (Chapter \@ref(packaging)).
Going through the process of formally packaging your code can be a good option
if there's a user community out there interested in using and/or building it,
but often the scripts and notebooks we write
to produce a particular figure or table
will not be of broad interest.
In other words,
these scripts were written to get a specific job done
(e.g. to produce Figure 1 in a report)
and do not have a wider application.
To fully capture the provenance of the results presented in a report,
these analysis scripts and/or notebooks 
(along with the details of the associated software environment and data processing steps)
can be archived with a repository like [Figshare][figshare] or [Zenodo][zenodo],
which specialize in storing the "long-tail" of research projects
(i.e. supplementary figures, data and/or code).
Traditionally that meant uploading a zip file full of analysis scripts to the repository
(which is still a valid option),
but more recently the process has been streamlined via direct integration
between GitHub and Zenodo.
The process (described in [this tutorial][github-zenodo-tutorial]) involves
creating a new release of your repository in GitHub (e.g. Figure \@ref(fig:provenance-release)),
which Zenodo copies and then issues a DOI.

```{r provenance-release, echo=FALSE, fig.cap="A new code release in GitHub"}
knitr::include_graphics("figures/py-rse/provenance/release.png")
```

### Reproducibility Versus Inspectability {#provenance-inspectability}

In most cases,
documenting your software environment, analysis scripts and data processing steps
will ensure that your computational analysis is reproducible/repeatable 
at the time your report is published.
But what about five or ten years from now?
As we have discussed,
data analysis workflows usually depend on a hierarchy of packages.
Our Zipf's Law analysis depends on a a collection of Python libraries (numpy, matplotlib, scipy, etc),
which in turn depend on the Python language itself.
Some workflows also depend critically on a particular operating system or firmware.
Over time some of these dependencies will inevitably be updated or no longer supported,
meaning the workflow you have documented will no longer be reproducible.
Fortunately,
most readers are not looking to exactly re-run a decade old analysis.
They really just want to be able to figure out what was run 
and what the important decisions were -- something referred to as inspectability (@Gil2016, @Brown2017). 
While exact repeatability has a short shelf-life,
inspectability is the enduring legacy of a well documented computational analysis.

FIXME: The following came from project structure, this chapter seems like the best fit for it?

-   `CONTRIBUTORS`
    lists everyone who has contributed to the project.
    Software projects often put this information in `README`,
    while research projects make it a section in `CITATION`.

The fourth file is normally only found in research projects:

-   `CITATION` explains how the work should be cited.
    This file should contain a plain text citation that can be copied and pasted into email,
    and may also include entries formatted for various bibliographic systems like [BibTeX][bibtex].



## Summary {#provenance-summary}

The Internet started a revolution in scientific publishing
that shows no sign of ending.
Where an inter-library loan once took weeks to arrive
and data had to be transcribed from published papers
(if it could be found at all),
we can now download one another's work in minutes:
*if* we can find it and make sense of it.
Organizations like [Our Research][our-research] are building tools to help with both;
by using DOIs and ORCIDs,
publishing on preprint servers,
following the FAIR Principles,
and documenting our workflow,
we help ensure that everyone can pursue their ideas as we did.

## Exercises {#provenance-exercises}

### ORCID {#provenance-ex-get-orcid}

If you don't already have an [ORCID][orcid],
go to the website and register now.
If you do have an ORCID,
log in and make sure that your details and publication record are up-to-date.

### A FAIR test {#provenance-ex-fair-test}

An [online questionnaire][fair-questionnaire]
for measuring the extent to which datasets are FAIR
has been created by the Australian Research Data Commons.
Take the questionnaire for a dataset you have published or that you use often.

### Evaluate a project's data provenance {#provenance-ex-understand-project}

*This exercise is modified from @Wick2016 and explore the dataset from @Meil2015.
Go to the dataset's page (http://doi.org/10.3886/E17507V2) and download the files.
You will need to make an ICPSER account and agree to their data agreement before you can download.*

Review the dataset's main page to get a sense of the study,
then review the spreadsheet file and the coded response file.

1.  Who are the participants of this study?
2.  What types of data was collected and used for analysis?
3.  Can you find information on the demographics of the interviewees?
4.  This dataset is clearly in support of an article.
    What information can you find about it, and can you find a link to it?

### Making permanent links {#provenance-ex-permanent-links}

The link to the UK Home Office's [accessibility guideline posters][ukho-accessibility] might change in future.
Use the [Wayback Machine][wayback-machine] to find a link that is more likely to be usable in the long run.
The fourth file is normally only found in research projects:

### Publishing your code {#provenance-ex-publish-code}

Think about a project that you're currently working on.
How would you go about publishing the code associated with that project
(i.e., the software description, analysis scripts, and data processing steps)?

## Key Points {#publish-keypoints}

```{r, child="keypoints/shared-rse/provenance.md"}
```
