# Python Packaging {#py-data-manipulation}

## Introduction {#py-data-manipulation-intro}

With just a few Python libraries and commands, you can rearrange, process, manipulate, and
analyze tabular data easily and reproducibly.

This material is based on a [workshop run by UofT Coders](https://github.com/UofTCoders/workshops-dc-python)
and a [Data Carpentry Python workshop](https://datacarpentry.org/python-ecology-lesson/).

## How can I read tabular data into a program? {#py-data-manipulation-reading}

The package `pandas` is built for working with tabular data, and the `pandas`
function `read_csv()` takes a tabular text file as input and returns a `pandas`
data frame.

```python
# reading in tabular data - example

import pandas as pd # import the pandas package

# where is the file located on your computer?
# you can use a relative path (shown in the example), or the full path (i.e. /home/madeleine/data/file.csv)
filename = "data/nyc-dog-licensing.csv"

# the function pd.read_csv() takes a file path as its default input and tries to read in a table of data.
# here we assign the function's output to a variable called "data"
dog_licenses = pd.read_csv(filename)
```

You can also pass a url as the `filename` argument in `read_csv()`:

```python
data_link = "https://raw.githubusercontent.com/merely-useful/merely-useful.github.io/book/data/measurements.csv"
dog_licenses = pd.read_csv(data_link)
```

The `read_csv()` function assumes by default that the column delimiter
in the file is a comma and that the first line of the file contains column names.
To specify a different delimiter (for example tab), pass the argument `sep = \t`.
If the data has no column names, pass the argument `header = None`, or give a list
of column names using the argument `names = [column names]`.

## How can I tidy up my data?

Even carefully created and curated data can present difficulties at the analysis step.
Here are a few tidying steps you might consider when exploring a new data set.

### Modifying column names

In this book we are using *snake case* for column names (`snake_case`), but another common format
is called *camel case* (`CamelCase`). The following code uses the package `inflection`
to convert a list of column names from camel to snake case.

```python
# import the underscore function which creates snake case
from inflection import underscore

# get a list of columns in snake case for the data frame 'df'
snake_case_columns = [underscore(column) for column in df.columns]

# reassign the columns of 'df'
df.columns = snake_case_columns
```

If the column names have any whitespace in them, the following one-liner will
rename them without the whitespace.

```python
# Remove whitespace from column names
df = df.rename(columns=lambda x: x.strip())
```

You might also decide that you don't want to keep some columns; you can drop them
from the data frame using the function `drop()`. For example, the following code drops the columns
`city_council_district` and `census_tract2010` from the `dog_licenses` data frame.

```python
# drop two columns
dog_licenses = dog_licenses.drop(columns = ["city_council_district", "census_tract2010"])
```

### Cleaning values

Often, data entered by humans can have errors or inconsistent formats. The values
in the `borough` column of the dog license data are a good example. To see all the
unique values in this column, we can use the pandas functions `unique()` or `value_counts()`.

```python
# list and count all the values of 'borough'
dog_licenses['borough'].value_counts()
```

Adding the function `count()` will show the total number of unique boroughs (79).

```python
dog_licenses['borough'].value_counts().count()
```

There are at least three different variations of `New York` with various capitalizations.
`New York`, `NEW YORK`, and `new york` should probably all be considered the same
borough, but unless we do something to these names, `pandas` will consider them unique.
There are also two `Manhattan` counts that are spelled and capitalized the same but are showing
up as different values. This could be due to some extra whitespace in the field.

A good first pass to clean this up is to apply some string methods to this column.
The method `str` allows us to use any Python function that works on strings and
apply it to a data frame.

We can remove whitespace from the left and right sides of each entry using
the string methods `lstrip()` and `rstrip()`.
Before we reassign the values to the 'borough' column, we can check if these operations
will decrease the number of unique values.

```python
print(dog_licenses['borough'].str.rstrip().value_counts().count())
print(dog_licenses['borough'].str.lstrip().value_counts().count())
```

Since only `rstrip()` actually reduces the number of unique values, we'll just do that.

```python
# strip whitespace from the end of each borough name and reassign to column
dog_licenses['borough'] = dog_licenses['borough'].str.rstrip()
```

After this operation, the total number of unique values has gone down to 73.

Next, we'll convert all the boroughs to lower case.

```python
dog_licenses['borough'] = dog_licenses['borough'].str.lower()
```

This reduces the unique boroughs down again to 58. Not bad, but another look
at `dog_licenses['borough'].value_counts()` shows us that we still have some issues:
'staten island' and 'staten is' are two different values that probably refer to the
same borough. At this point automatic processing might give way to some human
oversight: for instance, we could look up the true names of New York's boroughs
and see that there are only five. We could then cross-reference the 'borough' column
with other location information like the zip code or city council district to assign
correct boroughs to some of the mis-named ones. We could also do a few things
manually, such as renaming 'staten is'
to 'staten island', which would take care of 267 mis-labeled entries.

```python
# replace values in the 'borough' column
dog_licenses['borough'] = dog_licenses[['borough']].replace('staten is', 'staten island')
```

We could also decide that it's not worth it to meticulously fix the stragglers and
drop any rows whose borough appears less than, say, five times in the dataset. This
operation involves a few concepts that will be covered more in following sections,
but we'll show it here for completeness.

```python
# get a list of the unique borough names
borough_names = dog_licenses['borough'].value_counts().index

# get a list of all the borough names that have 5 or more members
large_borough_names = borough_names[dog_licenses['borough'].value_counts().values >= 5]

# the function `isin(list)` returns True or False for each row if the value is in `list`
dog_licenses_large_boroughs = dog_licenses[dog_licenses['borough'].isin(large_borough_names)]

# print the unique boroughs after this operation
print(dog_licenses_large_boroughs['borough'].value_counts())
```

## How can I select subsets of my data?

This data frame has rows and columns (it has 2 dimensions). To extract specific
data from it (also referred to as "subsetting"), columns can be selected by their name.

```python
# to see what column names there are, we can use the method .columns
dog_licenses.columns

# to get a single column, use square brackets and the column name in quotation marks. This is case sensitive!
dog_licenses["animal_name"].head()

# to get multiple columns, provide a list of column names inside the square brackets.
dog_licenses[["animal_name", "animal_gender"]].head()
```

Notice that if you provide a list, the returned object is a data frame, but if
you provide a single item not in a list, it's a `Series` object.

Selecting with single brackets (`[]`) is a shortcut to common operations such
as selecting columns by labels as above. The function `.loc[]` gives more
flexible control over both rows and columns by label. The syntax is `loc[<rows>, <columns>]`.

For example, you can select the first 10 rows and a slice of columns:

```python
dog_licenses.loc[0:10, "animal_name" : "breed_name"]
```

You can use logical operations to filter your data based on some criteria.

```python
# get rows where 'animal_gender' is female
dog_licenses[dog_licenses['animal_gender'] == 'F'].head()

# you can also get specific columns with .loc:
dog_licenses.loc[dog_licenses['animal_gender'] == 'F', ['animal_name', 'animal_gender']].head()
```

A single expression can also be used to filter for several criteria, either
matching all criteria (`&`) or any criteria (`|`). These special operators are
used instead of `and` and `or` to make sure that the comparison occurs for each
row in the data frame. Parentheses are added to indicate the priority of the
comparisons.


```python
# the symbol '&' can be used to combine multiple logical comparisons
# '==' means 'is equal to'
dog_licenses.loc[(dog_licenses['animal_gender'] == 'F')
         & (dog_licenses['breed_name'] == 'Boxer')
           & (dog_licenses['animal_name'] == 'MILEY'), 'animal_name' : 'breed_name']

# OR (|)
# the order of comparisons can be structured with parentheses. Here we want the OR comparison
# to happen before the AND comparison.

dog_licenses.loc[(dog_licenses['animal_name'] == 'MILEY') & ((dog_licenses['breed_name'] == 'Boxer') |  
          (dog_licenses['breed_name'] == 'Labrador Retriever')),  'animal_name' : 'breed_name']
```

## How can I calculate new values?

A new column can be added to a data frame by calling the new column name and
assigning values to it. The values should be either the same length as the
original data frame, or all the same value.

```python
# create a new column and fill all rows with the value 'test'
dog_licenses['new_column'] = 'test'
dog_licenses.head()

# drop the column we just made
dog_licenses = dog_licenses.drop('new_column', axis = 1)
```

We can also create new columns by doing operations on existing columns.
For example, let's create a column with the length of the dog's name.

```python
# the method .str allows string functions to be applied to the column contents (here we use 'len()')
name_lengths = dog_licenses['animal_name'].str.len()

# create a new column called 'name_length'
dog_licenses['name_length'] = name_lengths

# we can see that the new column has been added to the end of the data frame
dog_licenses.columns
```

Let's create a column with the age of each animal at the time it was licensed.
Note: this step takes a few seconds to run.

```python
# use to_datetime to create a datetime object for each date we're using
animal_birth_date = pd.to_datetime(dog_licenses['animal_birth_month'])
license_issued_datetime = pd.to_datetime(dog_licenses['license_issued_date'])

# subtract the birth date from the license date
age_when_licensed = license_issued_datetime - animal_birth_date

# assign age_when_licensed to a new column
dog_licenses['age_when_licensed'] = age_when_licensed
```

## How can I tell what's gone wronge in my programs?

Typos and errors are inevitable - they happen to everyone. Python provides
error messages if it can't run your code that will help you figure out where
the problem is.

```
# this code generates a syntax error
dog_licenses['animal_name'
```

The very bottom of the error message is the most helpul place to start looking:
this says what kind of error it is (e.g. `SyntaxError`, `ValueError`,
`TypeError`, etc) and sometimes includes a message that describes what went
wrong.

It also points to the place where the error happened: in this case, there's a
missing square bracket, so the error we got was that Python reached the end of
the file before it expected to because we didn't include proper punctuation.

```python
# this code generates a runtime error (there's nothing wrong with the syntax,
# but Python can't execute it because of a different error)

dog_licenses['animal_name'] < dog_licenses['zip_code']
```

The last line again tells us what went wrong, but this time we get a
*traceback*: Python is showing us the series of steps that it followed
internally before it hit an error. This is helpful because often the error will
ultimately occur inside a function we're using but whose code we haven't ever
seen, so Python shows us both what specifically went wrong inside the function,
but also what line of our code caused the original cascade. Here, the offending
line is pointed to at the very top of the traceback, and by reading the error
message at the bottom we can see that the problem was that we can't use the
'less than' `<` comparison between two objects when one is a string
(`dog_licenses['animal_name']`) and the other is a float (`dog_licenses['zip_code']`).

## How can I operate on subsets of my data?

Many data analysis tasks can be approached using the *split-apply-combine
paradigm*: split the data into groups, apply some analysis to each group, and
then combine the results.

`pandas` facilitates this workflow through the use of `groupby()` to split
data and summary/aggregation functions such as `mean()` which collapses each
group into a single-row summary of that group. The arguments to `groupby()` are
the column names that contain the categorical variables by which summary
statistics should be calculated.

Let's start with a smaller data frame of the top 10 most common dog breeds and
with only boroughs that are listed 5 or more times

```python
# get a list of the 10 most common breed names
common_breeds = dog_licenses_large_boroughs['breed_name'].value_counts()[:10].index

# create a new data frame with only the common breeds
dog_licenses_common_breeds = dog_licenses_large_boroughs[dog_licenses_large_boroughs['breed_name'].isin(common_breeds)]
```

`groupby()` enables us to break the data down into groups and perform operations
on each group. To look at how many animals of each sex there are in each of the
most common breeds, we can use `groupby()`:

```python
dog_licenses_common_breeds.groupby('breed_name')['animal_gender'].value_counts()
```

For some breeds, male dogs are much more common, but for other dogs the ratio
is about equal.

Other summary statistics, such as the mean, minimum, and maximum can be calculated:

```python
dog_licenses_common_breeds.groupby(['borough'])['name_length'].mean()
```

Groups can also be created from multiple columns. For example, we can group by
both breed and borough and look for the maximum name length:

```python
dog_licenses_common_breeds.groupby(['borough', 'breed_name'])['name_length'].max()

# add a sort to show the longest name first
dog_licenses_common_breeds.groupby(['borough', 'breed_name'])['name_length'].max().sort_values(ascending = False)
```

A Yorkshire Terrier from Manhattan holds the longest name in the dataset.

Instead of choosing a single summary statistic to calculate each time, the more
general `agg()` method can be called to aggregate or summarize by *any*
existing aggregation functions. This approach is more flexible and powerful
since multiple aggregation functions can be applied in the same line of code by
passing them as a list to `agg()`.

```python
dog_licenses_common_breeds.groupby(['breed_name'])['name_length'].agg(['count','min', 'max', 'mean'])
```

Yorkshire Terriers also have the largest mean name length of the top 10 breeds.

## How can I work with two or more datasets?

Often related data exists in multiple files. We often need to combine these
files into a single data frame to analyze the data. You can stack multiple data
frames together using the `concat()` function, and you can also *join* multiple
data frames using a common field.

### Concatenating data frames

Let’s grab two subsets of our data to see how `concat()` works.

```python
# get the first 10 rows
data_subset = dog_licenses.head(10)

# get the last 10 rows
data_subset_last10 = dog_licenses.tail(10)

# Reset the index values to the second data frame appends properly
data_subset_last10=dog_licenses_subset_last10.reset_index(drop=True)
# drop=True option avoids adding new index column with old index values
```

When we concatenate data frames, we need to specify the axis. `axis=0` tells
pandas to stack the second data frame under the first one. It will automatically
detect whether the column names are the same and will stack accordingly. `axis=1`
will stack the columns in the second data frame to the RIGHT of the first
data frame. To stack the data vertically, we need to make sure we have the same
columns and associated column format in both datasets. When we stack
horizonally, we want to make sure what we are doing makes sense (ie the data
are related in some way).

```python
# Stack the data frames on top of each other
vertical_stack = pd.concat([data_subset, data_subset_last10], axis=0)

# Place the data frames side by side
horizontal_stack = pd.concat([data_subset, data_subset_last10], axis=1)
```

### Joining data frames

When we concatenated our data frames we simply added them to each other -
stacking them either vertically or side by side. Another way to combine data
frames is to use columns in each dataset that contain common values (a common
unique id). Combining data frames using a common field is called “joining”. The
columns containing the common values are called “join key(s)”.



## How can I save my results?

We can use the `to_csv()` command to do export a data frame in CSV format. Note
that the code below will by default save the data into the current working
directory. We can save it to a different folder by adding the foldername and a
slash to the filename. We use the ‘index=False’ so that pandas doesn’t include
the index number for each line.

```python
# Write data frame to CSV
dog_licenses_common_breeds.to_csv('dog_licenses_common.csv', index=False)
```

## Summary {#py-data-manipulation-summary}


## Exercises {#py-data-manipulation-exercises}

1. **Understanding data frame structure**

Based on the output of `dog_licenses.info()`, can you answer the following questions?

 - What is the class of the object `dog_licenses` (where
   `dog_licenses = pd.read_csv("data/nyc-dog-licensing.csv")`)?
 - How many rows and how many columns are in this object?
 - Why is there not the same number of rows (observations) for each column?

2. **Finding messy values**

Get a list of the unique values in the `breed_name` column of the dog license
data using `value_counts()` or `unique()`. Sort this list alphabetically. (Hint:  
use the `sort_values()` function.) Look at the first 50 values. Do you see any
breed names that should be combined?

2. **Subsetting data by row**

- Extract the 200th and 201st row of the dog license dataset and assign the
resulting data frame to a new variable name (i.e. `data_200_201`). Remember
that Python indexing starts at 0!

- With the data frame `dog_licenses = pd.read_csv("data/nyc-dog-licensing.csv")`:
    - How can you get the same result as from `dog_licenses.head()` by using row slices
    instead of the `head()` method?

- There are at least three distinct ways to extract the last row of a data
frame. How many can you come up with?

3. **Selecting and filtering data**

Subset the dog license data frame `data` to include only female dogs and retain
only the columns 'animal_name', 'animal_gender' and 'breed_name'. There are
multiple ways this could be done.

3. **Error messages**

Run the following code. What type of error does it generate?

```python
dog_licenses.loc[0:10, 'animal_name', 'breed_name']
```

Edit the code to prevent it from causing an error.

4. **Creating new columns**

Create a new data frame from the dog license data frame `dog_licenses` that contains only the `animal_birth_month`, `animal_name`, and `breed_name` columns.
Add two new columns to this new data frame: one called `birth_year` which contains just the year from the `animal_birth_month` column, and one
called `birth_month` which contains just the month from the `animal_birth_month` column.

5. **groupby()**

  1. Use `groupby()` and `agg()` with the dog license data frame `dog_licenses` after adding the `age_when_licensed` column to find the mean, min, and max age when a dog was licensed in days grouped by `borough`. Hint: the pandas `timedelta` format of the `age_when_licensed` column will make calculating numeric quantities difficult; try creating a new column for the licensed age in days using `dog_licenses['age_when_licensed'].dt.days`.

  Which borough has the largest mean time until licensing? Return the columns `breed_name`, `borough`, and `age_when_licensed`. Hint: Look into the `idxmax()` method.

  2. How many dogs are female and how many are male?

6. **Joining data**



## Key Points {#py-data-manipulation-keypoints}
