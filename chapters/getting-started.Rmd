# Getting started {#getting-started}

> Everything starts somewhere, though many physicists disagree.
>
> --- Terry Pratchett\index{Pratchett, Terry}

If you've ever tried to learn a new programming language or library,
you've probably noticed that the process is much easier
if you have a real task/problem to work on.

This book uses data analysis as a motivating example,
and assumes that the learner's goal is to answer questions
rather than deliver commercial software products.

The data analysis task that we focus on
relates to a fascinating result in the field of quantitative linguistics.
[Zipf's Law][zipfs-law] states that the second most common word in a body of text
appears half as often as the most common,
the third most common appears a third as often, and so on.
To test Zipf's Law,
we analyze the distribution of word frequencies
in a collection of classic English novels
that are freely available from [Project Gutenberg][project-gutenberg].

## Syllabus {#intro-syllabus}

In the process of writing and publishing a Python package to verify Zipf's Law,
we will show you how to:

-   Use the Unix shell to efficiently manage your data and code.
-   Write Python programs that can be used on the command line.
-   Write and review code to make it readable as well as correct.
-   Use Git and GitHub to track and share your work.
-   Work productively in a small team where everyone is welcome.
-   Use Make to automate complex workflows.
-   Enable users to configure your software without modifying it directly.
-   Find, handle, and fix errors in your code.
-   Test your software and know which parts have not yet been tested.
-   Publish your code and research in open and reproducible ways.
-   Organise small and medium-sized data science projects.
-   Create Python packages that can be installed in standard ways.

Each chapter concludes with some exercises,
whose solutions are discussed in Appendix \@ref(solutions).
Early chapters have many small exercises;
later chapters have fewer but larger exercises.

## Intended audience {#intro-personas}

Amira Khan
:   completed a master's in library science five years ago
    and has since worked for a small aid organization.
    She did some statistics during her degree,
    and has learned some R and Python by doing data science courses online,
    but has no formal training in programming.
    Amira would like to tidy up the scripts, data sets, and reports she has created
    in order to share them with her colleagues.
    These lessons will show her how to do this and what "done" looks like.

Jun Hsu
:   completed an [Insight Data Science][insight] fellowship last year after doing a PhD in Geology
    and now works for a company that does forensic audits.
    He uses a variety of machine learning and visualization packages,
    and would now like to turn some of his own work into an open source project.
    This book will show him how such a project should be organized
    and how to encourage people to contribute to it.

Sami Virtanen
:   became a competent programmer during a bachelor's degree in applied math
    and was then hired by the university's research computing center.
    The kinds of applications they are being asked to support
    have shifted from fluid dynamics to data analysis;
    this guide will teach them how to build and run data pipelines
    so that they can pass those skills on to their users.

## Software prerequisites {#install-software}

Learners should already be using Python regularly for data analysis,
and should be comfortable reading data from files
and writing loops, conditionals, and functions.

Learners will need a computer with Internet access
that has the following software installed:

1. A \gref{Bash shell}{shell}
2. \gref{Git}{git} version control
3. A text editor
4 [Python 3][python] (via the Anaconda distribution)
5. [GNU Make][gnu-make]

Software installation instructions for Windows, Mac and Linux operating systems
(with video tutorials) are maintained by [The Carpentries][carpentries]
as part of their workshop website template:  
<https://carpentries.github.io/workshop-template/#setup>

Follow those instructions to install the bash shell, Git, a text editor and Anaconda.

Check if Make is installed on your computer by typing `make -v` into the Bash
shell. If it isn't installed, then:

- *Linux (Debian/Ubuntu)*: Install it from the Bash shell using `sudo apt-get install make`.
- *Mac*: Install [Xcode][xcode] (via the App Store).
- *Windows*: Follow the [installation instructions][ubc-mds-make-windows] maintained by the
  Master of Data Science at the University of British Columbia.

> **conda in the shell on windows**
>
> If you are using Windows and the `conda` command isn’t available at the Bash shell,
> you’ll need to open the Anaconda Prompt program (via the Windows start menu)
> and run the command `conda init bash` (this only needs to be done once).
> After that, your shell will be configured to use conda going forward.

## Where we'll end up: Project structure {#intro-structure}

Project organization is like a diet:
everyone has one,
it's just a question of whether it's healthy or not.
In the case of a project,
"healthy" means that people can find what they need and do what they want without becoming frustrated.
This depends on how well organized the project is
and how familiar people are with that style of organization.

As with coding style (Appendix \@ref(style)),
small pieces in predictable places with readable names are easier to find and use
than large chunks that vary from project to project and have names like "stuff".
We can be messy while we are working and then tidy up later,
but experience teaches that we will be more productive if we make tidiness a habit.  

In building the Zipf's Law project we'll follow 
a widely-used template
for organizing small and medium-sized data analysis projects [@Nobl2009].
The project will live in a directory called `zipf`, 
which will also be a Git repository stored on GitHub (Chapter \@ref(git-cmdline)).
The following is an abbreviated version of the project directory tree
as it appears towards the end of the book:

```text
zipf/
├── .gitignore
├── CITATION.md
├── CONDUCT.md
├── CONTRIBUTING.md   
├── LICENSE.md   
├── README.md
├── Makefile   
├── bin   
│   ├── book_summary.sh   
│   ├── collate.py   
│   ├── countwords.py   
|   └── ...    
├── data
│   ├── README.md   
│   ├── dracula.txt  
│   ├── frankenstein.txt  
│   └── ...   
├── docs
│   └── ...
├── results
│   ├── collated.csv
│   ├── dracula.csv
│   ├── dracula.png
|   └── ...
└── ...
```

The full, final directory tree is documented in Appendix \@ref(tree).

### Standard Information {#intro-boilerplate}

Our project will contain a few standard files
that should be present in every research software project,
open source or otherwise:

-   `README` includes basic information on our project.
     We'll create it in Chapter \@ref(git-advanced), 
     and extend it in Chapter \@ref(packaging).

-   `LICENSE` is the project's license. We'll add it in Section \@ref(teams-license).

-   `CONTRIBUTING` explains how to contribute to the project. We'll add it in Section \@ref(teams-documentation).

-   `CONDUCT` is the project's code of conduct. We'll add it in Section \@ref(teams-coc).

-   `CITATION` explains how to cite the software. We'll add it in Section \@ref(packaging-software-journals).

Some projects also include a `CONTRIBUTORS` or `AUTHORS` file that
lists everyone who has contributed to the project,
while others include that information in the `README` (we do this in Chapter \@ref(git-advanced))
or make it a section in `CITATION`.
These files are often called \gref{boilerplate}{boilerplate},
meaning they are copied without change from one use to the next.

### Organizing Project Content {#intro-organize}

Following @Nobl2009, 
the directories in the repository's root are organized according to purpose:

-   Runnable programs go in `bin/`
    (an old Unix abbreviation for "binary", meaning "not text").
    This will include both shell scripts, 
    e.g. `book_summary.sh` developed in Chapter \@ref(bash-advanced), 
    and Python programs, 
    e.g. `countwords.py`, developed in Chapter \@ref(scripting).

-   Raw data goes in `data/` 
    and is never modified after being stored.  
    You'll set up this directory, 
    and its contents in Section \@ref(download-data).  

-   Results are put in `results/`.
    This includes cleaned-up data,
    figures,
    and everything else created using what's in `bin` and `data`.
    In this project, 
    we'll describe exactly how `bin` and `data` are used 
    with `Makefile` created in Chapter \@ref(automate).

-   Finally, 
    documentation and manuscripts go in `docs/`.
    In this project `docs` will contain automatically generated
    documentation for the Python package, created in 
    Section \@ref(packaging-sphinx).

This structure works well for many computational research projects and
we encourage its use beyond just this book. 
We will add some more folders and files  not directly addressed by @Nobl2009
when we talk about provenance (Chapter \@ref(provenance)),
testing (Chapter \@ref(testing)),
and packaging (Chapter \@ref(packaging)).

### Downloading the data {#download-data}

Over the course of this book,
you'll build up the project structure described above.

The data files used in the book have been archived
at an online repository called Figshare (covered in Section \@ref(provenance-data-where)).
They can be accessed at the following URL:  
<https://doi.org/10.6084/m9.figshare.13040516>

Download a zip file containing the data files by clicking "download all" at the URL above.
Unzip the contents into your `zipf/data` directory
following the project structure described below.

These are the only files you'll need to get started.
When you are done,
you should have a directory (also called a \gref{folder}{folder}) 
called `zipf`,
containing a single sub-directory called `data`
with the following contents:

```text
zipf/
└── data
    ├── README.md
    ├── dracula.txt
    ├── frankenstein.txt
    ├── jane_eyre.txt
    ├── moby_dick.txt
    ├── sense_and_sensibility.txt
    ├── sherlock_holmes.txt
    └── time_machine.txt
```
